System,Domain,Task,Organization,Authors,Publication date,Reference,Link,Citations,Notability criteria,Notability criteria notes,Training compute (FLOP),Training compute notes,Training dataset notes,Training time (hours),Training time notes,Training hardware,Compute sponsor categorization,Abstract,Confidence,Model accessibility,Exclude,Hardware quantity,Training cost trends,Training chip-hours,Last modified,Created By,Country (from Organization),Organization categorization,Organization categorization (from Organization),Training dataset size (datapoints),Dataset size notes,Approach,Epochs,Hardware utilization,Foundation model,Training compute cost (2020 USD),Compute cost notes,Accessibility notes,Parameters,Parameters notes,Inference compute (FLOP),Inference compute notes,Training cloud compute vendor,Batch size,Batch size notes,Base model,Finetune compute (FLOP),Dataset accessibility,Code accessibility,Finetune compute notes,Training dataset,Training data center,Benchmark data,Archived links
Gemini Ultra,Multimodal,"Language modelling,Visual question answering,Chat,Translation",Google DeepMind,Gemini Team,2023-12-06,Gemini: A Family of Highly Capable Multimodal Models,https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,436.0,SOTA improvement,""" Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined.""",5.0000000001e+25,"This number is an estimate based on limited evidence. In particular, we combine information about the performance of Gemini Ultra on various benchmarks compared to other models, and guesstimates about the hardware setup used for training to arrive at our estimate. Our reasoning and calculations are detailed in this Colab notebook.
https://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c","""Gemini models are trained on a dataset that is both multimodal and multilingual. Our pretraining dataset uses data from web documents, books, and code, and includes image, audio, and video data... We find that data quality is critical to a highlyperforming model, and believe that many interesting questions remain around finding the optimal
dataset distribution for pretraining.""",2400.0,"Dylan Patel, author of SemiAnalysis, speculates that the training duration of Gemini may have been 100 days.",Google TPU v4,Industry,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",Speculative,Hosted access (no API),0.0,55000.0,Gemini Ultra,132000000.0,2024-04-01 14:02:06+00:00,Anonymous,Multinational,Industry,Industry,,,,,,,,,,,,,,,,,,,,,,,,,
GPT-4,Multimodal,Language modelling,OpenAI,OpenAI,2023-03-15,GPT-4 Technical Report,https://arxiv.org/abs/2303.08774,3280.0,"Highly cited,SOTA improvement","See the paper, p.1: ""On a suite of traditional NLP benchmarks, GPT-4 outperforms both previous large language models and most state-of-the-art systems (which often have benchmark-specific training or hand-engineering).""",2.1e+25,"90% CI: 8.2E+24 to 4.4E+25

NOTE: this is a rough estimate based on public information, much less information than most other systems in the database.

Calculation and confidence intervals here: https://colab.research.google.com/drive/1O99z9b1I5O66bT78r9ScslE_nOj5irN9?usp=sharing",,2280.0,(Speculative) SemiAnalysis conjectures that GPT-4 training took 90-100 days with utilization of 32-36%.,NVIDIA A100 SXM4 40 GB,Industry,,Speculative,API access,0.0,25000.0,GPT-4,57000000.0,2024-04-01 14:30:18+00:00,Robi Rahman,United States of America,Industry,Industry,4900000000000.0,"Speculative. Reported secondhand by online sources such as Semianalysis, but not verified by OpenAI. If total number of tokens seen was 13T, text was repeated for 2 epochs, and text was the majority of tokens, then dataset size roughly is 13T*0.75/2 = 4.9T words.

Note this examines only the text dataset, since GPT-4 was first and foremost a language model. However, the vision component had its own vision dataset, which we believe accounted for a much smaller part of the compute budget.",Self-supervised learning,2.0,0.34,True,,,,,,,,,,,,,,,,,,,
Mistral Large,Language,Chat,Mistral AI,,2024-02-26,"Mistral Large, our new flagship model",https://mistral.ai/news/mistral-large/,,,,2.0000000001e+25,"https://www.wsj.com/tech/ai/the-9-month-old-ai-startup-challenging-silicon-valleys-giants-ee2e4c48
https://x.com/EMostaque/status/1762152740938031484?s=20
""assuming this is on H100s with @Scaleway who are €1.9/hour => 10m H100 hours (c 30m A100 hrs), 3 months at 4k H100s :timer_clock:"" -Emad Mostaque
Assuming bf16 or fp16, H100 PCIe performance is 1513 TFLOPS
At 1.9 euro per H100-hour and 33% utilization, spending 20M euro produces 1.9*10^25 FLOP.
https://www.wolframalpha.com/input?i=20+million+%2F+%281.9%2Fhour%29+*+3958+TFLOPS+*+0.33
https://www.scaleway.com/en/h100-pcie-try-it-now/",,2500.0,Speculation by Emad Mostaque: 20M euro spent at Scaleway (1.9 euro per H100-hour) would be around 3 months on 4000 H100s.,NVIDIA H100 PCIe,,,Speculative,API access,0.0,,,,2024-03-20 14:05:35+00:00,Anonymous,France,Industry,Industry,,,,,,,18500000.0,"In February 2024, 20M EUR = 22M USD
Converting to 2020 USD, this is 18.5M
https://www.in2013dollars.com/us/inflation/2024?endYear=2020&amount=22000000",,,,,,,,,,,,,,,,,
Inflection-2,Language,Language modelling,Inflection AI,,2023-11-22,Inflection-2: The Next Step Up,https://inflection.ai/inflection-2,,Significant use,"Inflection-2 either already powers Pi or soon will: https://inflection.ai/inflection-2

Inflection has claimed that Pi has >1m users: https://x.com/inflectionAI/status/1699100179390210091?s=20",1.001e+25,"""Inflection-2 was trained on 5,000 NVIDIA H100 GPUs in fp8 mixed precision for ~10²⁵ FLOPs""

(the second 1 is there because of airtable being wonky, it's not a real sig fig)",,,,NVIDIA H100 SXM5,Industry,"Today we are proud to announce that we have completed training of Inflection-2, the best model in the world for its compute class and the second most capable LLM in the world today. Our mission at Inflection is to create a personal AI for everyone. Just a few months ago, we announced Inflection-1 — a best-in-class language model that currently powers Pi. Our new model, Inflection-2, is substantially more capable than Inflection-1, demonstrating much improved factual knowledge, better stylistic control, and dramatically improved reasoning.",Likely,Hosted access (no API),0.0,5000.0,Inflection-2,,2024-03-27 22:27:35+00:00,Anonymous,United States of America,Industry,Industry,,,,,,True,,,"via Pi, no API",,,,,,,,,,,,,,,,
PaLM 2,Language,Language modelling,Google,"Andrew M. Dai, David R. So, Dmitry Lepikhin, Jonathan H. Clark, Maxim Krikun, Melvin Johnson, Nan Du, Rohan Anil, Siamak Shakeri, Xavier Garcia, Yanping Huang, Yi Tay, Yong Cheng, Yonghui Wu, Yuanzhong Xu, Yujing Zhang, Zachary Nado, Bryan Richter, Alex Polozov, Andrew Nystrom, Fangxiaoyu Feng, Hanzhao Lin, Jacob Austin, Jacob Devlin, Kefan Xiao, Orhan Firat, Parker Riley, Steven Zheng, Yuhuai Wu, Zhongtao Liu, Jiahui Yu, Guy Gur-Ari, Weikang Zhou, Sneha Kudugunta, Sunipa Dev, Frederick Liu, Gustavo Hernandez Abrego, Kelvin Xu, Abe Ittycheriah, Daniel Sohn, John Nham, Le Hou, Siyuan Qiao, Pidong Wang, Zirui Wang, Laurent El Shafey, Hyeontaek Lim, Marcello Maggioni, Michael Isard, Paul Barham, Qiao Zhang, Tao Wang, Yash Katariya, Aurko Roy, Benjamin Lee, Brennan Saeta, Ce Zheng, Hadi Hashemi, Junwhan Ahn, Rajkumar Samuel, Steven Hand, Zhifeng Chen, Kiran Vodrahalli, Aakanksha Chowdhery, Ethan Dyer, Emanuel Taropa, Vlad Feinberg, James Bradbury, Reiner Pope, Wei Li, YaGuang Li, Eric Chu, Jeffrey Hui, Joshua Howland, Vlad Fienber, Aroma Mahendru, Michele Catasta, Vedant Misra, Kevin Robinson, Maysam Moussalem, Sebastian Ruder, Erica Moreira, Eric Ni, Paige Bailey, Lucas Gonzalez, Alexandre Passos, Slav Petrov, Gaurav Mishra, Mark Omernick, Ambrose Slone, Andrea Hu, Colin Cherry, Denny Zhou, Jan Botha, John Wieting, Joshua Maynez, Kathleen Kenealy, Kevin Brooks, Linting Xue, Markus Freitag, Martin Polacek, Pengcheng Yin, Sebastian Gehrmann, Xuezhi Wang, Kathy Meier-Hellstern, Christopher A. Choquette-Choo, Daniel Smilkov, Emily Reif, Alicia Parrish, Alex Castro Ros, Clément Crepy, Dasha Valter, Jeremy Hurwitz, Katherine Lee, Mark Díaz, Marie Pellat, Matthew Jagielski, Renee Shelby, Shachi Dave",2023-05-10,PaLM 2 Technical Report,https://arxiv.org/abs/2305.10403,583.0,SOTA improvement,,7.34e+24,"Compute Requirements ""Not reported.""
Paper suggests heuristic of  C=6ND. Based on 340B parameters and 3.6*10^12 tokens, training compute would be around 7.3*10^24 FLOP.","""The PaLM 2 pre-training corpus is composed of a diverse set of sources: web documents, books, code, mathematics, and conversational data. The pre-training corpus is significantly larger than the corpus used to train PaLM (Chowdhery et al., 2022). PaLM 2 is trained on a dataset that includes a higher percentage of non-English data than previous large language models, which is beneficial for multilingual tasks"" (page 9)",,,Google TPU v4,Industry,"We introduce PaLM 2, a new state-of-the-art language model that has better multilingual and reasoning capabilities and is more compute-efficient than its predecessor PaLM (Chowdhery et al., 2022). PaLM 2 is a Transformer-based model trained using a mixture of objectives similar to UL2 (Tay et al., 2023). Through extensive evaluations on English and multilingual language, and reasoning tasks, we demonstrate that PaLM 2 has significantly improved quality on downstream tasks across different model sizes, while simultaneously exhibiting faster and more efficient inference compared to PaLM. This improved efficiency enables broader deployment while also allowing the model to respond faster, for a more natural pace of interaction. PaLM 2 demonstrates robust reasoning capabilities exemplified by large improvements over PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable performance on a suite of responsible AI evaluations, and enables inference-time control over toxicity without additional overhead or impact on other capabilities. Overall, PaLM 2 achieves state-of-the-art performance across a diverse set of tasks and capabilities.",Likely,API access,0.0,,PaLM 2,,2024-04-01 14:52:17+00:00,Robi Rahman,United States of America,Industry,Industry,2700000000000.0,"""The pre-training corpus is significantly larger than the corpus used to train PaLM"" so greater than 6e+11. According to the leaked documents viewed by CNBC, the corpus was 3.6 trillion tokens or around 2.7*10^12 words.

https://www.cnbc.com/2023/05/16/googles-palm-2-uses-nearly-five-times-more-text-data-than-predecessor.html",,,,True,,PaLM 2 was trained on TPU v4 according to the model card (pages 91-92),,340000000000.0,"Model Architecture: ""PaLM-2 is a new state-of-the-art language model. We have small, medium, and large variants that use stacked layers based on the Transformer architecture, with varying parameters depending on model size. Further details of model size and architecture are withheld from external publication.""
However, the parameter count was leaked to CNBC: https://www.cnbc.com/2023/05/16/googles-palm-2-uses-nearly-five-times-more-text-data-than-predecessor.html",,,,,,,,,,,,,,
Claude 2,Language,Language modelling,Anthropic,,2023-07-11,,"https://www.anthropic.com/index/claude-2, https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf",0.0,Historical significance,,3.866e+24,https://colab.research.google.com/drive/1MdPuhS4Emaf23VXYZ-ooExDW-5GXZkw0#scrollTo=Ds0Q5X8aMnOY,"From model card: ""Claude models are trained on a proprietary mix of publicly available information from the Internet, datasets
that we license from third party businesses, and data that our users affirmatively share or that crowd workers provide. Some of the human feedback data used to finetune Claude was made public [12] alongside our RLHF [2] and red-teaming [4] research.
Claude 2’s training data cuts off in early 2023, and roughly 10 percent of the data included was non-English.""",,,,,,Speculative,API access,0.0,,Claude 2,,2024-03-20 14:06:16+00:00,Anonymous,United States of America,Industry,Industry,,,,,,True,,,,,,,,,,,,,,,,,,,
Falcon-180B,Language,Language modelling,Technology Innovation Institute,"Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, Daniele Mazzotta, Badreddine Noune, Baptiste Pannier, Guilherme Penedo",2023-09-06,The Falcon Series of Open Language Models,https://falconllm.tii.ae/falcon-180b.html; https://arxiv.org/abs/2311.16867,59.0,SOTA improvement,"""It's currently at the top of the Hugging Face Leaderboard for pre-trained Open Large Language Models and is available for both research and commercial use.""

""This model performs exceptionally well in various tasks like reasoning, coding, proficiency, and knowledge tests, even beating competitors like Meta's LLaMA 2.""",3.76e+24,"43,500 petaflop-days per Table 1 of the paper

43500 * 1e15 * 24 * 3600 = 3.76e24


C = 6ND = 6 FLOP/token/parameter * 3.5 trillion tokens * 180 billion parameters = 3.78*10^24 FLOP","""The Falcon series is made of three causal decoder-only models trained on up to 4,096 A100. We assembled a pretraining dataset of 3,500 billion tokens, predominantly sourced from our work on RefinedWeb (Penedo et al., 2023)–a massive filtered and deduplicated web dataset""

Training dataset composition is described in Table 3. Falcon was trained for 1 epoch.",4320.0,"Stanford CRFM foundation model ecosystem graph data page https://crfm.stanford.edu/ecosystem-graphs/index.html?asset=Falcon-180B says 9 months, which is the maximum possible amount of time: training began sometime in 2023, and it was released in September. 

However, 6 months is more realistic. That is the length of the gap between Falcon 40B and Falcon 180B. Additionally, the amount of compute is specified in the paper, so there is only one degree of freedom in the uncertain values of training duration and hardware utilization rate. At six months, the utilization is unusually low, so the training was probably not longer than that.",NVIDIA A100 SXM4 40 GB,,"Falcon 180B is a super-powerful language model with 180 billion parameters, trained on 3.5 trillion tokens. It's currently at the top of the Hugging Face Leaderboard for pre-trained Open Large Language Models and is available for both research and commercial use.

This model performs exceptionally well in various tasks like reasoning, coding, proficiency, and knowledge tests, even beating competitors like Meta's LLaMA 2.

Among closed source models, it ranks just behind OpenAI's GPT 4, and performs on par with Google's PaLM 2 Large, which powers Bard, despite being half the size of the model.",Likely,Open access (restricted use),0.0,4096.0,Falcon 180B,17694720.0,2024-04-02 13:14:32+00:00,Anonymous,United Arab Emirates,Government,Government,2625000000000.0,3.5 trillion tokens * (~3 words per 4 tokens) ~= 2.625 trillion words,,1.0,0.1876,,,"From Hugging Face:
""Falcon-180B was trained on up to 4,096 A100 40GB GPUs, using a 3D parallelism strategy (TP=8, PP=8, DP=64) combined with ZeRO.""
""Falcon-180B was trained on AWS SageMaker, on up to 4,096 A100 40GB GPUs in P4d instances.""
https://huggingface.co/tiiuae/falcon-180B

Utilization must have been at least 12.5%, and they probably did not use the whole 4096 GPU cluster for 9 months, so it was probably higher. Lower bound estimate:
https://www.wolframalpha.com/input?i=%286+FLOP+*+3.5+trillion+*+180+billion%29+%2F+%284096*312+teraFLOPS+*+9+months%29","""Falcon 180b can be commercially used but under very restrictive conditions, excluding any ""hosting use""."" https://huggingface.co/blog/falcon-180b",180000000000.0,"""Falcon 180B is a super-powerful language model with 180 billion parameters""",360000000000.0,C_inference = 2 FLOP / token / param * N => 360B FLOP per token,Amazon Web Services,4194304.0,"from paper (https://arxiv.org/pdf/2311.16867.pdf):

Batch size 2048 (presumably sequences) per Table 16. Warmed up using smaller batches for first 100B tokens.

""All Falcon models are pretrained with a 2,048 sequence length""

2048*2048 = 4194304",,,,,,,,,
Minerva (540B),Language,Quantitative reasoning,Google,"Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, Vedant Misra",2022-06-29,Solving Quantitative Reasoning Problems with Language Models,https://arxiv.org/abs/2206.14858,408.0,SOTA improvement,,2.7415e+24,"Minerva was fine-tuned from PaLM using the same hardware. Assume the same model FLOPs utilization rate for pre-training and fine-tuning.

PaLM pretraining time: 6144 TPU for 1200 hours + 3072 TPU for 336 hours = @8404992 TPU-hours
Minerva finetuning time: 1024 TPU for 696 hours = 712704 TPU-hours
So fine-tuning added 8.5% more compute.

Minerva total compute = PaLM pretraining compute * (712704+8404992)/(8404992) = 2.7415*10^24 FLOP
https://www.wolframalpha.com/input?i=%28712704%2B8404992%29%2F%288404992%29+*+2.5272*10%5E24

","PaLM, finetuned on arxiv",696.0,,Google TPU v4,Industry,"Language models have achieved remarkable performance on a wide range of tasks that require natural language understanding. Nevertheless, state-of-the-art models have generally struggled with tasks that require quantitative reasoning, such as solving mathematics, science, and engineering problems at the college level. To help close this gap, we introduce Minerva, a large language model pretrained on general natural language data and further trained on technical content. The model achieves state-of-the-art performance on technical benchmarks without the use of external tools. We also evaluate our model on over two hundred undergraduate-level problems in physics, biology, chemistry, economics, and other sciences that require quantitative reasoning, and find that the model can correctly answer nearly a third of them.",,Unreleased,0.0,1024.0,Minerva (540B),712704.0,2024-04-01 14:02:07+00:00,Robi Rahman,United States of America,Industry,Industry,613875000000.0,"""Our models were trained on a dataset of 38.5B tokens"" + PaLM",Self-supervised learning,,,True,3267257.74977208,,,540350000000.0,"""To further our understanding of the
impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer
language model, which we call Pathways Language Model (PaLM).""

Our approach is to start with the PaLM pretrained decoder-only transformer language models Chowdhery
et al. (2022), and further train (finetune) them on our mathematical dataset using an autoregressive objective.
Table 2 contains the main model and training hyperparameters.

See Table 2",,,,,,PaLM (540B),2.1429e+23,Unreleased,Unreleased,,,,,
DBRX,Language,"Chat,Code generation",Databricks,Mosaic Research Team,2024-03-27,Introducing DBRX: A New State-of-the-Art Open LLM,https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm,,,,2.6e+24,"36 billion params * 12 trillion tokens * 6 ~= 2.6e24
https://www.wolframalpha.com/input?i=6+FLOP+*+36+billion+*+12+trillion

also, it was trained on 3072 NVIDIA H100s, but with an unclear timeframe (end-end process was three months, including evals and red-teaming).","12T tokens, text and code

""It was pre-trained on 12T tokens of text and code data...

DBRX was pretrained on 12T tokens of carefully curated data and a maximum context length of 32k tokens. We estimate that this data is at least 2x better token-for-token than the data we used to pretrain the MPT family of models""

from HF: https://huggingface.co/databricks/dbrx-base

The training mix used for DBRX contains both natural-language and code examples. The vast majority of our training data is in the English language",,,NVIDIA H100 SXM5,,"Today, we are excited to introduce DBRX, an open, general-purpose LLM created by Databricks. Across a range of standard benchmarks, DBRX sets a new state-of-the-art for established open LLMs. Moreover, it provides the open community and enterprises building their own LLMs with capabilities that were previously limited to closed model APIs; according to our measurements, it surpasses GPT-3.5, and it is competitive with Gemini 1.0 Pro. It is an especially capable code model, surpassing specialized models like CodeLLaMA-70B on programming, in addition to its strength as a general-purpose LLM.",Confident,Open access (restricted use),0.0,,,,2024-03-27 18:11:16+00:00,Anonymous,United States of America,Industry,Industry,9000000000000.0,"12T tokens is equivalent to 9T words. Though it includes code data, so not very literally 9T words",,1.0,,,,,"license: https://www.databricks.com/legal/open-model-license
conditions based on monthly users",132000000000.0,132B mixture of experts. 36B parameters active per inference,,,,,,,,Unreleased,Unreleased,,,,,
GPT-3.5 (text-davinci-003),Language,Language modelling,OpenAI,,2022-11-28,,https://platform.openai.com/docs/models/gpt-3-5,,"Historical significance,Significant use,SOTA improvement",,2.578e+24,https://colab.research.google.com/drive/1QSxa8YCWjEBQU7mrXLhw6TP1VX5oqgdW#scrollTo=Gt6Z6oZ26clI,,,,NVIDIA A100 SXM4 40 GB,Industry,,Speculative,API access,0.0,,GPT-3.5 (text-davinci-003),,2024-03-20 14:06:25+00:00,Anonymous,United States of America,Industry,Industry,,,Reinforcement learning,,,,,,,,"Parameter count may be 175B based on OpenAI's statements that text-davinci-003 is in the GPT-3.5 series of models. It was also stated to be 175B in the Microsoft CODEFUSION paper, but the paper was reportedly retracted because the authors did not know the parameter count.",,,,,,,,,,,,,,
U-PaLM (540B),Language,Language generation,Google,"Yi Tay, Jason Wei, Hyung Won Chung, Vinh Q. Tran, David R. So, Siamak Shakeri, Xavier Garcia, Huaixiu Steven Zheng, Jinfeng Rao, Aakanksha Chowdhery, Denny Zhou, Donald Metzler, Slav Petrov, Neil Houlsby, Quoc V. Le, Mostafa Dehghani",2022-10-20,Transcending Scaling Laws with 0.1% Extra Compute,https://arxiv.org/abs/2210.11399,45.0,SOTA improvement,"""We show that U-PaLM 540B outperforms PaLM 540B on 21 out of 26 tasks. Given that PaLM is
the SOTA language model on these tasks, this makes U-PaLM the new state-of-the-art on these tasks.""

performance improvement equivalent to 2x training efficiency: ""Impressively, at 540B scale, we show an approximately 2x computational savings rate where U-PaLM achieves the same performance as the final PaLM 540B model at around half its computational budget """,2.53e+24,"""The total number of extra tokens we train on for the 540B
model is approximately 1.3 Billion which constitutes 0.16% extra computation... Training an U-PaLM 540B model only consumes 512 TPUv4 chips and finishes in about 5 days which is considered to be lightweight.""

original PaLM was 2.527e+24. adding 0.16% is ~2.53e24","""To keep things consistent, we train this model with the same data mixture as PaLM and do not rely on
additional sources of data (labeled or unlabeled).""",120.0,5 days,Google TPU v4,,"Scaling language models improves performance but comes with significant computational costs. This paper proposes UL2R, a method that substantially improves existing language models and their scaling curves with a relatively tiny amount of extra compute. The key idea is to continue training a state-of-the-art large language model (e.g., PaLM) on a few more steps with UL2's mixture-of-denoiser objective. We show that, with almost negligible extra computational costs and no new sources of data, we are able to substantially improve the scaling properties of large language models on downstream metrics. In this paper, we continue training PaLM with UL2R, introducing a new set of models at 8B, 62B, and 540B scale which we call U-PaLM. Impressively, at 540B scale, we show an approximately 2x computational savings rate where U-PaLM achieves the same performance as the final PaLM 540B model at around half its computational budget (i.e., saving ∼4.4 million TPUv4 hours). We further show that this improved scaling curve leads to 'emergent abilities' on challenging BIG-Bench tasks -- for instance, U-PaLM does much better than PaLM on some tasks or demonstrates better quality at much smaller scale (62B as opposed to 540B). Overall, we show that U-PaLM outperforms PaLM on many few-shot setups, i.e., English NLP tasks (e.g., commonsense reasoning, question answering), reasoning tasks with chain-of-thought (e.g., GSM8K), multilingual tasks (MGSM, TydiQA), MMLU and challenging BIG-Bench tasks. Finally, we provide qualitative examples showing the new capabilities of U-PaLM for single and multi-span infilling.",Confident,Unreleased,0.0,512.0,U-PaLM (540B),61440.0,2024-04-01 14:03:41+00:00,Anonymous,United States of America,Industry,Industry,,,,,,,,,,540000000000.0,,,,,,,PaLM (540B),4e+21,,,"""The total number of extra tokens we train on for the 540B
model is approximately 1.3 Billion which constitutes 0.16% extra computation... Training an U-PaLM 540B model only consumes 512 TPUv4 chips and finishes in about 5 days which is considered to be lightweight.""

PaLM was 2.5e24
0.16% of that is 4e21",,,,
PaLM (540B),Language,Language modelling,Google Research,"Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev,, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta ,Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, Noah Fiedel",2022-04-04,PaLM: Scaling Language Modeling with Pathways,https://arxiv.org/abs/2204.02311,3532.0,"Highly cited,SOTA improvement","Demonstrates continued benefits of scaling, as well as discontinuous improvements in performance",2.5272e+24,"See Table 20: https://storage.googleapis.com/pathways-language-model/PaLM-paper.pdf

6144 TPUv4 for 1200 hours + 3072 TPUv4 for 336 hours.
Equivalent to 6144 TPUv4 for 1368 hours.

46.2% model FLOPs utilization",,1368.0,"6144 TPUv4 for 1200 hours + 3072 TPUv4 for 336 hours.

Equivalent to 6144 TPUv4 for 1368 hours.",Google TPU v4,Industry,"Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",Likely,Unreleased,0.0,6144.0,PaLM (540B),8404992.0,2024-04-01 14:03:41+00:00,Robi Rahman,Multinational,Industry,Industry,585000000000.0,"""The PaLM pretraining dataset consists of a high-quality corpus of 780 billion tokens that represent a wide range of natural language use cases.""

1 token ~ 0.75 words",Self-supervised learning,,0.462,True,3232806.53266529,"Training compute and utilization rate exclude rematerialization FLOP, but cost should account for rematerialization.",,540350000000.0,"""To further our understanding of the
impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer
language model, which we call Pathways Language Model (PaLM).""",,,,4000000.0,"""For the largest model, we use batch size 512 (1M tokens) until step 50k, then double it to 1024 (2M tokens) until step 115k, and finally double again it to 2048 (4M tokens) until training is complete at step 255k""",,,,,,,,,
Qwen-72B,Language,"Chat,Code generation",Alibaba,"Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, Tianhang Zhu",2023-11-30,,https://huggingface.co/Qwen/Qwen-72B,,SOTA improvement,"SOTA on several Chinese benchmarks, with highest average rating overall for Chinese benchmarks:

https://opencompass.org.cn/leaderboard-llm",1.3e+24,"72 billion params, 3 trillion tokens
72b * 3T * 6 = 1.3e24","""It is pretrained on over 3 trillion tokens, including Chinese, English, multilingual texts, code, and mathematics, covering general and professional fields""",,,,,"Qwen-72B is the 72B-parameter version of the large language model series, Qwen (abbr. Tongyi Qianwen), proposed by Alibaba Cloud. Qwen-72B is a Transformer-based large language model, which is pretrained on a large volume of data, including web texts, books, codes, etc. Additionally, based on the pretrained Qwen-72B, we release Qwen-72B-Chat, a large-model-based AI assistant, which is trained with alignment techniques.",Likely,Open access (restricted use),0.0,,Qwen-72B,,2024-03-27 19:22:10+00:00,Anonymous,China,Industry,Industry,,,,,,,,,"up to 100m active users:
https://github.com/QwenLM/Qwen/blob/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT",72000000000.0,72B,,,,4000000.0,"Table 1 https://arxiv.org/abs/2309.16609
(this is uncertain because this table only lists sizes up to 14B. 72B was released after the paper)",,,Unreleased,Unreleased,,,,,
XVERSE-65B-2,Language,Chat,XVERSE Technology,,2023-12-08,,https://github.com/xverse-ai/XVERSE-65B/blob/main/README_EN.md,,,,1.24800000000001e+24,C = 6ND = 6 * 3.2T tokens * 65B params = 1.248e24 FLOP,"[2023/12/08] Released the XVERSE-65B-2 base model. This model builds upon its predecessor through Continual Pre-Training, reaching a total training volume of 3.2 trillion tokens.",4096.0,November 6 to December 8 is 32 days. They did 600B tokens of continual pretraining during this period. The model's total tokens are 3200B. Therefore the total pretraining time was around (32 days * 24 hours/day)*(3200/600) = 4096 hours.,,,,Likely,Open source,0.0,,,,2024-03-27 18:58:15+00:00,Robi Rahman,China,Industry,Industry,2720000000000.0,"Training Data: The model has been thoroughly trained on a diversified and high-quality dataset consisting of 2.6 trillion of tokens, including more than 40 languages such as Chinese, English, Russian, and Spanish. The sampling ratio of different types of data is finely set, which makes the performance of Chinese and English excellent, and also takes into account the effect of other languages.

Assume 0.85 words per token on average for the mix of languages.",,,,,,,Apache 2.0,65000000000.0,Based on the name. Exact count unknown but may be listed on Hugging Face.,,,,,,,,,,,,,,
Code Llama-70B,Language,Code generation,Meta AI,"Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Ellen Tan, Yossef (Yossi) Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Defossez, Jade Copet, Faisal Azhar, Hugo Touvron, Gabriel Synnaeve, Louis Martin, Nicolas Usunier, Thomas Scialom",2024-01-29,Code Llama: Open Foundation Models for Code,"https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/
https://arxiv.org/abs/2308.12950",463.0,,"""In our own benchmark testing, Code Llama outperformed state-of-the-art publicly available LLMs on code tasks""",1.230000000001e+24,Finetune compute for 70B model: 1T tokens of code * ,"We are releasing four sizes of Code Llama with 7B, 13B, 34B, and 70B parameters respectively. Each of these models is trained with 500B tokens of code and code-related data, apart from 70B, which is trained on 1T tokens.",6480.0,Assuming Code Llama 70B training continued on same hardware as Llama 2 70B.,NVIDIA A100 SXM4 80 GB,Industry,"We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B and 34B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B and 13B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 53% and 55% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use.",Likely,Open access (restricted use),0.0,400.0,,2592000.0,2024-04-01 14:02:07+00:00,Robi Rahman,United States of America,Industry,Industry,3000000000000.0,Llama 70B training dataset was 2 trillion tokens. Code Llama finetuning dataset was 1 trillion tokens of code.,,1.0,0.435,,,,"Llama 2 license. can't use outputs to train models.

https://github.com/meta-llama/llama/blob/main/LICENSE",70000000000.0,70B,140000000000.0,"Assume 70B parameters, dense architecture, 2 FLOP/inference -> 140B FLOP/inference",,4000000.0,"Llama 2 pretraining used 4M batches. I believe the sentence below refers to the training from Llama 2 -> Code Llama-base. 

""We use a batch size of 4M tokens which are presented as sequences of 4,096 tokens each.""

Subsequent fine-tuning batch sizes are 500k-1M. 

""For Code Llama - Instruct, we train with a batch size of 524,288 tokens and on approx. 5B tokens in total... For long context fine-tuning (LCFT)... the batch size is set to 2M tokens for model sizes 7B and 13B and to 1M tokens for model size 34B, respectively. Training lasts for 10,000 gradient steps by default."" ",Llama 2-70B,4.200000000001e+23,,,"Training the nine Code Llama models took 400k A100-hours across all the models, per model card. It's nine models because there are three base models at 7B, 13B, 34B, and then Instruct and Python models across all three sizes. I'll calculate for Code Llama Python-34B since it's the most trained.

Code Llama-base is trained from Llama 2 with 500B tokens: ""We train Code Llama on 500B tokens during the initial phase, starting from the 7B, 13B, and 34B versions of Llama 2""

Code Llama-Python required an additional 100B tokens in fine-tuning: ""We train Code Llama on 500B additional tokens and Code Llama - Python further on 100B tokens.""
Code Llama-Instruct is fine-tuned on 5B tokens: ""For Code Llama - Instruct, we train with a batch size of 524,288 tokens and on approx. 5B tokens in total.""

34B * (500B+100B) * 6 = 1.22e23

Using the hardware method, we get 400,000 * 3600 * 312 trillion * 0.3 = 1.32e23 for the whole family. Using 34/(34+13+7) as the proportion of compute used for the 34B model, we get 0.63 * 1.32e23 = 8.5e22. (this is rounding down instruct tuning to 0).

Token*params estimate is probably better; seems like evidence of utilization > 0.3.",,,,
Megatron-Turing NLG 530B,Language,Language modelling,"Microsoft,NVIDIA","Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, Bryan Catanzaro",2021-10-11,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",https://arxiv.org/abs/2201.11990,530.0,SOTA improvement,"The 105-layer, transformer-based MT-NLG improved upon the prior state-of-the-art models in zero-, one-, and few-shot settings",1.17e+24,https://www.lesswrong.com/posts/bGuMrzhJdENCo8BxX/nvidia-and-microsoft-releases-530b-parameter-transformer?commentId=HSJSNspKp94tFcSCx," In addition to Common Crawl data, we leveraged a number of other previously generated datasets. From The Pile, we selected Books3, OpenWebText2, Stack Exchange, PubMed Abstracts,
Wikipedia, Gutenberg (PG-19), BookCorpus2, NIH ExPorter, and Pile-CC datasets. We also included the
CC-Stories and RealNews datasets used to train Megatron",770.0,"Total compute was 1.17*10^24 FLOP.
They don't directly report the utilization and training speed when using the full Selene supercomputer with 560 DGX * 8 A100/DGX = 4480 GPUs. See section 2.3 Hardware Setup.

At 280 DGX, the utilization is 126/312 = 40% and a batch takes 60 seconds; at 350, it is 39% for 50 seconds; at 420, it is 36% for 44 seconds.

The overall utilization was 30.2% and the full cluster has 560 DGX. Dividing the total compute by the total performance of 4480 A100 at 30.2% utilization gives 770 hours.",NVIDIA A100 SXM4 80 GB,Industry,"Pretrained general-purpose language models can achieve state-of-the-art accuracies in various natural language processing domains by adapting to downstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of their success, the size of these models has increased rapidly, requiring high-performance hardware, software, and algorithmic techniques to enable training such large models. As the result of a joint effort between Microsoft and NVIDIA, we present details on the training of the largest monolithic transformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530 billion parameters. In this paper, we first focus on the infrastructure as well as the 3D parallelism methodology used to train this model using DeepSpeed and Megatron. Next, we detail the training process, the design of our training corpus, and our data curation techniques, which we believe is a key ingredient to the success of the model. Finally, we discuss various evaluation results, as well as other interesting observations and new properties exhibited by MT-NLG. We demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning accuracies on several NLP benchmarks and establishes new state-of-the-art results. We believe that our contributions will help further the development of large-scale training infrastructures, large-scale language models, and natural language generations.",,Unreleased,0.0,4480.0,Megatron-Turing NLG 530B,3449600.0,2024-04-01 14:35:28+00:00,Robi Rahman,"United States of America,United States of America","Industry,Industry","Industry,Industry",202500000000.0,"""Our training dataset consists of 339 billion tokens and we
trained MT-NLG on 270 billions tokens by blending the 15 training datasets as described above. We also set aside 2% of our data for validation.""

1 token ~ 0.75 words",Self-supervised learning,,0.302,True,3046994.0871934,,,530000000000.0,,,,,3932160.0,"""The sequence length is 2048 and the global batch size is 1920. We used 8-way tensor and 35-way pipeline parallelism. The learning rate is 5.0e −5 . We used one billion tokens for linear learning rate warmup. We used cosine decay for the learning rate targeting to reach 10% of its value over 340 billion tokens. Over the first 12 billion tokens, we started at a batch size of 32 and gradually increased the batch size in increments of 32, until we reach the final batch size of 1920"" 

Final batch size is 1920 * 2048 = 3932160",,,,,,"Common Crawl,The Pile",,,
ChatGLM3,Multimodal,"Chat,Visual question answering",Zhipu AI,,2023-10-27,Zhipu AI launches third-generation base model,https://www.zhipuai.cn/en/news/76,,SOTA improvement,"Aiming at GPT-4V, ChatGLM3 has implemented iterative upgrades of several new functions this time, including:

CogVLM with multi-modal understanding capabilities, looks at image semantics, and achieved SOTA on more than 10 international standard image and text evaluation data sets;",1.09200000000001e+24,"Highly speculative.
Assume 1 epoch on 1.4T tokens.
6 FLOP/token/param * 1.4T tokens * 130B params
https://www.wolframalpha.com/input?i=6*130+billion*1.4+trillion",ChatGLM2 corpus pretraining plus human preference alignment training,,,,Industry,"On October 27, 2023, at the 2023 China Computer Conference (CNCC), Zhipu AI launched the fully self-developed third-generation large base model ChatGLM3 and related series of products.",Speculative,,0.0,,ChatGLM3,,2024-03-07 20:22:50+00:00,Anonymous,China,Industry,Industry,1050000000000.0,"The ChatGLM website states that the latest ChatGLM service is based on (and upgraded from) ChatGLM2, which was trained on 1.4T tokens. Assume that ChatGLM3 is trained on at least the same number of tokens.
Sources:
https://chatglm.cn/
https://github.com/THUDM/ChatGLM2-6B/blob/main/README_EN.md
https://www.zhipuai.cn/en/news/76",,,,True,,,,130000000000.0,"Highly speculative. The ChatGLM website https://chatglm.cn/ states that the model has hundreds of billions of parameters, so at least 100e9. It also states that the new model is based on ChatGLM2 and the GLM architectures. There is a previous GLM 130B model, so this may be the most likely size.",,,,,,,,,,,,,,
ERNIE 3.0 Titan,Language,,"Baidu,Peng Cheng Laboratory","Shuohuan Wang, Yu Sun, Yang Xiang, Zhihua Wu, Siyu Ding, Weibao Gong, Shikun Feng",2021-12-23,ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation,https://arxiv.org/abs/2112.12731,49.0,SOTA improvement,"""Empirical results show that the ERNIE 3.0 Titan outperforms the state-of-the-art models on 68 NLP datasets.""",1.0421e+24,"The paper suggests that ERNIE 3.0 Titan uses more compute than GPT-3. This is consistent with the 6ND approximation.

C = 6ND = 6 (FLOP/param/token) * (260B params) * (668B tokens) = 1.0421*10^24 FLOP",,,,"Huawei Ascend 910,NVIDIA Tesla V100 DGXS 32 GB",Industry,"Pre-trained language models have achieved state-of-the-art results in various Natural Language Processing (NLP) tasks. GPT-3 has shown that scaling up pre-trained language models can further exploit their enormous potential. A unified framework named ERNIE 3.0 was recently proposed for pre-training large-scale knowledge enhanced models and trained a model with 10 billion parameters. ERNIE 3.0 outperformed the state-of-the-art models on various NLP tasks. In order to explore the performance of scaling up ERNIE 3.0, we train a hundred-billion-parameter model called ERNIE 3.0 Titan with up to 260 billion parameters on the PaddlePaddle platform. Furthermore, we design a self-supervised adversarial loss and a controllable language modeling loss to make ERNIE 3.0 Titan generate credible and controllable texts. To reduce the computation overhead and carbon emission, we propose an online distillation framework for ERNIE 3.0 Titan, where the teacher model will teach students and train itself simultaneously. ERNIE 3.0 Titan is the largest Chinese dense pre-trained model so far. Empirical results show that the ERNIE 3.0 Titan outperforms the state-of-the-art models on 68 NLP datasets.",Likely,Unreleased,0.0,1920.0,ERNIE 3.0 Titan,,2024-04-01 14:30:19+00:00,Robi Rahman,"China,China","Industry,Academia","Industry,Academia",668000000000.0,"""To ensure the success of the pre-training of ERNIE 3.0 Titan, we utilize the ERNIE 3.0 Corpus [ 2 ], a large-scale, wide-variety, and high-quality Chinese text corpora amounting to 4TB""

Assuming 167M words per GB",,,,True,,,,260000000000.0,"""[We] developed... distributed training technology, including fine-grained parallelism, heterogeneous hardware-aware training, and fault tolerance mechanism to train the 260B model on both Nvidia V100 GPU and Ascend 910 NPU clusters.""
See also:
https://twitter.com/BaiduResearch/status/1468633977242243078?t=6q4zuLNdTSc4GUBe9OM5Aw&s=19",,,,1048576.0,"""The maximum sequence length of context and
the memory length of language generation is 512 and 128, respectively""

In table 1, they use a global batch size of 512 when data parallelism is ""1"" and 2048 when DP is ""4"". Not sure I fully understand this part but I guess they'd use parallelism as much as possible given how they talk about it.

2048 * 512 = 1048576.",,,,,,ERNIE 3.0 Corpus,,,
TigerBot-70B,Language,"Chat,Language generation",Tigerobo,"Ye Chen, Wei Cai, Liangmin Wu, Xiaowei Li, Zhanxuan Xin, Cong Fu",2023-09-06,TigerBot: An Open Multilingual Multitask LLM,"https://github.com/TigerResearch/TigerBot/blob/main/README_en.md, https://arxiv.org/abs/2312.08688",,,"Outperforms Llama 2:

""We use 10 mainstream benchmark test sets in the industry to evaluate the model's reading comprehension, reasoning, world knowledge, common sense Q&A, mathematics and coding capabilities, including: mmlu, arc, squad_v2, squad, mrqa, web_questions, openbook_qa, commonsense_qa, trivia_qa, wiki_qa. The evaluation results are shown in the figure below. The comprehensive capabilities of Tigerbot-70b are better than Llama-2-70b, and also ahead of large open source models at home and abroad.""

https://github.com/TigerResearch/TigerBot/wiki/TigerBot%E2%80%9070B%E5%8F%91%E5%B8%83%EF%BC%81",1.02e+24,"~1.02e24

Tigerobo did ~2.1e23 additional pre-training. We estimated Llama 2 was trained on 8.1e23 FLOP.","In addition to Llama 2 pretraining dataset, TigerBot was trained on 500B tokens including Chinese data

""Training data: 500B tokens pre-trained data, knowledge截止 to August 2023. More high-quality data, including: tens of thousands of volumes, arXiv, Chinese textbooks, legal and patent data;""",,,,,"(translated from https://github.com/TigerResearch/TigerBot/wiki/TigerBot%E2%80%9070B%E5%8F%91%E5%B8%83%EF%BC%81)

We are pleased to release Tigerbot-70b, which continues to be open source and free for commercial use, including:

Tigerbot-70b-base: Continuing pre-training on the basis of Llama-2-70b, the model's comprehensive capabilities are better than Llama-2-70b in 10 mainstream benchmark tests such as mmlu, reaching SOTA in the industry.

a. Using high-quality multi-lingual data of 300 billion tokens,

b. The algorithm uses GQA, flash-attn, RoPE, holistic-training and other technologies,

c. The training uses tensor/pipeline-partition technology, and the computing efficiency reaches the SOTA reported in the Llama-2 paper.",Likely,Permissive license (depr.),0.0,,,,2024-04-02 13:21:43+00:00,Anonymous,China,Industry,Industry,,,,,,,,,,70000000000.0,70B,,,,4000000.0,"from paper:

""We pretrained TigerBot models using a global batch size (GBS) of 4M tokens, while fine-tuned models with a GBS as small as 100–400k tokens""

It's also based on pretrained Llama 2, which also used a batch size of 4M",Llama 2-70B,2.1e+21,,,"""Training data: 500B tokens pre-trained data, knowledge截止 to August 2023. More high-quality data, including: tens of thousands of volumes, arXiv, Chinese textbooks, legal and patent data;""

70b * 500b * 6 = 2.1e23",,,,
Inflection-1,Language,Language modelling,Inflection AI,,2023-06-23,Inflection-1 technical memo,https://inflection.ai/assets/Inflection-1.pdf,0.0,,"""Inflection-1 outperforms models trained with at most the same amount of compute as PaLM-540B on MMLU and the other benchmarks in Table 1.""",1.0001e+24,"<= 2.5e24

They define two ""compute classes"", one for models with more compute than PaLM 540B, i.e. GPT-4 and PaLM 2, and one for models with as much compute or less, i.e. GPT-3.5, Chinchilla, LLaMA, and Inflection-1.

PaLM 540B required 2.5e24 FLOP to train (confirmed by Google)","""Inflection-1 was trained using thousands of NVIDIA H100 GPUs on a very large dataset.""",,,NVIDIA H100 SXM5,Industry,"Large language models (LLMs) based on the Transformer architecture have been shown to possess a range of advanced capabilities in language generation and understanding. These capabilities have paved the way for deployment of LLMs in products like OpenAI’s Chat-GPT and Google’s Bard. At Inflection AI, our mission is to create personal AIs for everyone, and in May 2023 we released Pi (pi.ai) – an LLM-based personal AI which is designed to be empathetic, useful, and safe. In this work we introduce the foundation model powering Pi, dubbed Inflection-1, and evaluate its performance characteristics across a variety of benchmarks.",Speculative,Hosted access (no API),0.0,,,,2024-03-20 14:30:01+00:00,Anonymous,United States of America,Industry,Industry,,,,,,True,,,,,,,,,,,,,,,,,,,
Llama 2-70B,Language,Language modelling,Meta AI,"Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez,Robert Stojnic, Sergey Edunov, Thomas Scialom
",2023-07-18,Llama 2: Open Foundation and Fine-Tuned Chat Models,"https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/
https://arxiv.org/abs/2307.09288",3131.0,"Historical significance,Significant use,Highly cited",Model has been open-sourced and frequently downloaded. The paper claims that Llama 2 is the current best open-source chat model as of its release date.,8.1e+23,"""Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB"" of which 1720320 GPU hours were used to train the 70B model.

311.84 BF16 TFLOP/s * 1720320 hours * 0.40 utilization = 7.725e+23 FLOP.

Alternatively: the model was trained for 1 epoch on 2 trillion tokens and has 70B parameters. C = 6ND = 6*70B*2T = 8.4e+23 FLOP.","2 trillion tokens of publicly available text, with no text from Meta's products.
""Our training corpus includes a new mix of data from publicly available sources, which does not include data from Meta’s products or services. We made an effort to remove data from certain sites known to contain a high volume of personal information about private individuals. We trained on 2 trillion tokens of data as this provides a good performance–cost trade-off, up-sampling the most factual sources in an effort to increase knowledge and dampen hallucinations.""",2160.0,"Model was trained from January 2023 to July 2023, which is six months. However, the training run duration did not take up this whole period. According to a Meta employee interviewed by Epoch, Llama 2 34B and 70B were trained on different clusters, with overlapping training periods.",NVIDIA A100 SXM4 80 GB,Industry,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",Confident,Open access (restricted use),0.0,1000.0,Llama 2-70B,2160000.0,2024-04-01 14:35:44+00:00,Anonymous,United States of America,Industry,Industry,1500000000000.0,2 trillion tokens ~= 1.5 trillion words,Supervised,1.0,0.435,,1620000.0,"A100 cost in 2023: $1.10/hour
Training time: 1720320 A100 GPU-hours
Inflation adjustment: $1.000 2020 = $1.145 2023","Llama 2 license. can't use outputs to train models.

https://github.com/meta-llama/llama/blob/main/LICENSE",70000000000.0,"Llama has been released in 7B, 13B, 34B, and 70B variants.",140000000000.0,Inference compute usage for the 70B model is 140 billion operations per token of input.,,4000000.0,,,,,,,Llama 2 dataset,Meta’s Research Super Cluster,,
DeepSeek LLM 67B,Language,Chat,DeepSeek,"Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y.K. Li, Wenfeng Liang, Fangyun Lin, A.X. Liu, Bo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y. Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei Xu, R.X. Xu, Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping Yu, Xingkai Yu, B. Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, Qihao Zhu, Yuheng Zou",2024-01-05,DeepSeek LLM: Scaling Open-Source Language Models with Longtermism,"https://arxiv.org/abs/2401.02954, https://github.com/deepseek-ai/DeepSeek-LLM",,,"One of the best open/Chinese models: ""Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.""",8.04e+23,67B * 2T * 6 = 8.04e23,"""We collect 2 trillion tokens for pre-training, primarily in Chinese and English.""

""We have gained valuable insights from reputable sources such as (Computer, 2023; Gao et al., 2020; Penedo et al., 2023; Touvron et al., 2023a)... We adopted an aggressive deduplication strategy, expanding the deduplication scope. Our analysis revealed that deduplicating the entire Common Crawl corpus results in higher removal of duplicate instances compared to deduplicating within a single dump""",,,,,"The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.",Likely,Open source,0.0,,,,2024-03-27 18:53:25+00:00,Anonymous,China,Industry,Industry,1750000000000.0,"""We collect 2 trillion tokens for pre-training, primarily in Chinese and English""

if it's half English and half Chinese, that's
750B English words + 1T Chinese words = 1.75T words

https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.1m0mpkhs9ljx",,1.0,,,,,https://github.com/deepseek-ai/deepseek-LLM/blob/main/LICENSE-MODEL,67000000000.0,"67B
",,,,,,,,,,,,,,
Gopher (280B),Language,Language modelling,DeepMind,"Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d’Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu and Geoffrey Irving",2021-12-08,"Scaling Language Models: Methods, Analysis & Insights from Training Gopher",https://arxiv.org/abs/2112.11446,880.0,SOTA improvement,"""These models are evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority""",6.31e+23,"Table A26
6.31E+08 Train PFLOPs",,920.0,"""We trained Gopher for 920 hours in November and December 2020 in Google’s Georgia datacentre. The PUE of the datacenter at this time was 1.08; the net tCO2e per MWh in October 2020 was 0.33. Using an estimate of 283W drawn per chip, this leads to a total of 380 net tCO2e""",Google TPU v3,Industry,"We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a 2 trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25× fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.",Confident,Unreleased,0.0,4096.0,Gopher (280B),3768320.0,2024-04-01 14:52:16+00:00,Robi Rahman,United Kingdom of Great Britain and Northern Ireland,Industry,Industry,225000000000.0,"""We train all models for 300 billion tokens with a 2048 token context window, using the Adam (Kingma and Ba, 2014) optimiser.""

1 token ~ 0.75 words",Self-supervised learning,1.0,0.378,,891638.804314709,,,280000000000.0,"Language modelling provides a step towards intelligent communication systems by harnessing large repositories of written human knowledge to better predict and understand the world. In this paper, we present an analysis of Transformer-based language model performance across a wide range of model scales -- from models with tens of millions of parameters up to a 280 billion parameter model called Gopher. These models are evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority. Gains from scale are largest in areas such as reading comprehension, fact-checking, and the identification of toxic language, but logical and mathematical reasoning see less benefit. We provide a holistic analysis of the training dataset and model's behaviour, covering the intersection of model scale with bias and toxicity. Finally we discuss the application of language models to AI safety and the mitigation of downstream harms.",,,,6000000.0,"Table 1. ""Furthermore, we increase Gopher’s batch size from three to six million tokens per batch during training""",,,Unreleased,Unreleased,,,,Gopher (280B),
Yi-34B,Language,Chat,01.AI,"Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, Zonghong Dai",2023-11-02,Yi: Open Foundation Models by 01.AI,https://arxiv.org/abs/2403.04652,,Significant use,"2nd most popular model on HuggingFace: https://decrypt.co/206195/new-open-source-ai-model-from-china-boasts-twice-the-capacity-of-chatgpt

also maybe the best open-source model, does better than Llama 2-70B on several benchmarks",6.1e+23,"""The dataset we use contains Chinese & English only. We used approximately 3T tokens"" sounds like this means it was trained on 3T tokens, not necessarily that the dataset contains 3T tokens?

If so, 34b * 3T * 6 = 6.1e23",Chinese and English dataset,,,,,The Yi series models are large language models trained from scratch by developers at 01.AI.,Speculative,Open access (restricted use),0.0,,Yi-34B,,2024-04-02 13:20:40+00:00,Anonymous,China,Industry,Industry,,,,,,,,,"apply for commercial:
https://github.com/01-ai/Yi/blob/main/MODEL_LICENSE_AGREEMENT.txt",34000000000.0,34b,,,,,,,,,,,,,,Yi-34B
xTrimoPGLM -100B,Biology,Proteins,"Tsinghua University,BioMap Research","Bo Chen, Xingyi Cheng, Yangli-ao Geng, Shen Li, Xin Zeng, Boyan Wang, Jing Gong, Chiming Liu, Aohan Zeng, Yuxiao Dong, Jie Tang, Le Song",2023-07-06,xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein,https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1,31.0,SOTA improvement,"""Our extensive experiments reveal that xTrimoPGLM significantly outperforms other advanced baselines in diverse protein understanding tasks (13 out of 15 tasks across four categories)""",6.0001e+23,"""xTrimoPGLM-100B is trained on a cluster of 96 DGX-A100 GPU (8×40G) servers in FP16 precision from January 18 to June 30, 2023. During this time, xTrimoPGLM-100B has consumed 1
trillion tokens from the dataset consisting of Uniref90 and ColAbFoldDB. As of the current date,
xTrimoPGLM-100B continues its pre-training process to pass through as many tokens as possible""

6 * 100 billion params * 1T tokens = 6e23

8*96 * 312 trillion * 163 days * 24 * 3600 * 0.3 ~= 1e24",,3912.0,163 days,NVIDIA A100 SXM4 40 GB,,"Protein language models have shown remarkable success in learning biological information from protein sequences. However, most existing models are limited by either autoencoding or autoregressive pre-training objectives, which makes them struggle to handle protein understanding and generation tasks concurrently. This paper proposes a unified protein language model, xTrimoPGLM, to address these two types of tasks simultaneously through an innovative pre-training framework. Our key technical contribution is an exploration of the compatibility and the potential for joint optimization of the two types of objectives, which has led to a strategy for training xTrimoPGLM at an unprecedented scale of 100 billion parameters and 1 trillion training tokens. Our extensive experiments reveal that xTrimoPGLM significantly outperforms other advanced baselines in diverse protein understanding tasks (13 out of 15 tasks across four categories) and generates novel protein sequences which are structurally similar to natural ones. Furthermore, using the same xTrimoPGLM framework, we train an antibody-specific model (xTrimoPGLM-Ab) using 1 billion parameters. This model set a new record in predicting antibody naturalness and structures, both essential to the field of antibody-based drug design, and demonstrated a significantly faster inference speed than AlphaFold2. These results highlight the substantial capability and versatility of xTrimoPGLM in understanding and generating protein sequences.",Likely,Unreleased,,768.0,,2352.0,2024-04-02 08:46:58+00:00,Anonymous,"China,China","Academia,Industry","Academia,Industry",,~24M protein sequences,Self-supervised learning,,,,,,,100000000000.0,"Abstract: ""training xTrimoPGLM at an unprecedented scale of 100 billion
parameters and 1 trillion training tokens""",,,,,,,,,Unreleased,,UniRef50,,,
BLOOM-176B,Language,Language modelling,"Hugging Face,BigScience","Margaret Mitchell, Giada Pistilli, Yacine Jernite, Ezinwanne Ozoani, Marissa Gerchick, Nazneen Rajani, Sasha Luccioni, Irene Solaiman, Maraim Masoud, Somaieh Nikpoor, Carlos Muñoz Ferrandis, Stas Bekman, Christopher Akiki, Danish Contractor, David Lansky, Angelina McMillan-Major, Tristan Thrush, Suzana Ilić, Gérard Dupont, Shayne Longpre, Manan Dey, Stella Biderman, Douwe Kiela, Emi Baylor, Teven Le Scao, Aaron Gokaslan, Julien Launay, Niklas Muennighoff",2022-11-08,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model,https://arxiv.org/abs/2211.05100,1313.0,"Historical significance,Highly cited","Was the largest open-source model at the time. 1000+ researchers, many from important orgs such as Microsoft and NVIDIA.

https://huggingface.co/bigscience/bloom",5.7700000000001e+23,"https://towardsdatascience.com/run-bloom-the-largest-open-access-ai-model-on-your-desktop-computer-f48e1e2a9a32

384 A100 GPUs * 150 TFLOPS throughput per GPU * 116 days = 5.77e+23 FLOP
https://www.wolframalpha.com/input?i=384+*+150+TFLOPS+*+116+days","In total, 1.6 terabytes of pre-processed text was converted into 350 billion unique tokens as BLOOM's training datasets.
arXiv:2210.15424",2808.0,117 days * 24 hours/day,NVIDIA A100 SXM4 80 GB,,,Confident,Open access (restricted use),0.0,384.0,BLOOM-176B,1078272.0,2024-04-01 14:30:18+00:00,Robi Rahman,"Multinational,Multinational","Industry,Research collective","Industry,Research collective",262500000000.0,350B tokens ~= 262B words,Self-supervised learning,1.0,0.4808,True,,,responsible use restrictions: https://bigscience.huggingface.co/blog/the-bigscience-rail-license,176247271424.0,"See ""Technical Specifications"" on Hugging Face:
https://huggingface.co/bigscience/bloom",,,,4194304.0,Table 3. 2048*2048,,,,,,TB scale multilingual dataset,,,
Chinchilla,Language,Language modelling,DeepMind,"Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals and Laurent Sifre",2022-03-29,Training Compute-Optimal Large Language Models,https://arxiv.org/abs/2203.15556,993.0,SOTA improvement,"Proposes new scaling law, with good empirical results",5.76e+23,"""Both Chinchilla and Gopher have been trained for the same number of FLOPs but differ in the size of the model and the number of training tokens.""

We see the number of flops in table 3","MassiveWeb, Books, C4, News, Github, Wikipedia (Table A1)",,,"Google TPU v4,Google TPU v3",Industry,"We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over \nummodels language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, \chinchilla, that uses the same compute budget as \gopher but with 70B parameters and 4× more more data. \chinchilla uniformly and significantly outperforms \Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that \chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, \chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over \gopher.",Likely,Unreleased,0.0,,Chinchilla,,2024-04-01 14:35:28+00:00,Robi Rahman,United Kingdom of Great Britain and Northern Ireland,Industry,Industry,1050000000000.0,"Table 1 shows Chinchilla was training on 1.4 trillion tokens

1 token ~ 0.75 words",Self-supervised learning,1.0,,True,753491.57852839,,,70000000000.0,"""We test this hypothesis by training a predicted compute-optimal model, \chinchilla, that uses the same compute budget as \gopher but with 70B parameters and 4× more more data. \chinchilla uniformly and significantly outperforms \Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks.""",,,,3000000.0,"Table 1. ""1.5M → 3M""",,,Unreleased,Unreleased,,"MassiveWeb,C4",,Chinchilla,
BIG-G 137B,Language,Language modelling/generation,Google,,2022-06-09,Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models,https://arxiv.org/abs/2206.04615,610.0,,,5.6e+23,"""BIG-G models were trained at Google. We use 13 dense decoder-only Transformer models (Vaswani
et al., 2017) with gated activation layers (Dauphin et al., 2017) and GELU activations based on the LaMDA
architectures (Thoppilan et al., 2022). These models were trained on a dataset consisting of a mixture of web
documents, code, dialog, and Wikipedia data, with approximately three billion documents tokenized to 2.8
trillion BPE tokens using a 32k-token SentencePiece vocabulary""

Appendix:

""We use a pre-training batch size of 262k tokens for all models...""

2.6M steps for 137B, per Table App.1. So trained on 2.6M * 262k = 681B tokens (~0.25 epochs)
681B * 137B * 6 = 5.6e23","""These models were trained on a dataset consisting of a mixture of web
documents, code, dialog, and Wikipedia data, with approximately three billion documents tokenized to 2.8
trillion BPE tokens using a 32k-token SentencePiece vocabulary""",,,,Industry,"Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit ""breakthrough"" behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.",Likely,,0.0,,,,2024-03-07 20:22:50+00:00,Anonymous,United States of America,Industry,Industry,,,,,,,,,,137000000000.0,"137B. Table App.1
",,,,,,,,,,,,,,
LLaMA-65B,Language,Language modelling,Meta AI,"Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample",2023-02-24,LLaMA: Open and Efficient Foundation Language Models,https://arxiv.org/abs/2302.13971,4640.0,"Historical significance,Highly cited",Widely-used foundation model that has been adapted for others such as Alpaca.,5.5e+23,"1.4e12 tokens * 6.52e10 parameters * 6 FLOP/token/parameter = 5.5e23 FLOP

Compared to 2048 A100 GPUs each with 311.84 TFLOPS maximum performance for 21 days, this implies 47% utilization.
https://www.wolframalpha.com/input?i=5.5*10%5E23+FLOP+%2F+%282048+*+311.84+teraFLOPS+*+21+days%29","The model was trained using the following source of data: CCNet [67%], C4 [15%], GitHub [4.5%], Wikipedia [4.5%], Books [4.5%], ArXiv [2.5%], Stack Exchange[2%]. The Wikipedia and Books domains include data in the following languages: bg, ca, cs, da, de, en, es, fr, hr, hu, it, nl, pl, pt, ro, ru, sl, sr, sv, uk. See the paper for more details about the training set and corresponding preprocessing.",500.0,"""When training a 65B-parameter model, our code processes around 380 tokens/sec/GPU on 2048 A100 GPU with 80GB of RAM. This means that training over our dataset containing 1.4T tokens takes approximately 21 days.""",NVIDIA A100,Industry,"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",Likely,Open access (non-commercial),0.0,2048.0,LLaMA-65B,1024000.0,2024-04-01 15:00:19+00:00,Anonymous,United States of America,Industry,Industry,1050000000000.0,1.4 trillion tokens * 0.75 words/token = 1.05 trillion words,Supervised,1.09,0.4746,True,1179384.75,"1023384 processor-hours on A100 GPUs. May 2023 cost rate is $1.36/GPU-hour on Azure ML cloud. https://azure.microsoft.com/en-us/pricing/details/machine-learning/ 
According to https://www.bls.gov/data/inflation_calculator.htm, $1.18 in May 2023 = $1.00 in January 2020.
$1391674 / 1.18 = $1179385 in 2020 USD.","""we are releasing our model under a noncommercial license focused on research use cases"" https://ai.meta.com/blog/large-language-model-llama-meta-ai/",65200000000.0,"Model card, table 1: https://github.com/facebookresearch/llama/blob/53011c3d7946dadb8274a4c5c7586ab54edf792d/MODEL_CARD.md",130000000000.0,per token,,4000000.0,,,,,,,"CCNet,GitHub,Wikipedia,books,arXiv,Stack Exchange",,LLaMA-65B,
Code Llama-34B,Language,Code generation,Meta AI,"Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Ellen Tan, Yossef (Yossi) Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Defossez, Jade Copet, Faisal Azhar, Hugo Touvron, Gabriel Synnaeve, Louis Martin, Nicolas Usunier, Thomas Scialom",2023-08-14,Code Llama: Open Foundation Models for Code,"https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/
https://arxiv.org/abs/2308.12950",463.0,,"SOTA for open models: ""Moreover, our largest model,
with its 34B parameters, is significantly larger than previous open-source models – GPT-NeoX-20B (Black
et al., 2022) and StarCoder with 15.5B parameters – which allows it to achieve state-of-the-art performances
on HumanEval, MBPP and MultiPL-E among open-source models""",5.3e+23,"1.22e23 finetune compute, or ~5.3e23 including Llama-2 34B base compute
","""As shown in Table 1, Code Llama is trained predominantly on a near-deduplicated dataset of
publicly available code. We also source 8% of our samples data from natural language datasets related to
code. This dataset contains many discussions about code and code snippets included in natural language
questions or answers. To help the model retain natural language understanding skills, we also sample a small
proportion of our batches from a natural language dataset""",,,NVIDIA A100 SXM4 80 GB,,"We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B and 34B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B and 13B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 53% and 55% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use.",Likely,Permissive license (depr.),0.0,,,,2024-04-02 13:11:58+00:00,Anonymous,United States of America,Industry,Industry,,,,,,,,,,34000000000.0,34B,,,,4000000.0,"Llama 2 pretraining used 4M batches. I believe the sentence below refers to the training from Llama 2 -> Code Llama-base. 

""We use a batch size of 4M tokens which are presented as sequences of 4,096 tokens each.""

Subsequent fine-tuning batch sizes are 500k-1M. 

""For Code Llama - Instruct, we train with a batch size of 524,288 tokens and on approx. 5B tokens in total... For long context fine-tuning (LCFT)... the batch size is set to 2M tokens for model sizes 7B and 13B and to 1M tokens for model size 34B, respectively. Training lasts for 10,000 gradient steps by default."" ",Llama 2-34B,1.22e+23,,,"Training the nine Code Llama models took 400k A100-hours across all the models, per model card. It's nine models because there are three base models at 7B, 13B, 34B, and then Instruct and Python models across all three sizes. I'll calculate for Code Llama Python-34B since it's the most trained.

Code Llama-base is trained from Llama 2 with 500B tokens: ""We train Code Llama on 500B tokens during the initial phase, starting from the 7B, 13B, and 34B versions of Llama 2""

Code Llama-Python required an additional 100B tokens in fine-tuning: ""We train Code Llama on 500B additional tokens and Code Llama - Python further on 100B tokens.""
Code Llama-Instruct is fine-tuned on 5B tokens: ""For Code Llama - Instruct, we train with a batch size of 524,288 tokens and on approx. 5B tokens in total.""

34B * (500B+100B) * 6 = 1.22e23

Using the hardware method, we get 400,000 * 3600 * 312 trillion * 0.3 = 1.32e23 for the whole family. Using 34/(34+13+7) as the proportion of compute used for the 34B model, we get 0.63 * 1.32e23 = 8.5e22. (this is rounding down instruct tuning to 0).

Token*params estimate is probably better; seems like evidence of utilization > 0.3.",,,,
PanGu-Σ,Language,"Code generation,Language modelling",Huawei Noah's Ark Lab,"Xiaozhe Ren, Pingyi Zhou, Xinfan Meng, Xinjing Huang, Yadao Wang, Weichao Wang, Pengfei Li, Xiaoda Zhang, Alexander Podolskiy, Grigory Arshinov, Andrey Bout, Irina Piontkovskaya, Jiansheng Wei, Xin Jiang, Teng Su, Qun Liu, Jun Yao",2023-03-20,PanGu-Σ: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing,https://arxiv.org/abs/2303.10845,34.0,SOTA improvement,"""Our experimental findings show that PanGu-{\Sigma} provides state-of-the-art performance in zero-shot learning of various Chinese NLP downstream tasks.""",4.67e+23,"It has sparse architecture, so we can't use C=6ND.
""We develop PanGu-Σ model under the framework of MindSpore and train it on a cluster with only 512 Ascend 910 AI Accelerators with 329 billion tokens over 100 days.""
100 days * 512 processors * 320 teraFLOPS/processor * 33% utilization = 4.67e+23 FLOP
https://www.wolframalpha.com/input?i=100+days+*+512+*+320+terahertz+*+0.33","""329B tokens in more than 40 natural and programming languages""",2400.0,"We develop PanGu-Σ model under the framework of MindSpore 5
and train it on a cluster with only 512 Ascend 910 AI Accelerators [28] with 329 billion tokens over 100 days.",Huawei Ascend 910,Industry,"The scaling of large language models has greatly improved natural language understanding, generation, and reasoning. In this work, we develop a system that trained a trillion-parameter language model on a cluster of Ascend 910 AI processors and MindSpore framework, and present the language model with 1.085T parameters named PanGu-{\Sigma}. With parameter inherent from PanGu-{\alpha}, we extend the dense Transformer model to sparse one with Random Routed Experts (RRE), and efficiently train the model over 329B tokens by using Expert Computation and Storage Separation(ECSS). This resulted in a 6.3x increase in training throughput through heterogeneous computing. Our experimental findings show that PanGu-{\Sigma} provides state-of-the-art performance in zero-shot learning of various Chinese NLP downstream tasks. Moreover, it demonstrates strong abilities when fine-tuned in application data of open-domain dialogue, question answering, machine translation and code generation.",Confident,Unreleased,0.0,512.0,PanGu-Σ,1228800.0,2024-03-28 22:40:44+00:00,Anonymous,China,Industry,Industry,246750000000.0,329B tokens ~= 247B words,Self-supervised learning,1.836,,,,,,1085000000000.0,"""In this work, we present PanGu-Σ , a large language model with sparse architecture containing 1.085 trillion parameters.""",,,,524288.0,"""We train PanGu-Σ with global batch size of 512 with sequence length of 1024 for each sample""",,,,,,,,,
OPT-175B,Language,Language modelling,Meta AI,"Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",2022-05-02,OPT: Open Pre-trained Transformer Language Models,"https://ai.facebook.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/
https://arxiv.org/abs/2205.01068",1987.0,"Significant use,Highly cited",https://ai.meta.com/blog/opt-175b-large-language-model-applications/,4.3e+23,"https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/final_update.md

""As of yesterday, at 12:46pm PST on January 6, our 175B model finally completed its training run on 300B tokens. This required ~4.30E+23 FLOPs of compute""",,793.5,"4.3*10^23 FLOP / (147 TFLOPS) = 813000 A100-hours
https://www.wolframalpha.com/input?i=4.3*10%5E23+FLOP+%2F+%28147+TFLOPS%29

""As of yesterday, at 12:46pm PST on January 6, our 175B model finally completed its training run on 300B tokens. This required ~4.30E+23 FLOPs of compute, or roughly ~33 days of continuous training on 1024 80GB A100s (assuming no hardware issues, no numerical instabilities, etc.).""",NVIDIA A100 SXM4 80 GB,Industry,"Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3,1 while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models",Confident,Open access (non-commercial),0.0,1024.0,OPT-175B,812544.0,2024-04-01 14:28:10+00:00,Robi Rahman,United States of America,Industry,Industry,135000000000.0,"""The training data contains 180B tokens corresponding to 800 GB of data""

1 token ~ 0.75 words",Self-supervised learning,1.6667,0.47115,,1654082.50447642,,,175000000000.0,"""In line with Meta AI’s commitment to open science, we are sharing Open Pretrained Transformer (OPT-175B), a language model with 175 billion parameters trained on publicly available data sets""",,,,2000000.0,Table 1,,,,Open access (non-commercial),,,,OPT-175B,
BlenderBot 3,Language,Chat,"McGill University,Meta AI,Mila- Quebec AI","Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung, Moya Chen, Kushal Arora, Joshua Lane, Morteza Behrooz, William Ngan, Spencer Poff, Naman Goyal, Arthur Szlam, Y-Lan Boureau, Melanie Kambadur, Jason Weston",2022-08-10,BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage,https://arxiv.org/abs/2208.03188,175.0,SOTA improvement,"""Human evaluations show its superiority to existing open-domain dialogue agents, including its predecessors""",4.3e+23,(taken from OPT-175 base),"Fine-tuned from OPT-175B.

""The fine-tuning data for BB3 comprises roughly 4 million source/target examples spread across the various
training modules. This corresponds to around 1.13B training tokens. When fine-tuning the OPT-based
BB3 models, we additionally included 600k examples ( 170m tokens) of pre-training data to help with
training stability. Table 16 and Table 17 enumerate the breakdown by module.""
",,,NVIDIA A100 SXM4 40 GB,Industry,"We present BlenderBot 3, a 175B parameter dialogue model capable of open-domain conversation with access to the internet and a long-term memory, and having been trained on a large number of user defined tasks. We release both the model weights and code, and have also deployed the model on a public web page to interact with organic users. This technical report describes how the model was built (architecture, model and training scheme), and details of its deployment, including safety mechanisms. Human evaluations show its superiority to existing open-domain dialogue agents, including its predecessors (Roller et al., 2021; Komeili et al., 2022). Finally, we detail our plan for continual learning using the data collected from deployment, which will also be publicly released. The goal of this research program is thus to enable the community to study ever-improving responsible agents that learn through interaction.",Likely,Permissive license (depr.),0.0,128.0,BlenderBot 3,,2024-04-01 15:00:19+00:00,Anonymous,"Canada,United States of America,Canada","Academia,Industry,Academia","Academia,Industry,Academia",,,,,,,,,,175000000000.0,,,,,262144.0,"Note that this is batch size for fine-tuning. Blenderbot is based on OPT-175B which had batch size 2M.

""The 175B model was trained with a batch size of 2^18""
2^18 = 262144",OPT-175B,1.5e+21,,,"""The 30B and 175B parameter BlenderBot 3 models were each trained for one epoch of the training data
on 64 (30B) or 128 (175B) x 40gb A100 GPUs; we found that the model (especially the 175B version)
overfit significantly when seeing the training data more than once. The 175B model was trained with
a batch size of 2^18 and the 30B model was trained with a batch size of 2^19, resulting in roughly 5600
updates and 2800 updates respectively.""

175b params * 5600 * 2^18 * 6 = 1.5e21
",,,,
Llama 2-34B,Language,,Meta AI,"Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez,Robert Stojnic, Sergey Edunov, Thomas Scialom",2023-07-18,Llama 2: Open Foundation and Fine-Tuned Chat Models,"https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/
https://arxiv.org/abs/2307.09288",3131.0,Highly cited,,4.08e+23,"All models sizes trained on 2.0T tokens, per table 1
2T * 34b * 6 = 4.08e23

Also trained on 1038336 A100-hours, which is 3.5e23 at 30% utilization. So the utilization was probably around 35%.","2 trillion tokens of publicly available text, with no text from Meta's products.
""Our training corpus includes a new mix of data from publicly available sources, which does not include data from Meta’s products or services. We made an effort to remove data from certain sites known to contain a high volume of personal information about private individuals. We trained on 2 trillion tokens of data as this
provides a good performance–cost trade-off, up-sampling the most factual sources in an effort to increase knowledge and dampen hallucinations.""",,,NVIDIA A100 SXM4 80 GB,,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned
large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.
Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our
models outperform open-source chat models on most benchmarks we tested, and based on
our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety
improvements of Llama 2-Chat in order to enable the community to build on our work and
contribute to the responsible development of LLMs.",Confident,Unreleased,1.0,,,,2024-04-01 14:35:45+00:00,Anonymous,United States of America,Industry,Industry,,,,1.0,,,,,,34000000000.0,34B,,,,4000000.0,,,,,,,Llama 2 dataset,,,
ViT-22B,Vision,"Object detection,Image classification",Google,"Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, Rodolphe Jenatton, Lucas Beyer, Michael Tschannen, Anurag Arnab, Xiao Wang, Carlos Riquelme, Matthias Minderer, Joan Puigcerver, Utku Evci, Manoj Kumar, Sjoerd van Steenkiste, Gamaleldin F. Elsayed, Aravindh Mahendran, Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bastings, Mark Patrick Collier, Alexey Gritsenko, Vighnesh Birodkar, Cristina Vasconcelos, Yi Tay, Thomas Mensink, Alexander Kolesnikov, Filip Pavetić, Dustin Tran, Thomas Kipf, Mario Lučić, Xiaohua Zhai, Daniel Keysers, Jeremiah Harmsen, Neil Houlsby",2023-02-10,Scaling Vision Transformers to 22 Billion Parameters,https://arxiv.org/abs/2302.05442v1,259.0,SOTA improvement,"""The largest
ViT-22B sets the new SOTA on the challenging ObjectNet test set""",4.0001e+23,"""ViT-22B was trained using 256 visual tokens per image, where each token represents a
14 × 14 patch extracted from 224 × 224 sized images. ViT-22B is trained for 177k steps with batch size of 65k:
approximately 3 epochs""

""ViT-22B was trained on 1024 TPU V4 chips for 177K steps""

256 * 177k * 65k = 3T tokens
6 * 22B * 3T = 3.96e23 ~= 4e23


also, MFU was high:
""Using these techniques, ViT-22B processes 1.15k tokens per second per core during training (forward and
backward pass) on TPUv4 (Jouppi et al., 2020). ViT-22B’s model flops utilization (MFU) (Chowdhery et al.,
2022; Dehghani et al., 2021a) is 54.9%, indicating a very efficient use of the hardware.""","""Dataset. ViT-22B is trained on a version of JFT (Sun et al., 2017), extended to around 4B images (Zhai et al.,
2022a). These images have been semi-automatically annotated with a class-hierarchy of 30k labels""",,,Google TPU v4,,"The scaling of Transformers has driven breakthrough capabilities for language models. At present, the largest large language models (LLMs) contain upwards of 100B parameters. Vision Transformers (ViT) have introduced the same architecture to image and video modelling, but these have not yet been successfully scaled to nearly the same degree; the largest dense ViT contains 4B parameters (Chen et al., 2022). We present a recipe for highly efficient and stable training of a 22B-parameter ViT (ViT-22B) and perform a wide variety of experiments on the resulting model. When evaluated on downstream tasks (often with a lightweight linear model on frozen features), ViT-22B demonstrates increasing performance with scale. We further observe other interesting benefits of scale, including an improved tradeoff between fairness and performance, state-of-the-art alignment to human visual perception in terms of shape/texture bias, and improved robustness. ViT-22B demonstrates the potential for ""LLM-like"" scaling in vision, and provides key steps towards getting there.",Likely,,0.0,,,,2024-04-01 14:02:06+00:00,Anonymous,United States of America,Industry,Industry,4000000000.0,"""Dataset. ViT-22B is trained on a version of JFT (Sun et al., 2017), extended to around 4B images (Zhai et al.,
2022a). These images have been semi-automatically annotated with a class-hierarchy of 30k labels""",,2.9,,,,,,21743000000.0,"21.743B, Table 1",,,,,,,,,,,JFT-4B,,,
Parti,Image generation,Text-to-image,Google Research,"Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, Yonghui Wu",2022-06-22,Scaling Autoregressive Models for Content-Rich Text-to-Image Generation,https://arxiv.org/abs/2206.10789v1,640.0,SOTA improvement,"""Second, we achieve consistent quality improvements by scaling the encoder-decoder Transformer model up to 20B parameters, with a new state-of-the-art zero-shot FID score of 7.23 and finetuned FID score of 3.22 on MS-COCO""",3.962895376192635e+23,"Calculated from architecture. Does not take into account the encoding and decoding of text and images, only the transformer stack.

Table 1 shows for the 20B model
16 encoder layers
64 decoder layers
Dmodel = 4096
Dhidden = 16384
Num heads = 64

Just below table 1:
""We use a maximum length of text tokens of 128, and the length of image tokens are fixed to 1024""

I take the length of the sequence to be 100 for the encoder stack and 1024 for the decoder stack.

Section 3, Training: ""a total
of 450,000 steps and final ratio of 0.025. We use a global batch size of 8192 during training.""",,,,Google TPU v4,Industry,"We present the Pathways Autoregressive Text-to-Image (Parti) model, which generates high-fidelity photorealistic images and supports content-rich synthesis involving complex compositions and world knowledge. Parti treats text-to-image generation as a sequence-to-sequence modeling problem, akin to machine translation, with sequences of image tokens as the target outputs rather than text tokens in another language. This strategy can naturally tap into the rich body of prior work on large language models, which have seen continued advances in capabilities and performance through scaling data and model sizes. Our approach is simple: First, Parti uses a Transformer-based image tokenizer, ViT-VQGAN, to encode images as sequences of discrete tokens. Second, we achieve consistent quality improvements by scaling the encoder-decoder Transformer model up to 20B parameters, with a new state-of-the-art zero-shot FID score of 7.23 and finetuned FID score of 3.22 on MS-COCO. Our detailed analysis on Localized Narratives as well as PartiPrompts (P2), a new holistic benchmark of over 1600 English prompts, demonstrate the effectiveness of Parti across a wide variety of categories and difficulty aspects. We also explore and highlight limitations of our models in order to define and exemplify key areas of focus for further improvements. See https://parti.research.google/ for high-resolution images.",,Unreleased,0.0,,Parti,,2024-04-01 14:30:18+00:00,Robi Rahman,Multinational,Industry,Industry,4800000000.0,,Self-supervised learning,,,True,486659.76703549,,"""For these reasons, we have decided not to release our Parti models, code, or data for public use without further safeguards in place""
https://sites.research.google/parti/",20000000000.0,"Abstract: ""we achieve consistent quality improvements
by scaling the encoder-decoder Transformer model up to 20B parameters""",,,,,,,,Unreleased,Unreleased,,"LAION-400M,FIT400M,JFT-4B",,,
DeepSeek Coder 33B,Language,Code generation,DeepSeek,,2023-11-02,,https://github.com/deepseek-ai/DeepSeek-Coder,,,"SOTA among open-source: ""For coding capabilities, DeepSeek Coder achieves state-of-the-art performance among open-source code models on multiple programming languages and various benchmarks.""",3.96e+23,"""Step 1: Initially pre-trained with a dataset consisting of 87% code, 10% code-related language (Github Markdown and StackExchange), and 3% non-code-related Chinese language. Models are pre-trained using 1.8T tokens and a 4K window size in this step.
Step 2: Further Pre-training using an extended 16K window size on an additional 200B tokens, resulting in foundational models (DeepSeek-Coder-Base).
Step 3: Instruction Fine-tuning on 2B tokens of instruction data, resulting in instruction-tuned models (DeepSeek-Coder-Instruct).""

This means it was trained on 2T tokens. 2T * 33B * 6 = 3.96e23","""Trained from scratch on 2T tokens, including 87% code and 13% linguistic data in both English and Chinese languages.""",,,,,"DeepSeek Coder is composed of a series of code language models, each trained from scratch on 2T tokens, with a composition of 87% code and 13% natural language in both English and Chinese. We provide various sizes of the code model, ranging from 1B to 33B versions. Each model is pre-trained on project-level code corpus by employing a window size of 16K and an extra fill-in-the-blank task, to support project-level code completion and infilling. For coding capabilities, DeepSeek Coder achieves state-of-the-art performance among open-source code models on multiple programming languages and various benchmarks.",Likely,Permissive license (depr.),0.0,,,,2024-03-07 20:22:50+00:00,Anonymous,China,Industry,Industry,,,,,,,,,,33000000000.0,33B,,,,,,,,,,,,,,
StarCoder 2 15B,Language,"Code generation,Code autocompletion","Hugging Face,ServiceNow,NVIDIA,BigCode","Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krauß, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos Muñoz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, Harm de Vries",2024-02-29,StarCoder 2 and The Stack v2: The Next Generation,https://arxiv.org/abs/2402.19173,,,,3.87e+23,estimation is given in Table 6 ,created from repositorites from Github with permissive licences.,,,,,"The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder- 33B is the best-performing model at code completion for high-resource languages, we find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low-resource languages. We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data. ",Likely,Open access (restricted use),0.0,,,,2024-03-27 18:15:23+00:00,Bartosz Podkanowicz,"Multinational,United States of America,United States of America","Industry,Industry,Industry","Industry,Industry,Industry",4100000000000.0,"from Table 7, 
4.1T tokens ",,,,,,,"commercial use allowed, but various use cases restricted: https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement",7000000000.0,7B,,,,,,,,,,,StarCoder v2,,,
GLM-130B,Language,,Tsinghua University,,2022-08-04,GLM-130B: An open bilingual pre-trained model,https://keg.cs.tsinghua.edu.cn/glm-130b/posts/glm-130b/,60.0,SOTA improvement,"""GLM-130B achieves an accuracy of 80.2% on zero-shot LAMBADA (En), while 76.2% for GPT-3 175B and 77.9% for the SOTA offered by PaLM 540B.""",3.778e+23,"""96 NVIDIA A100 (40G * 8) servers for 2 months""

312 TFLOPS/GPU * 96 servers * 8 GPU/server * 2 months * 30% utilization = 3.778*10^23 FLOPhttps://www.wolframalpha.com/input?i=312+teraflops+*+96+*+8+*+2+months+*+30%25

utilization rate - citation from the paper: ""we report hardware FLOPs utilization (HFU) of 43.3% and model FLOPs utilization (MFU) of 32.5% due to re-materialization.""",,1440.0,see compute notes,NVIDIA A100 SXM4 40 GB,Industry,,,Open access (non-commercial),0.0,768.0,GLM-130B,1105920.0,2024-03-28 15:15:57+00:00,Robi Rahman,China,Academia,Academia,,,,1.0,0.433,True,,,non commercial: https://github.com/THUDM/GLM-130B/blob/main/MODEL_LICENSE,130000000000.0,Dense model,,,,,,,,,,,,,GLM-130B,
GLaM,Language,,Google,"Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten Bosma, Zongwei Zhou, Tao Wang, Yu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V Le, Yonghui Wu, Zhifeng Chen, Claire Cui",2021-12-13,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,https://arxiv.org/abs/2112.06905,415.0,SOTA improvement,"""As shown in Table 5, GLaM (64B/64E) is better than the dense model and outperforms the previous finetuned state-of-the-art (SOTA) on this dataset in the open-domain setting""",3.74e+23,"from paper: ""GLaM (64B/64E) training after 600B tokens consumes 456 MWh, about 1/3 of the energy cost of 1287 MWh used by GPT-3. Moreover, to reach similar (and slightly exceeded) scores as GPT-3, we train using 1,024 TPU-v4 chips for 574 hours (with 280B tokens). This consumes 213 MWh or 1/6 of the GPT-3 energy cost""

600/280 is almost exactly 456/213 (2.14) so the later tokens have the same per-token energy cost. 
2.14*574*1024 = 1,257,840 TPU-v4 hours
TPU-v4s are 275 teraFLOP/s. 
Using our usual 0.3 utilization assumption, 275 trillion * 1,257,840 * 3600 * 0.3 = 3.74e23

Later they say they measured 326W power usage per chip, which could maybe be used to estimate utilization.","""To train our model, we build a high-quality dataset of 1.6 trillion tokens that are representative of a wide range of natural language use cases. Web pages constitute the vast quantity of data in our unlabeled dataset. However, their
quality ranges from professional writing to low-quality comment and forum pages.""",1366.0,"Note that they give several energy estimates. Use the complete training figures for 600B tokens, not the GPT-3 comparison values with 280B tokens.

""326W measured system power per TPU-v4 chip""
""The complete GLaM training using 600B tokens consumes only
456 MWh""
1024 TPU v4 chips
(456 MWh) / (326W/chip * 1024 chips) = 1366 hours",Google TPU v4,,,Likely,Unreleased,0.0,1024.0,GLaM,1398784.0,2024-04-01 14:35:28+00:00,Anonymous,United States of America,Industry,Industry,800000000000.0,"The dataset is made of 1.6 trillion tokens, but later in the paper they say they only train the largest model for 600b tokens. 600b / 0.75 words/token = 800b words.",,,,True,,,,1200000000000.0,1.2 trillion parameters,180000000000.0,"180 GFLOPs/token, per Table 1",,1000000.0,"""We use a maximum sequence
length of 1024 tokens, and pack each input example to have
up to 1 million tokens per batch.""",,,Unreleased,Unreleased,,,,,
Jurassic-1-Jumbo,Language,,AI21 Labs,"Opher Lieber, Or Sharir, Barak Lenz, Yoav Shoham",2021-08-11,Jurassic-1: Technical Details and Evaluation,https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf,55.0,,,3.7e+23,see here https://docs.google.com/document/d/1B8x6XYcmB1u6Tmq3VcbAtj5bzhDaj2TcIPyK6Wpupx4/edit,,,,,Industry,"Jurassic-1 is a pair of auto-regressive language models recently released by AI21 Labs, consisting of J1-Jumbo, a 178B-parameter model, and J1-Large, a 7B-parameter model. We describe their architecture and training, and evaluate their performance relative to GPT-3. The evaluation is in terms of perplexity, as well as zero-shot and few-shot learning. To that end, we developed a zeroshot and few-shot test suite, which we made publicly available (https://github.com/ai21labs/ lm-evaluation) as a shared resource for the evaluation of mega language models.",,API access,0.0,,,,2024-03-27 22:23:28+00:00,Robi Rahman,Israel,Industry,Industry,225000000000.0,"""Our model was trained with the conventional self-supervised auto-regressive training objective on 300B tokens drawn from publicly available resources""

1 token ~ 0.75 words",Self-supervised learning,,,,805277.008758257,,,178000000000.0,"""Jurassic-1 models come in two sizes, where the Jumbo version, at 178B parameters, is the largest and most sophisticated language model ever released for general use by developers.""",,,,3200000.0,"""Namely, we used a base learning rate of 1.2 × 10−4 and 0.6 × 10−4 , and a batch size of 2M and 3.2M tokens, for J1-Large and J1-Jumbo, respectively. We also used a linear warm-up over roughly the first 375 million tokens, and gradually increased the batch size from 32K tokens up to its target value for the first few billion tokens.""",,,,,,,,,
LaMDA,Language,Language modelling,Google,"Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, Quoc Le",2022-02-10,LaMDA: Language Models for Dialog Applications,https://arxiv.org/abs/2201.08239,1082.0,Historical significance,,3.55e+23,"""The total FLOPS is 56.5% * 123 TFLOPS/s * 1024 chips * 57.7 days
= 3.55E+23""
From https://arxiv.org/pdf/2201.08239.pdf p.18
","LaMDA's underlying dataset is called 'Infiniset', and besides the dialogue also involves common crawl, wikipedia, a mixture of english and non-english web documents, and data from programming-related sites (so LaMDA models can also dabble in code).",1385.0,57.7 days * 24,Google TPU v3,Industry,"We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model's responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency.",Confident,Unreleased,0.0,1024.0,LaMDA,1418240.0,2024-04-01 14:28:10+00:00,Robi Rahman,United States of America,Industry,Industry,1560000000000.0,"""and are pre-trained on 1.56T words of public dialog data and web text""",Self-supervised learning,,0.565,True,484957.204278073,,,137000000000.0,"""LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters""",,,,256000.0,"""All models were trained with 256K tokens per batch""",,,,,,Infiniset,,,
Yuan 1.0,Language,Language modelling,Inspur,"Shaohua Wu, Xudong Zhao, Tong Yu, Rongguo Zhang, Chong Shen, Hongli Liu, Feng Li, Hong Zhu, Jiangang Luo, Liang Xu, Xuanwei Zhang, Jun Liu",2021-10-12,Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning,https://arxiv.org/abs/2110.04725,40.0,SOTA improvement,"""The zero-shot average scores of both LM and PLM are superior to the SOTA one. On Csldcp, Tnews and Iflytek tasks, we surpass the zero-shot SOTA by a large margin""",3.5380000000001e+23,"Table 9: 4095 petaFLOPS-days which equals 3.538*10^23 FLOP

https://www.wolframalpha.com/input?i=4095+petaFLOPS+*+1+day
","""A Chinese corpus with 5TB high-quality text is built, which is sufficient to train Yuan 245B model without sampling the dataset twice.""",,,,Industry,"Recent work like GPT-3 has demonstrated excellent performance of Zero-Shot and Few-Shot learning on many natural language processing (NLP) tasks by scaling up model size, dataset size and the amount of computation. However, training a model like GPT-3 requires huge amount of computational resources which makes it challengeable to researchers. In this work, we propose a method that incorporates large-scale distributed training performance into model architecture design. With this method, Yuan 1.0, the current largest singleton language model with 245B parameters, achieves excellent performance on thousands GPUs during training, and the state-of-the-art results on NLP tasks. A data processing method is designed to efficiently filter massive amount of raw data. The current largest high-quality Chinese corpus with 5TB high quality texts is built based on this method. In addition, a calibration and label expansion method is proposed to improve the Zero-Shot and Few-Shot performance, and steady improvement is observed on the accuracy of various tasks. Yuan 1.0 presents strong capacity of natural language generation, and the generated articles are difficult to distinguish from the human-written ones.",Confident,API access,0.0,2128.0,Yuan 1.0,,2024-04-01 14:03:41+00:00,Robi Rahman,China,Industry,Industry,1000000000000.0,"""Yuan 1.0 was trained on a new Chinese dataset of 5TB high-quality text that was built on 850TB raw data from Internet.""

1 GB ~ 167M words in English or 333M words in Chinese. For a mixed dataset of mostly Chinese, 5TB may be equivalent to around 1T words.",Self-supervised learning,0.22,0.45,,606364.74789733,,,245730000000.0,"Table 2: Parameters of Yuan models.
""Parameters (billion)""",,,,6881280.0,"Table 2. Batch size 3360, sequence length 2048. 3360*2048 = 6881280",,,,,,,,,
AlphaGo Zero,Games,Go,DeepMind,"D Silver, J Schrittwieser, K Simonyan, I Antonoglou",2017-10-18,Mastering the game of Go without human knowledge,https://www.researchgate.net/publication/320473480_Mastering_the_game_of_Go_without_human_knowledge,8103.0,Highly cited,,3.41e+23,"source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389


AGZ had two models, one of which was small and another of which was large. The compute for AGZ is for the large model, which has 40 residual blocks instead of 20.

A second way of looking at this... we believe multiple TPUs were used for training. 29 million games * 211 moves per game on average * 0.8 seconds per move = 4.8952E+09 seconds of player-time across all TPUs.

4.8952E+09 seconds of player-time / (40 days * 24 * 60 * 60 seconds of real time) ~= 1,416 players

4 TPUs per player => 4.8952E+09 * 4 = 1.95808E+10 TPU-seconds
Total compute = 1.95808E+10 TPU-seconds * 92E+12 FLOP/(TPU-second) * 0.4 = 7.2e23 FLOP

So similar to the Cotra and Davidson estimate (within a factor of 2).",,480.0,,Google TPU v1,Industry,"A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100-0 against the previously published, champion-defeating AlphaGo. © 2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.",,,0.0,,AlphaGo Zero,,2024-04-01 14:03:41+00:00,Robi Rahman,United Kingdom of Great Britain and Northern Ireland,Industry,Industry,5800000000.0,"""Over the course of training, 29 million games of self-play were generated""

Approx 200 moves per Go game on average

https://homepages.cwi.nl/~aeb/go/misc/gostat.html

Thus 200 * 29e6 = 5.8e9",Self-supervised learning,,,,1544149.41763173,,,46400244.0,Quick calculation,,,,,,,,,,,,,,
CodeFuse-13B,Language,Code generation,Ant Group,"Peng Di, Jianguo Li, Hang Yu, Wei Jiang, Wenting Cai, Yang Cao, Chaoyu Chen, Dajun Chen, Hongwei Chen, Liang Chen, Gang Fan, Jie Gong, Zi Gong, Wen Hu, Tingting Guo, Zhichao Lei, Ting Li, Zheng Li, Ming Liang, Cong Liao, Bingchang Liu, Jiachen Liu, Zhiwei Liu, Shaojun Lu, Min Shen, Guangpei Wang, Huan Wang, Zhi Wang, Zhaogui Xu, Jiawei Yang, Qing Ye, Gehao Zhang, Yu Zhang, Zelin Zhao, Xunjin Zheng, Hailian Zhou, Lifu Zhu, Xianying Zhu",2023-10-10,CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model,https://arxiv.org/abs/2310.06266,2.0,,"""The results demonstrate that CodeFuse-13B achieves a HumanEval pass@1 score of 37.10%, positioning it as one of the top multi-lingual code LLMs with similar parameter sizes""",3.3e+23,"""CodeFuse-13B was trained using 512 Nvidia A100 GPU cards, with
a Hardware FLOPs Utilization (HFU) of approximately 60%. The
training process took approximately 40 days to complete""

512 * 312 trillion * 40 * 24 * 3600 * 0.6 = 3.3e23

Using params*tokens, we have 13 billion * 1 trillion * 6 = 7.8e22. might be a sign of multiple epochs? 1T is the size of the dataset; they don't clearly state the number of training tokens","80% code, 10% English, 10% Chinese: ""The pre-training data for CodeFuse consists
of 196TB of code, 1.75TB of Chinese raw data, and 1.7TB of English raw data, totaling 200TB, that are tokenized into 800 billion
tokens of code, 100 billion tokens of Chinese corpus, and 100 billion
tokens of English corpus (see Section 3.1).""",960.0,~40 days,NVIDIA A100 SXM4 80 GB,,"Code Large Language Models (Code LLMs) have gained significant attention in the industry due to their wide applications in the full lifecycle of software engineering. However, the effectiveness of existing models in understanding non-English inputs for multi-lingual code-related tasks is still far from well studied. This paper introduces CodeFuse-13B, an open-sourced pre-trained code LLM. It is specifically designed for code-related tasks with both English and Chinese prompts and supports over 40 programming languages. CodeFuse achieves its effectiveness by utilizing a high quality pre-training dataset that is carefully filtered by program analyzers and optimized during the training process. Extensive experiments are conducted using real-world usage scenarios, the industry-standard benchmark HumanEval-x, and the specially designed CodeFuseEval for Chinese prompts. To assess the effectiveness of CodeFuse, we actively collected valuable human feedback from the AntGroup's software development process where CodeFuse has been successfully deployed. The results demonstrate that CodeFuse-13B achieves a HumanEval pass@1 score of 37.10%, positioning it as one of the top multi-lingual code LLMs with similar parameter sizes. In practical scenarios, such as code generation, code translation, code comments, and testcase generation, CodeFuse performs better than other models when confronted with Chinese prompts.",Likely,Permissive license (depr.),0.0,,,,2024-03-07 20:22:50+00:00,Anonymous,China,Industry,Industry,1000000000000.0,"1T tokens, mostly code but some Chinese/English",,,,,,,,13000000000.0,,,,,16777216.0,"4096 batch size, 4096 sequence length",,,,,,"The Stack,GitHub",,,
Galactica,"Language,Biology",Language modelling,Meta AI,"Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic",2022-11-16,Galactica: A Large Language Model for Science,https://arxiv.org/abs/2211.09085,401.0,SOTA improvement,"""We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3% to 35.7%, and PaLM 540B on MATH""",3.24e+23,"Authors state the model is trained on 450b tokens. Using 6 FLOP/token/parameter, this is 6*120b*450b = 3.24e23","""Our corpus consists of 106 billion tokens from papers, reference material, encyclopedias and other scientific sources. We combine natural language sources, such as papers and textbooks, and natural sequences, such as protein sequences and chemical formulae. We process LATEX where we can capture it, and also include
academic code to capture computational science""",,,NVIDIA A100 SXM4 80 GB,,"Information overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3% to 35.7%, and PaLM 540B on MATH with a score of 20.4% versus 8.8%. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6% and 52.9%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community.",Likely,Open access (non-commercial),0.0,128.0,Galactica,,2024-04-01 14:28:10+00:00,Robi Rahman,United States of America,Industry,Industry,,"""Total dataset size = 106 billion tokens""",Self-supervised learning,4.0,,True,,,,120000000000.0,"""The largest 120B model we train runs on a single NVIDIA A100 node""",,,,2000000.0,"Table 1: batch size 2M, warmup 1.1B (out of 450B tokens)",,,,,,Galactica Corpus,,,
GPT-3 175B (davinci),Language,Text autocompletion,OpenAI,"Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei",2020-05-28,Language models are Few-Shot Learners,https://arxiv.org/abs/2005.14165,23023.0,Highly cited,,3.14e+23,"Table D.1
https://arxiv.org/abs/2005.14165",Table 2.2 (other datasets also used),355.2,14.8 days according to https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf,NVIDIA Tesla V100 DGXS 32 GB,Industry,"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",Confident,API access,0.0,10000.0,GPT-3 175B (davinci),3552000.0,2024-04-01 14:35:28+00:00,Robi Rahman,United States of America,Industry,Industry,374000000000.0,"From table 2.2, we determine that there are 410 + 19 + 12 + 55 + 3 = 499 billion tokens. 

We multiply this by 0.75 to give 374B words. 

3.74e11

========================
[Anson: I think the calculation below doesn't look at all the data, the CommonCrawl data only constitutes 60% of the data. Multiplying by 5/3 gives 4.75e11]

""The CommonCrawl data was downloaded from 41 shards of monthly CommonCrawl covering 2016 to 2019, constituting 45TB of compressed plaintext before filtering and 570GB after filtering, roughly equivalent to 400 billion byte-pair-encoded tokens. ""

Converted to words using 
http://extraconversion.com/data-storage/gigabits/gigabits-to-words.html

2.85e11",Self-supervised learning,0.6,0.2196,True,1131415.12384028,,,175000000000.0,"""we train GPT-3, an autoregressive language model with 175 billion parameters""",740000000000000.0,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,3200000.0,"3.2M, per table 2.1",,,,,,"Common Crawl,OpenWebText2,Wikipedia,Books1,Books2",,GPT-3 175B (davinci),
Luminous-supreme,Language,Language generation,Aleph Alpha,,2022-08-15,Model Card Luminous,https://docs.aleph-alpha.com/docs/introduction/model-card/,,,,2.8e+23,"""~839000h"" GPU-hours on A100s, per Environmental Impact section of model card.

312 trillion * 839000 * 3600 * 0.3 = 2.8e23","""The Luminous family has been trained on a dataset compiled of sources in English, German, French, Spanish and Italian...""

more details in model card 
https://docs.aleph-alpha.com/docs/introduction/model-card/",,,"NVIDIA A100 SXM4 40 GB,NVIDIA A100 SXM4 80 GB",,"The Luminous series is a family of large language models. Large language models are powerful technological tools that can process and produce text. These capabilities emerge during model “training” where the model is exposed to significant amounts of human text data. Similar to a person who deliberately absorbs information while reading a whole library and half of the internet, large language models acquire structural understanding (and not necessarily also knowledge) of language and accumulated information about the world.

The Luminous family currently consists of three vanilla models, which differ in complexity and ability. They are, from the smallest to the largest, luminous-base, luminous-extended and luminous-supreme. All Luminous models are trained in the five most commonly spoken European languages: English, German, French, Italian and Spanish.",Likely,,,,,,2024-03-07 20:22:50+00:00,Anonymous,Germany,Industry,Industry,,,,,,,,,,70000000000.0,"""~70B""",,,,,,,,,,,,,,
LLaMA-33B,Language,Language modelling,Meta AI,"Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample",2023-02-27,LLaMA: Open and Efficient Foundation Language Models,https://arxiv.org/abs/2302.13971,4640.0,,,2.7300000000001e+23,1.4T tokens * 32.5B params * 6 FLOP/token/param = 2.73e+23 FLOP,,,,,,,Unverified,Open access (non-commercial),1.0,,,,2024-04-01 15:00:19+00:00,Robi Rahman,United States of America,Industry,Industry,,,,1.09,,,,,,32500000000.0,Table 2 in the paper,,,,4000000.0,,,,,,,,,LLaMA-33B,
Flamingo,Multimodal,"Visual question answering,Image captioning",DeepMind,"Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, Karen Simonyan",2022-04-29,Flamingo: a Visual Language Model for Few-Shot Learning,https://arxiv.org/abs/2204.14198,1573.0,"Highly cited,SOTA improvement","""For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.""",2.7e+23,"1536 TPU v4 chips for 15 days. Assuming 50% utilization:
C = 1536 TPU * 275*10^12 FLOP/s/TPU * 15 day * 86400 s/day * 0.50 = 2.7*10^23 FLOP

All training and evaluation
was performed on TPUv4 instances. The largest model containing 80 billion parameters is trained on
QUSV chips for 15 days and sharded across 16 devices.

All trained parameters and optimizer accumulators are stored
and updated in float32; all activations and gradients are computed in bfloat16 after downcasting
of parameters from float32 to bfloat16",,360.0,1536 TPU v4 chips for 15 days,Google TPU v4,Industry,"Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.",Likely,Unreleased,0.0,1536.0,Flamingo,552960.0,2024-04-01 14:54:00+00:00,Anonymous,United Kingdom of Great Britain and Northern Ireland,Industry,Industry,,"Flamingo was trained on a mixture of web-scraped datasets:
43M pages of text with interleaved images (MultiModal MassiveWeb dataset)
312M image-text pairs (LTIP dataset)
27M video-text pairs (VTP dataset)
1.8B image-alt text pairs (ALIGN dataset)

Training dataset size is at least 2.1 billion.",Supervised,,,True,,,,80000000000.0,"""We obtain three models, Flamingo-3B, Flamingo-9B and Flamingo-80B""",,,,,,,,,,,"MultiModal MassiveWeb,LTIP,VTP,ALIGN",,,
Whisper v3,Speech,Audio speech recognition,OpenAI,,2023-11-06,,https://huggingface.co/openai/whisper-large-v3,,,"seems not SOTA: ""Our studies show that... accuracy on speech recognition and translation is near the state-of-the-art level""",2.7e+23,"Could derive this in terms of Whisper v1, which according to the paper was trained for 680k hours for between 2-3 epochs. Whisper v3 was trained on 5 million hours for 2 epochs, or ~5-7x as much data, and has the same architecture. We have an estimate of 4.65e22 for Whisper 1.

Assume Whisper v1 was trained on 2.5 epochs, or 2.5*680k = 1.7M hours. Whisper v3 was trained on 10M hours. 10/1.7 * 4.65e22 ~= 2.7e23","""The Whisper large-v3 model is trained on 1 million hours of weakly labeled audio and 4 million hours of pseudolabeled audio collected using Whisper large-v2""",,,,,,Likely,Open source,0.0,,,,2024-03-27 20:46:57+00:00,Anonymous,United States of America,Industry,Industry,60000000000.0,"English audio is roughly 228 wpm: https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.sxcem9l5k3ce

The dataset is multilingual and other languages seem to have lower wpms. So using 200 wpm, we have

200*60*5 million hours = 60,000,000,000 (60B) words",Supervised,2.0,,,,,Apache 2.0,1550000000.0,,,,,,,,,,,,,,,
Gemma 7B,Language,"Language modelling/generation,Chat",Google DeepMind,"Gemma Team, Google DeepMind",2024-02-21,Gemma: Open Models Based on Gemini Research and Technology,https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf,,,,2.52e+23,"6ND aproximation 6*7B*6T = 2.5e23
""Gemma 2B and 7B are trained on 2T and 6T tokens respectively of primarily-English data from web documents, mathematics, and code.""
we can also try to estimate from: ""We estimate the carbon emissions from pretrain-
ing the Gemma models to be ∼ 131 𝑡𝐶𝑂2𝑒𝑞. ""","""Gemma 2B and 7B are trained on 2T and 6T tokens respectively of primarily-English data from web documents, mathematics, and code.""",,,Google TPU v5e,Industry,,Likely,Open source,,4096.0,,,2024-03-27 18:38:28+00:00,Bartosz Podkanowicz,Multinational,Industry,Industry,4500000000000.0,"assuming 0.75 words per token - so 4500000000000.0 words
""Gemma 2B and 7B are trained on 2T and 6T tokens respectively of primarily-English data from web documents, mathematics, and code.""",,,,,,,"https://ai.google.dev/gemma/terms

no illegal use or abuse",7751248896.0,table 2,14000000000.0,2N,,,,,,Unreleased,Unreleased,,,,,
Skywork-13B,Language,Language modelling,Kunlun Inc.,"Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng, Weiwei Lü, Rui Hu, Chenxia Li, Liu Yang, Xilin Luo, Xuejie Wu, Lunan Liu, Wenjun Cheng, Peng Cheng, Jianhao Zhang, Xiaoyu Zhang, Lei Lin, Xiaokun Wang, Yutuan Ma, Chuanhai Dong, Yanqi Sun, Yifu Chen, Yongyi Peng, Xiaojuan Liang, Shuicheng Yan, Han Fang, Yahui Zhou",2023-10-30,Skywork: A More Open Bilingual Foundation Model,https://arxiv.org/abs/2310.19341,28.0,SOTA improvement,"""We show that our model not only excels on popular benchmarks, but also achieves state of the art performance in Chinese language modeling on diverse domains""",2.5e+23,"""Our Skywork-13B is trained on a cluster of 64 NVIDIA-HGX-A800 nodes, a total of 512 A800-80G SXM GPUs... The training process of Skywork-13B spanned a total of 39 days.""

They note that ""we achieved a token throughput of 1873 per GPU per second and a model flops utilization (MFU) of 56.5%... "". 

""MFU"" was coined in the Palm paper (https://arxiv.org/pdf/2204.02311.pdf) and only counts operations used to train the model, not all operations observed on the hardware. MFU is lower than traditionally measured utilization.

Using the 56.5% number, and a peak tensor performance of 623.8 TFLOPS for the A800, this suggests 512 * 623.8 TFLOPS * 39 days * 86400 seconds/day * 0.565 = 6.08e23 FLOP.

Based on C=6ND, with 13B parameters and 3.2T tokens, we have C=6*(13B)*(3.2T)=2.5e23 FLOP.

Since the reported MFU is quite high, and would imply a higher compute usage than 6ND, it seems they may have trained on mixed precision and with the GPUs not always operating in the 623.8 TFLOPS mode.","""In order to train Skywork-13B, we build SkyPile, a vast, high quality corpus comprising more than 6 trillion tokens. A segment of the corpus, comprising over 150 billion tokens of web text, has been open sourced to facilitate research and training on Chinese LLMs""",940.0,39 days,NVIDIA A800,,"In this technical report, we present Skywork-13B, a family of large language models (LLMs) trained on a corpus of over 3.2 trillion tokens drawn from both English and Chinese texts. This bilingual foundation model is the most extensively trained and openly published LLMs of comparable size to date. We introduce a two-stage training methodology using a segmented corpus, targeting general purpose training and then domain-specific enhancement training, respectively. We show that our model not only excels on popular benchmarks, but also achieves state of the art performance in Chinese language modeling on diverse domains. Furthermore, we propose a novel leakage detection method, demonstrating that test data contamination is a pressing issue warranting further investigation by the LLM community. To spur future research, we release Skywork-13B along with checkpoints obtained during intermediate stages of the training process. We are also releasing part of our SkyPile corpus, a collection of over 150 billion tokens of web text, which is the largest high quality open Chinese pre-training corpus to date. We hope Skywork-13B and our open corpus will serve as a valuable open-source resource to democratize access to high-quality LLMs.",Likely,Permissive license (depr.),0.0,512.0,Skywork-13B,,2024-04-01 14:03:42+00:00,Anonymous,China,Industry,Industry,2780000000000.0,"The full SkyPile dataset is 6 trillion tokens, roughly half English and half Chinese: (https://huggingface.co/Skywork/Skywork-13B-base).

The model is trained for the equivalent of 0.53 epochs on the full dataset, or 3.18 trillion unique tokens. This is around 2.78 trillion words, based on an average of 1 word/token for the Chinese portion and 0.75 word/token on the English portion.",,1.0,0.46,,,,,13000000000.0,13B,,,,16000000.0,Table 3,,,,,,SkyPile,,,
Qwen-14B,Language,,Alibaba,"Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, Tianhang Zhu",2023-09-28,Qwen Technical Report,https://arxiv.org/abs/2309.16609,169.0,,,2.5e+23,"3T tokens per Table 1

14b*3T*6 = 2.5e23","""Our dataset is designed to meet these requirements and includes public
web documents, encyclopedia, books, codes, etc. Additionally, our dataset is multilingual, with a
significant portion of the data being in English and Chinese.""",,,,,"Large language models (LLMs) have revolutionized the field of artificial intelligence, enabling natural language processing tasks that were previously thought to be exclusive to humans. In this work, we introduce Qwen, the first installment of our large language model series. Qwen is a comprehensive language model series that encompasses distinct models with varying parameter counts. It includes Qwen, the base pretrained language models, and Qwen-Chat, the chat models finetuned with human alignment techniques. The base language models consistently demonstrate superior performance across a multitude of downstream tasks, and the chat models, particularly those trained using Reinforcement Learning from Human Feedback (RLHF), are highly competitive. The chat models possess advanced tool-use and planning capabilities for creating agent applications, showcasing impressive performance even when compared to bigger models on complex tasks like utilizing a code interpreter. Furthermore, we have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, as well as mathematics-focused models, Math-Qwen-Chat, which are built upon base language models. These models demonstrate significantly improved performance in comparison with open-source models, and slightly fall behind the proprietary models.",Likely,Permissive license (depr.),1.0,,,,2024-04-01 14:02:06+00:00,Anonymous,China,Industry,Industry,,,,,,,,,,14000000000.0,14B,,,,4000000.0,Table 1,,,,,,,,,
Granite 13B,Language,Chat,IBM,,2023-11-30,Granite Foundation Models,https://www.ibm.com/downloads/cas/X9W4O6BM,,,"Possible significant use - it is (one of) the foundation models behind IBM's ""watsonx"" product, alongside open models like Llama 2

https://www.ibm.com/products/watsonx-ai",2.44e+23,"Estimate using hardware:

""Granite.13b.v1 used 256 A100 GPUs for 1056 hours and 120 TFLOPs.
Granite.13b.v2 was trained on the same infrastructure for an
additional 1152 hours with 120 TFLOPS, bringing the total to
2208 hours""

Seems like 120 TFLOPS is the output per GPU after utilization, though they don't explicitly explain that part. That's 38% utilization.

256 * 2208 * 3600 * 120 TFLOPS = 2.44e23

Using 6ND:

""The second version of the granite.13b models leverages an updated base model trained on 2.5T trillion tokens.""

""The granite.13b.v1 base model is trained for 300K iterations,
with a batch size of 4M tokens, for a total of 1.25 trillion
5 tokens. The granite.13b.v2 base model continued pre-training
on top of the granite.13b.v1 checkpoint for an additional 300K
iterations and a total of 2.5 trillion tokens.""

2.5T * 13B * 6 = 1.95e23","""To support the training of large enterprise-grade foundation
models, including granite.13b, IBM curated a massive dataset
of relevant unstructured language data from sources across
academia, the internet, enterprise (e.g., financial, legal), and
code.""

More breakdowns in paper, 20 sources in total",2208.0,"""Granite.13b.v1 used 256 A100 GPUs for 1056 hours and 120 TFLOPs. Granite.13b.v2 was trained on the same infrastructure for an
additional 1152 hours with 120 TFLOPS, bringing the total to
2208 hours""",NVIDIA A100,,"We introduce the Granite series of decoder-only foundation models for generative artificial intelligence (AI) tasks that
are ready for enterprise use. We report on the architecture,
capabilities, underlying data and data governance, training algorithms, compute infrastructure, energy and carbon footprint,
testing and evaluation, socio-technical harms and mitigations,
and usage policies.",Likely,API access,0.0,,,,2024-03-20 14:28:38+00:00,Anonymous,United States of America,Industry,Industry,1875000000000.0,"2.5T tokens, 1.875T words at 0.75 words/token",,1.0,,,,,,13000000000.0,13 billion,,,,,,,,,,,"Common Crawl,arXiv,many others",,,
Falcon-40B,Language,Language modelling,Technology Innovation Institute,,2023-03-15,Abu Dhabi-based Technology Innovation Institute Introduces Falcon LLM: Foundational Large Language Model (LLM) outperforms GPT-3 with 40 Billion Parameters,https://arxiv.org/abs/2311.16867; https://www.tii.ae/news/abu-dhabi-based-technology-innovation-institute-introduces-falcon-llm-foundational-large,0.0,Historical significance,,2.4e+23,C = 6ND = 6 * 40B * 1000B = 2.4e+23 FLOP (assuming one epoch),"Falcon-40B was trained on 1,000B tokens of RefinedWeb, a high-quality filtered and deduplicated web dataset which we enhanced with curated corpora. Significant components from our curated copora were inspired by The Pile (Gao et al., 2020).",1440.0,"""Falcon-40B was trained on AWS SageMaker, on 384 A100 40GB GPUs in P4d instances.""
""Training started in December 2022 and took two months.""",NVIDIA A100,Academia,,Confident,Open source,0.0,384.0,Falcon-40B,552960.0,2024-04-02 13:14:56+00:00,Anonymous,United Arab Emirates,Government,Government,750000000000.0,1000B tokens ~= 750B words,,,0.3864,True,,,apache 2.0,40000000000.0,Model comes in 7B and 40B variants.,80000000000.0,80B FLOP per token,,2359296.0,"Batch size 1152 (presumably sequences) per Table 16. Warmed up using smaller batches for first 100B tokens.

""All Falcon models are pretrained with a 2,048 sequence length""

https://arxiv.org/pdf/2311.16867.pdf
",,,Unreleased,Unreleased,,RefinedWeb,,,
Nanbeige-16B,Language,Chat,Nanbeige LLM Lab,,2023-11-01,,https://github.com/Nanbeige/Nanbeige/blob/main/README_EN.md,,,"a little worse than Qwen on most metrics, and well short of GPT-4, ofc",2.4e+23,"""It uses 2.5T Tokens for pre-training"". I think that's the number of tokens the model was trained on, not the dataset size, but I'm not sure.

16 billion * 2.5 trillion * 6 = 2.4e23","""The training data includes a large amount of high-quality internet corpus, various books, code, etc""",,,,,"Nanbeige-16B is a 16 billion parameter language model developed by Nanbeige LLM Lab. It uses 2.5T Tokens for pre-training. The training data includes a large amount of high-quality internet corpus, various books, code, etc. It has achieved good results on various authoritative evaluation data sets. This release includes the Base, Chat, Base-32k and Chat-32k.",Likely,Permissive license (depr.),0.0,,,,2024-03-07 20:22:50+00:00,Anonymous,China,Industry,Industry,,,,,,,,,,16000000000.0,16 billion,,,,,,,,,,,,,,
LightOn Mini,Language,"Language modelling/generation,Chat",LightOn,,2023-03-21,LightOn's Large Language Model of 40 billion parameters: MINI,https://www.lighton.ai/blog/lighton-s-blog-4/lighton-s-large-language-model-of-40-billion-parameters-mini-19,,,,2.4e+23,6ND aproximation: 6*40B*1T = 2.4e23,"""The amount of data in Mini corpus is 1 trillion tokens. We mainly used data from the public web to pre-train our model, with strong filtering, toxicity reduction, and deduplication to ensure that only high-quality data is retained.""",,,,,,Likely,,,,,,2024-03-07 20:22:50+00:00,Bartosz Podkanowicz,France,Industry,Industry,750000000000.0,"""The amount of data in Mini corpus is 1 trillion tokens. We mainly used data from the public web to pre-train our model, with strong filtering, toxicity reduction, and deduplication to ensure that only high-quality data is retained.""  assuming 0.75 words per token - 750000000000.0 words",Self-supervised learning,,,,,,,40000000000.0,"""Boasting an impressive 40 billion parameters, Mini is a formidable addition to the growing array of language models available in the market today.""",80000000000.0,2N,,,,,,,,,,,,
BloombergGPT,Language,Language modelling,"Bloomberg,Johns Hopkins University","Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, Gideon Mann",2023-03-30,BloombergGPT: A Large Language Model for Finance,https://arxiv.org/abs/2303.17564,303.0,SOTA improvement,"""We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general LLM benchmarks.""",2.36e+23,"2.36e23 per Table 4

(using our usual hardware method, 512 A100s over 53 days would be 512 * 312 teraFLOP/s * 53 * 24 * 3600 * 0.3 = 2.19e23)","""To train BloombergGPT, we construct “FinPile”, a comprehensive dataset consisting of a range of English financial documents including news, filings, press releases, web-scraped financial documents, and social media drawn from the Bloomberg archives. These documents have been acquired through our business process over the past two decades. We augment FinPile with public data widely used to train LLMs. The result is a training corpus that is roughly half domain-specific text and half general-purpose text.""",1270.0,"""~53 days""",NVIDIA A100,,"The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg's extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general LLM benchmarks. Additionally, we explain our modeling choices, training process, and evaluation methodology. We release Training Chronicles (Appendix C) detailing our experience in training BloombergGPT.",Confident,Unreleased,0.0,512.0,BloombergGPT,650240.0,2024-04-01 15:00:19+00:00,Anonymous,"United States of America,United States of America","Industry,Academia","Industry,Academia",532000000000.0,"708.9 billion tokens. At 0.75 English words per token, that's 532B words",Self-supervised learning,0.8,0.32,True,,,,50558868480.0,,,,,4200000.0,"""in the first 7,200 steps, we use a batch size of 1,024 (2.1M tokens), then switch to a batch size of 2,048 (4.2M tokens) for the remainder of training.""",,,Unreleased,Unreleased,,,,,
YaLM,Language,Language modelling,Yandex,,2022-06-23,Yandex Publishes YaLM 100B. It’s the Largest GPT-Like Neural Network in Open Source,https://medium.com/yandex/yandex-publishes-yalm-100b-its-the-largest-gpt-like-neural-network-in-open-source-d1df53d0e9a6,0.0,,,2.2e+23,"""It took us 65 days to train the model on a pool of 800 A100 graphics cards and 1.7 TB of online texts, books, and countless other sources.""",,1560.0,65 days,NVIDIA A100,Industry,,Likely,Open source,0.0,800.0,,1248000.0,2024-03-28 21:01:05+00:00,Robi Rahman,Russia,Industry,Industry,300000000000.0,"1.7TB of data 300B tokens – from github https://github.com/yandex/YaLM-100B
I've assumed that 1 token correspond to 1 word in russian language.",Self-supervised learning,,,True,,,apache 2.0,100000000000.0,100B,,,,,,,,Unreleased,,,,,,
AlexaTM 20B,Language,Language modelling,Amazon,"Saleh Soltan, Shankar Ananthakrishnan, Jack FitzGerald, Rahul Gupta, Wael Hamza, Haidar Khan, Charith Peris, Stephen Rawls, Andy Rosenbaum, Anna Rumshisky, Chandana Satya Prakash, Mukund Sridhar, Fabian Triefenbach, Apurv Verma, Gokhan Tur, Prem Natarajan",2022-08-02,AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model,https://arxiv.org/abs/2208.01448,63.0,SOTA improvement,The Abstract reports SOTA improvement on multiple benchmarks.,2.04374016e+23,"Training throughput is reported as 154 TFLOP/s - see p.5 of the paper.
""We relied on an internal and optimized version of DeepSpeed that we have since open-sourced (Chiu & Zheng, 2022) to obtain training throughput of up to 154 TFLOPS/GPU on 16 AWS p4d.24xlarge compute instances.""

Accelerator compute days are reported as 15,360 days - see Table 17 on p.18 of the paper.",See Table 2 on p.3 of the paper.,2880.0,"See p.5 of the paper: ""We trained AlexaTM 20B for 120 days on 128 A100 GPUs...""",NVIDIA A100,Industry,"In this work, we demonstrate that multilingual large-scale sequence-to-sequence (seq2seq) models, pre-trained on a mixture of denoising and Causal Language Modeling (CLM) tasks, are more efficient few-shot learners than decoder-only models on various tasks. In particular, we train a 20 billion parameter multilingual seq2seq model called Alexa Teacher Model (AlexaTM 20B) and show that it achieves state-of-the-art (SOTA) performance on 1-shot summarization tasks, outperforming a much larger 540B PaLM decoder model. AlexaTM 20B also achieves SOTA in 1-shot machine translation, especially for low-resource languages, across almost all language pairs supported by the model (Arabic, English, French, German, Hindi, Italian, Japanese, Marathi, Portuguese, Spanish, Tamil, and Telugu) on Flores-101 dataset. We also show in zero-shot setting, AlexaTM 20B outperforms GPT3 (175B) on SuperGLUE and SQuADv2 datasets and provides SOTA performance on multilingual tasks such as XNLI, XCOPA, Paws-X, and XWinograd. Overall, our results present a compelling case for seq2seq models as a powerful alternative to decoder-only models for Large-scale Language Model (LLM) training. ",Confident,API access,0.0,128.0,AlexaTM 20B,368640.0,2024-04-01 14:45:25+00:00,Robi Rahman,United States of America,Industry,Industry,,,,,0.4935,,,,https://aws.amazon.com/about-aws/whats-new/2022/11/alexatm-20b-model-available-sagemaker-jumpstart/?nc1=h_ls,19750000000.0,See Table 1 on p.3 of the paper,,,Amazon Web Services,2000000.0,"""We trained AlexaTM 20B for 120 days on 128 A100 GPUs for the total of 500k updates with the accumulated batch size of 2 million tokens""",,,,,,"mC4,Wikipedia",,,
Baichuan2-13B,Language,Chat,Baichuan,"Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai, Guosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji, Jian Xie, JunTao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma, Mang Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Peidong Guo, Ruiyang Sun, Tao Zhang, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong Zeng, Xiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yanjun Shen, Yiding Wang, Yiyu Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou, Zhiying Wu",2023-09-06,Baichuan 2: Open Large-scale Language Models,"https://huggingface.co/baichuan-inc/Baichuan2-13B-Base, https://arxiv.org/abs/2309.10305",,,,2.03e+23,"They describe the dataset as having 2.6T tokens, but the checkpoint graph makes it clear that's also the number of tokens the model was trained on.

13b * 2.6t * 6 = 2.03e23","2.6 trillion tokens, bilingual.

paper/model card don't give breakdown between English and Chinese",,,,,,Likely,Permissive license (depr.),0.0,,,,2024-04-02 13:21:22+00:00,Anonymous,China,Industry,Industry,2300000000000.0,"2.6T tokens, or ~2.3T words assuming that the dataset is roughly even English (0.75 words/token) and Chinese (1 word/token)

1.3T Chinese tokens * (1 word/token) = 1.3T Chinese words
1.3T English tokens * (0.75 words/token) = 0.975T English words
total: 2.275T, or ~2.3T",,1.0,,,,,,13000000000.0,,,,,,,,,,,,,,,
MPT-30B,Language,"Language generation,Code generation",MosaicML,,2023-06-22,,https://huggingface.co/mosaicml/mpt-30b,,,,1.8e+23,30b * 1T tokens * 6 = 1.8e23,among others,,,"NVIDIA A100 SXM4 40 GB,NVIDIA H100 SXM5",,"MPT-30B is a decoder-style transformer pretrained from scratch on 1T tokens of English text and code. This model was trained by MosaicML.

MPT-30B is part of the family of Mosaic Pretrained Transformer (MPT) models, which use a modified transformer architecture optimized for efficient training and inference.

MPT-30B comes with special features that differentiate it from other LLMs, including an 8k token context window (which can be further extended via finetuning; see MPT-7B-StoryWriter), support for context-length extrapolation via ALiBi, and efficient inference + training via FlashAttention. It also has strong coding abilities thanks to its pretraining mix. MPT models can also be served efficiently with both standard HuggingFace pipelines and NVIDIA's FasterTransformer. The size of MPT-30B was also specifically chosen to make it easy to deploy on a single GPU—either 1xA100-80GB in 16-bit precision or 1xA100-40GB in 8-bit precision.",Likely,Open source,0.0,,,,2024-03-28 16:56:14+00:00,Anonymous,United States of America,Industry,Industry,3000000000000.0,"~4T tokens across sources, or 3T words at 0.75 words/token (ignoring the fact that some of the data is code)",,0.25,,,,,apache 2.0,30000000000.0,30b,,,,4096000.0,"last two batch sizes were 3,456,000 and 4,096,000, but 4,096,000 only used for last 5% of training

""To build 8k support into MPT-30B efficiently, we first pre-trained on 1T tokens using sequences that were 2k tokens long, and then trained for an additional 50B tokens using sequences that were 8k tokens long...

The model was trained in three stages using the MosaicML Platform: (i) First it was trained on 440 A100-40GBs with a batch size of 1760. (ii) Then, on 216 A100-40GBs with a batch size of 1728. (iii) Training was completed on 256 H100-80GBs with a batch size of 512 with 8k context length and 50B tokens""


",,,,,,"mC4,C4,RedPajama,The Stack",,,
Nemotron-3-8B,Language,"Chat,Language generation",NVIDIA,,2023-11-15,NVIDIA AI Foundation Models: Build Custom Enterprise Chatbots and Co-Pilots with Production-Ready LLMs,https://developer.nvidia.com/blog/nvidia-ai-foundation-models-build-custom-enterprise-chatbots-and-co-pilots-with-production-ready-llms/,,SOTA improvement,"""The Nemotron-3-8B-QA model offers state-of-the-art performance, achieving a zero-shot F1 score of 41.99% on the Natural Questions dataset. This metric measures how closely the generated answer resembles the truth in ‌QA. """,1.8e+23,"https://huggingface.co/nvidia/nemotron-3-8b-base-4k

""This model was trained on a dataset containing 3.8 Trillion tokens of text""

8 billion * 3.8 trillion * 6 = 1.8e23

Also, using the hardware method: ""1,024 A100s were used for 19 days to train the model.""

19*1024 * 312 trillion * 24 * 3600 * 0.3 = 1.57e23","""NVIDIA models are trained on a diverse set of public and proprietary datasets. This model was trained on a dataset containing 3.8 Trillion tokens of text. The dataset contains 53 different human languages (including English, German, Russian, Spanish, French, Japanese, Chinese, Italian, and Dutch) and 37 programming languages. The model also uses the training subsets of downstream academic benchmarks from sources like FLANv2, P3, and NaturalInstructions v2""",456.0,19 days,NVIDIA A100,,"Large language models (LLMs) are revolutionizing data science, enabling advanced capabilities in natural language understanding, AI, and machine learning. Custom LLMs, tailored for domain-specific insights, are finding increased traction in enterprise applications.

The NVIDIA Nemotron-3 8B family of foundation models is a powerful new tool for building production-ready generative AI applications for the enterprise–fostering innovations ranging from customer service AI chatbots to cutting-edge AI products.",Likely,Open access (restricted use),0.0,1024.0,Nemotron-3-8B,,2024-03-27 20:40:25+00:00,Anonymous,United States of America,Industry,Industry,,,,,0.34,,,,"can't use to train other models:

https://developer.download.nvidia.com/ai-foundation-models/nvidia-ai-foundation-models-license-10Nov2023.pdf",8000000000.0,,,,,,,,,,,,,,,
Llama 2-13B,Language,Language modelling,Meta AI,"Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez,Robert Stojnic, Sergey Edunov, Thomas Scialom
",2023-07-18,Llama 2: Open Foundation and Fine-Tuned Chat Models,"https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/
https://arxiv.org/abs/2307.09288",3131.0,"Historical significance,Significant use,Highly cited",Model has been open-sourced and frequently downloaded. The paper claims that Llama 2 is the current best open-source chat model as of its release date.,1.6e+23,13 billion * 2 trillion * 6 = 1.6e23,"2 trillion tokens of publicly available text, with no text from Meta's products.
""Our training corpus includes a new mix of data from publicly available sources, which does not include data from Meta’s products or services. We made an effort to remove data from certain sites known to contain a high volume of personal information about private individuals. We trained on 2 trillion tokens of data as this
provides a good performance–cost trade-off, up-sampling the most factual sources in an effort to increase knowledge and dampen hallucinations.""",,,NVIDIA A100 SXM4 80 GB,Industry,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",Confident,Open access (restricted use),1.0,,,,2024-04-01 14:35:45+00:00,Epoch AI,United States of America,Industry,Industry,1500000000000.0,2 trillion tokens ~= 1.5 trillion words,Supervised,1.0,,,,,"Llama 2 license. can't use outputs to train models.

https://github.com/meta-llama/llama/blob/main/LICENSE",13000000000.0,"Llama has been released in 7B, 13B, and 70B variants.",,,,4000000.0,,,,,,,Llama 2 dataset,Meta’s Research Super Cluster,,
SparseOPT-175B,Language,,"Institute of Science and Technology Austria (ISTA),Neural Magic","Elias Frantar, Dan Alistarh",2023-01-02,SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot,https://arxiv.org/abs/2301.00774,188.0,,,1.58e+23,,,,,,,,,,0.0,,,,2024-04-01 14:35:45+00:00,Robi Rahman,"Austria,United States of America","Academia,Industry","Academia,Industry",,,,1.67,,,,,,87500000000.0,,,,,,,,,,,,,,SparseOPT-175B,
AlphaCode,Language,Code generation,DeepMind,"Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d'Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, Oriol Vinyals",2022-02-02,Competition-Level Code Generation with AlphaCode,https://arxiv.org/abs/2203.07814,688.0,SOTA improvement,,1.568160000001e+23,"Figure 7 (a) shows a maximum training compute budget of approx 20000 TPU-days per model.
20000 days * 275 TFLOPS * 0.33 utilization = 1.6e23 FLOP
https://www.wolframalpha.com/input?i=20000+*+275+teraFLOPS+*+1+day+*+0.33",,,,"Google TPU v4,Google TPU v4i",Industry,"Programming is a powerful and ubiquitous problem-solving tool. Developing systems that can assist programmers or even generate programs independently could make programming more productive and
accessible, yet so far incorporating innovations in AI has proven challenging. Recent large-scale language models have demonstrated an impressive ability to generate code, and are now able to complete
simple programming tasks. However, these models still perform poorly when evaluated on more complex, unseen problems that require problem-solving skills beyond simply translating instructions into
code. For example, competitive programming problems which require an understanding of algorithms
and complex natural language remain extremely challenging. To address this gap, we introduce AlphaCode, a system for code generation that can create novel solutions to these problems that require deeper
reasoning. In simulated evaluations on recent programming competitions on the Codeforces platform,
AlphaCode achieved on average a ranking of top 54.3% in competitions with more than 5,000 participants. We found that three key components were critical to achieve good and reliable performance:
(1) an extensive and clean competitive programming dataset for training and evaluation, (2) large and
efficient-to-sample transformer-based architectures, and (3) large-scale model sampling to explore the
search space, followed by filtering based on program behavior to a small set of submissions.
",,,0.0,3750.0,AlphaCode,,2024-04-01 14:52:16+00:00,Robi Rahman,United Kingdom of Great Britain and Northern Ireland,Industry,Industry,,Appendix part A has answers for pretraining.,Self-supervised learning,,,True,,,,41100000000.0,41.1B. Table 3,,,,4718592.0,"2304 token sequences, 2048 batch size. 2304 * 2048 = 4718592

trained on 967B tokens and 205k steps. 967B/205k = 4717073, so seems they didn't do warmup",,,,,,,,,
StarCoder 2 7B,Language,"Code generation,Code autocompletion","Hugging Face,ServiceNow,NVIDIA,BigCode","Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krauß, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos Muñoz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, Harm de Vries",2024-02-29,StarCoder 2 and The Stack v2: The Next Generation,https://arxiv.org/abs/2402.19173,,,,1.55e+23,estimation is given in Table 6 ,created from repositorites from Github with permissive licences.,,"""A cumulative of 145,152 hours of computation was performed on hardware of type H100""",NVIDIA H100 SXM5,,"The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder- 33B is the best-performing model at code completion for high-resource languages, we find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low-resource languages. We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data. ",Likely,Open access (restricted use),1.0,,,,2024-03-27 18:14:48+00:00,Bartosz Podkanowicz,"Multinational,United States of America,United States of America","Industry,Industry,Industry","Industry,Industry,Industry",3500000000000.0,"from Table 7, 
3.5T tokens ",,,,,,,https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement,15000000000.0,15B,,,,,,,,,,,StarCoder v2,,,
Poro34B (700B token checkpoint),Language,"Code generation,Language modelling/generation","High-Performance Language Technologies (HPLT),University of Turku",,2023-12-14, Poro 34B Model Card ,https://huggingface.co/LumiOpen/Poro-34B,,,,1.53e+23,"6ND = 6*0.75T*34B= 153000000000000000000000
""Poro is a 34B parameter decoder-only transformer pretrained on Finnish, English and code. It is being trained on 1 trillion tokens (700 billion as of this release). Poro is a fully open source model and is made available under the Apache 2.0 License.""","""The Finnish dataset is a combination of many Finnish resources:

    Finnish Internet Parsebank
    mC4 multilingual colossal, cleaned Common Crawl
    Common Crawl Finnish
    Finnish Wikipedia
    Lönnrot Projekti Lönnrot
    Suomi24 The Suomi 24 Corpus 2001-2020
    Reddit r/Suomi submissions and comments
    STT Finnish News Agency Archive 1992-2018
    Yle Finnish News Archive 2011-2018
    Yle Finnish News Archive 2019-2020
    Yle News Archive Easy-to-read Finnish 2011-2018
    Yle News Archive Easy-to-read Finnish 2019-2020""
""

""Poro is a 34B parameter decoder-only transformer pretrained on Finnish, English and code. It is being trained on 1 trillion tokens (700 billion as of this release). Poro is a fully open source model and is made available under the Apache 2.0 License.""",,,AMD Instinct MI250X,,,Likely,Open source,,512.0,,,2024-03-27 18:55:44+00:00,Anonymous,Finland,Academia,Academia,750000000000.0,"1T tokens, assuming 0.75 word per token
""Poro is a 34B parameter decoder-only transformer pretrained on Finnish, English and code. It is being trained on 1 trillion tokens (700 billion as of this release). Poro is a fully open source model and is made available under the Apache 2.0 License.""",,,,,,,Apache 2.0,34000000000.0,"""Poro is a 34B parameter decoder-only transformer pretrained on Finnish, English and code. It is being trained on 1 trillion tokens (700 billion as of this release). Poro is a fully open source model and is made available under the Apache 2.0 License.""",,,,,,,,,,,mC4,,,
AlphaGo Master,Games,Go,DeepMind,"D Silver, J Schrittwieser, K Simonyan, I Antonoglou",2017-01-01,Mastering the game of Go without human knowledge,https://www.researchgate.net/publication/320473480_Mastering_the_game_of_Go_without_human_knowledge,8103.0,Highly cited,,1.5e+23,"This is a guess. There was no single journal publication that accompanied this model, that gave information about architecture/model training time etc. All I could find was that it has the same architecture as AlphaGo Zero, and that it had roughly the same power consumption as AGZ. See for instance: 
https://deepmind.com/blog/article/alphago-zero-starting-scratch

Since AGZ reaches the ELO of AlphaGo Master in about 20 days (half of the total training time), I estimate the compute to be around half that of AGZ. I round this down to 1.5e23, and I expect this to only be accurate within an OOM.",,,,Google TPU v1,Industry,"A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100-0 against the previously published, champion-defeating AlphaGo. © 2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.",,,0.0,,AlphaGo Master,,2024-04-01 14:03:40+00:00,Robi Rahman,United Kingdom of Great Britain and Northern Ireland,Industry,Industry,,,,,,,852748.081042578,,,,,,,,,,,,,,,,,,
HyperCLOVA,Language,,"NAVER,Search Solutions","Boseop Kim, HyoungSeok Kim, Sang-Woo Lee, Gichang Lee, Donghyun Kwak, Dong Hyeon Jeon, Sunghyun Park, Sungju Kim, Seonhoon Kim, Dongpil Seo, Heungsub Lee, Minyoung Jeong, Sungjae Lee, Minsub Kim, Suk Hyun Ko, Seokhun Kim, Taeyong Park, Jinuk Kim, Soyoung Kang, Na-Hyeon Ryu, Kang Min Yoo, Minsuk Chang, Soobin Suh, Sookyo In, Jinseong Park, Kyungduk Kim, Hiun Kim, Jisu Jeong, Yong Goo Yeo, Donghoon Ham, Dongju Park, Min Young Lee, Jaewook Kang, Inho Kang, Jung-Woo Ha, Woomyoung Park, Nako Sung",2021-09-10,What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers,https://arxiv.org/abs/2109.04650,88.0,SOTA improvement,"""HyperCLOVA with our training configuration shows state-of-the-art in-context zero-shot and few-shot learning performances on various downstream tasks in Korean""",1.476e+23,"""For experiments in Section 4, the model trained with 150B is used for fair comparison, because not all models are finished training at the same iteration. However, experiments in Section 5.2 use the model trained with 300B tokens, as HyperCLOVA Studio provided the 39B and 82B models trained with 300B tokens.""

82e9 connections * 2 FLOP/connection * 300e9 tokens * 3 backward pass = 1.476e23 FLOP

Calculation using GPU time corroborates this:
- ""Our model is based on megatron-LM (Shoeybi et al., 2019) and trained on the NVIDIA Superpod, which includes 128 strongly clustered DGX servers with 1,024 A100 GPUs.""
- ""It takes 13.4 days to train a model with 82B parameters with 150B tokens."" Assume 300B tokens takes twice as long, 26.8 days.
- Assume the default of 30% utilization rate for large language models.

1024 A100 GPUs * 312e12 FLOP/second * 0.3 utilization * 26.8 days * 24 * 60 * 60 seconds/day = 2.219e+23 FLOP",,643.2,see compute notes,NVIDIA A100,Industry,,Likely,API access,0.0,1024.0,HyperCLOVA,658637.0,2024-04-01 14:53:59+00:00,Robi Rahman,"Korea (Republic of),Korea (Republic of)","Industry,Industry","Industry,Industry",190000000000.0,"""We introduce HyperCLOVA, a large-scale
Korean in-context learning-based LM with
nearly 100B parameters, by constructing a
large Korean-centric corpus of 560B tokens.""

Based on tokenizing the Hyperclova article itself using OpenAI's tiktoken BPE tokenizer (https://github.com/openai/tiktoken), there are 3285 tokens for 1069 words - about 3 tokens per word.

This work uses a special tokenizer, but based on Figure 5 in Appendix E, the number of tokens seems similar between different tokenization methods.

Based on that, 5.6e11 Korean tokens ~= 1.9e11 words",Self-supervised learning,,0.2,True,103802.314398667,,"""We introduce HyperCLOVA Studio, an interactive prompt engineering interface which provides GUI and API interfaces like the OpenAI
playground1""",82000000000.0,"""We introduce a Korean in-context large-scale LM with 82B parameters, i.e., HyperCLOVA. This is the first discovery on near
100B-scale non-English LM.""

According to media reports, HyperCLOVA has 204B parameters (i.e. a different version than in the paper)
https://m.koreaherald.com/view.php?ud=20210525000824 ",,,,,"""All models use the mini-batch size of 1,024"". Doesn't state sequence length or number of steps",,,,,,,,,
UL2,Language,,"Google Research,Google Brain","Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, Donald Metzler",2022-05-10,Unifying Language Learning Paradigms,https://arxiv.org/abs/2205.05131v1,171.0,SOTA improvement,"""by scaling our model up to 20B parameters, we achieve SOTA
performance on 50 well-established supervised NLP tasks""",1.2e+23,"Trained on 1T tokens
20B * 1T * 6 = 1.2e23 ",'The model is trained on a total of 1 trillion tokens on C4 (2 million steps).',744.0,"around 31 days from 'Pre-training took approximately slight more than one month for about 1 trillion
tokens.' from section 5.1
so around 31*24 = 744
",Google TPU v4,Industry,,Likely,Open source,0.0,512.0,UL2,380928.0,2024-04-01 14:53:59+00:00,Robi Rahman,"Multinational,United States of America","Industry,Industry","Industry,Industry",750000000000.0,"1T tokens, assuming 0.75 words per token we have 0.75T words",,,0.318,True,,,Apache 2.0,20000000000.0,Taken from Directory of LLMs,,,,65536.0,"""We pre-train all models for 500K steps with a batch size of 128 and a sequence length of 512 inputs and 512 targets using the C4 corpus. The total approximate tokens seen during pre-training is approximately 32 billion tokens.""

500k*128*512 ~= 32B
128*512=65,536",,,,,,C4,,,
PLaMo-13B,Language,"Language modelling/generation,Chat",Preferred Networks Inc,"Preferred Networks, Inc",2023-09-28, PLaMo-13B,https://huggingface.co/pfnet/plamo-13b,,,,1.17e+23,"6ND = 6*13e9*1.5e12=1.17e+23
from https://huggingface.co/pfnet/plamo-13b#model-details
",from https://huggingface.co/pfnet/plamo-13b#training-dataset,,,,Industry,,Likely,Fully open-source (depr.),,,,,2024-03-07 20:22:50+00:00,Bartosz Podkanowicz,Japan,Industry,Industry,1170000000000.0,"0.75*1.32T + 0.18T = 1170000000000
0.75 words per token for English
1 for Japanese ",Self-supervised learning,,,,,,,13000000000.0,,26000000000.0,2N = 26000000000.0,,,,,,,,,"C4,Project Gutenberg,RedPajama,mC4,Wikipedia (ja)",,,
IDEFICS,Multimodal,Language modelling,Hugging Face,"Hugo Laurencon, Daniel van Strien, Stas Bekman, Leo Tronchon, Lucile Saulnier, Thomas Wang, Siddharth Karamcheti, Amanpreet Singh, Giada Pistilli, Yacine Jernite, Victor Sanh",2023-08-22,Introducing IDEFICS: An Open Reproduction of State-of-the-Art Visual Language Model,https://huggingface.co/blog/idefics,0.0,,,1.1593580544e+23,"flops = 512 * 312e12 * 28*24*3600 * 0.3
(num gpus) * (peak perforemence) * (time in seconds) * (assumed utilization rate)

""The IDEFICS models were trained on an AWS SageMaker cluster with 8x80GB A100 GPUs nodes and EFA network.
    IDEFICS-80B took ~28 days of training on 64 nodes (512 GPUs).""","IDEFICS was trained on a mixture of openly available datasets: Wikipedia, Public Multimodal Dataset, and LAION, as well as a new 115B token dataset called OBELICS that we created. OBELICS consists of 141 million interleaved image-text documents scraped from the web and contains 353 million images.",,,,Industry,"We are excited to release IDEFICS (Image-aware Decoder Enhanced à la Flamingo with Interleaved Cross-attentionS), an open-access visual language model. IDEFICS is based on Flamingo, a state-of-the-art visual language model initially developed by DeepMind, which has not been released publicly. Similarly to GPT-4, the model accepts arbitrary sequences of image and text inputs and produces text outputs.",Likely,Open access (non-commercial),,,,,2024-03-29 21:03:22+00:00,Anonymous,Multinational,Industry,Industry,,150B tokens - images and words,,,,True,,,Llama license (non commercial),80000000000.0,IDEFICS... comes in two variants—the base version and the instructed version. Each variant is available at the 9 billion and 80 billion parameter sizes.,,,,,,,,,,,"Wikipedia,Public Multimodal Dataset,LAION,OBELICS",,,
Meena,Language,Text autocompletion,Google Brain,"Dongling Xiao, Han Zhang, Yukun Li, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang",2020-01-28,Towards a Human-like Open-Domain Chatbot,https://arxiv.org/abs/2001.09977,795.0,SOTA improvement,"""We also propose a human evaluation metric called Sensibleness and
Specificity Average (SSA)... the full version of Meena (with a filtering mechanism and
tuned decoding) scores 79% SSA, 23% higher in absolute SSA than the existing chatbots we evaluated""",1.12e+23,"https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf
Table 4",,720.0,"We trained our best model for 30 days on a TPUv3 Pod (2,048 TPU cores)",Google TPU v3,Industry,"We present Meena, a multi-turn open-domain chatbot trained end-to-end on data mined and filtered from public domain social media conversations. This 2.6B parameter neural network is simply trained to minimize perplexity of the next token. We also propose a human evaluation metric called Sensibleness and Specificity Average (SSA), which captures key elements of a human-like multi-turn conversation. Our experiments show strong correlation between perplexity and SSA. The fact that the best perplexity end-to-end trained Meena scores high on SSA (72% on multi-turn evaluation) suggests that a human-level SSA of 86% is potentially within reach if we can better optimize perplexity. Additionally, the full version of Meena (with a filtering mechanism and tuned decoding) scores 79% SSA, 23% higher in absolute SSA than the existing chatbots we evaluated.",,Unreleased,0.0,1024.0,Meena,737280.0,2024-04-01 14:54:00+00:00,Robi Rahman,United States of America,Industry,Industry,40000000000.0,"""The final Meena dataset contains 341GB of text
(40B words)""

Converting from GB to words yields 6.8e10, which is in the same OOM",Self-supervised learning,,0.3439,,263099.940265426,,,2600000000.0,"""We present Meena, a multi-turn open-domain chatbot trained end-to-end on data mined and filtered from public domain social media conversations. This 2.6B parameter neural network is simply trained to minimize perplexity of the next token.""",,,,82655.0,"61B tokens over 738k training steps, or 82655 tokens per batch on average. Not certain about warmup, etc",,,,,,,,,
StarCoder,Language,Code generation,"Hugging Face,ServiceNow,Northeastern University,Mila- Quebec AI,Carnegie Mellon University (CMU),Johns Hopkins University,Leipzig University,ScaDS.AI,Queen Mary University of London,Roblox,Sea AI Lab,Technion - Israel Institute of Technology,Monash University,CSIRO,Data61,McGill University,Saama,University of British Columbia (UBC),Massachusetts Institute of Technology (MIT),Technical University of Munich,IBM,University of Vermont,UnfoldML,SAP,University of Notre Dame,Columbia University,New York University (NYU),University of Allahabad,Discover Dollar,Toloka,Telefonica,Stanford University,Weizmann Institute of Science,Alan Turing Institute,Wellesley College,EleutherAI,Forschungszentrum Julich","Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, Harm de Vries",2023-05-09,StarCoder: may the source be with you!,https://arxiv.org/abs/2305.06161,253.0,SOTA improvement,"""We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python""",1.12e+23,"""We trained our model on a GPU cluster with 512 A100 80 GB GPUs... Based on the total number of GPU hours that training took (320,256) and an average power usage of 280W per GPU... The fine-tuned model adds 3.5% of training time""

320256 * 312 tFLOP/s * 3600 * 1.035 * 0.3 (utilization assumption) = 1.12e23","""StarCoderBase is trained on 1 trillion tokens sourced from The Stack (Kocetkov et al., 2022), a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process""",625.5,"625.5 hours = 320256 /512
512 GPUs from ""We trained our model on a GPU cluster with 512 A100 80 GB GPUs ""

320256 GPU hours from ""Based on the total number of GPU hours that training took (320,256)""
citations from sections 5.6 and 5.7",NVIDIA A100 SXM4 80 GB,,"""The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40\% pass@1 on HumanEval, and still retains its performance on other programming languages. We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license.""",Likely,Fully open-source (depr.),0.0,512.0,StarCoder,320256.0,2024-04-01 14:30:19+00:00,Anonymous,"Multinational,United States of America,United States of America,Canada,United States of America,United States of America,Germany,Germany,United Kingdom of Great Britain and Northern Ireland,United States of America,Singapore,Israel,Australia,Australia,Australia,Canada,United States of America,Canada,United States of America,Germany,United States of America,United States of America,Sweden,Multinational,United States of America,United States of America,United States of America,India,India,Multinational,Spain,United States of America,Israel,United Kingdom of Great Britain and Northern Ireland,United States of America,Multinational,Germany","Industry,Industry,Academia,Academia,Academia,Academia,Academia,Academia,Industry,Academia,Academia,Government,Government,Academia,Academia,Academia,Academia,Industry,Academia,Industry,Academia,Academia,Academia,Academia,Industry,Industry,Industry,Academia,Academia,Government,Academia,Research collective,Government","Industry,Industry,Academia,Academia,Academia,Academia,Academia,Academia,Industry,Academia,Academia,Government,Government,Academia,Academia,Academia,Academia,Industry,Academia,Industry,Academia,Academia,Academia,Academia,Industry,Industry,Industry,Academia,Academia,Government,Academia,Research collective,Government",,"""StarCoderBase is trained on 1 trillion tokens sourced from The Stack""",,1.0,,True,,,,15500000000.0,"""We trained a 15.5B parameter model""",,,,4194304.0,"""We train for 100,000 steps with a global batch size of 4,096 sequences of a maximum length of 1,024 so that approximately 400B tokens are observed""",,,,,,The Stack,,,
OPT-66B,Language,Language modelling,Meta AI,"Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",2022-06-21,,https://huggingface.co/facebook/opt-66b,,,,1.100000000001e+23,"OPT-66B was trained for 140k steps, using a batch size of 2M tokens (see the OPT baselines logbook and Table 1 in Zhang et al. (2022), respectively), so training took 140e3 ∗ 2e6 ∗ 66e9 ∗ 6 = 1.1e23 FLOP",,,,,,,Confident,Fully open-source (depr.),1.0,,,,2024-04-02 13:16:28+00:00,Robi Rahman,United States of America,Industry,Industry,,,,1.67,,,,,,66000000000.0,,,,,,,,,,,,,,OPT-66B,
Whisper v2,Speech,Speech recognition,OpenAI,,2022-12-05,openai/whisper-large-v2,https://huggingface.co/openai/whisper-large-v2,,,,1.1e+23,"""Compared to the Whisper large model, the large-v2 model is trained for 2.5x more epochs with added regularization for improved performance.""

We (roughly) estimated Whisper v1 as 4.65e22. 2.5x that is 1.16e23 or ~1.1e23","""The models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet. 65% of this data (or 438,000 hours) represents English-language audio and matched English transcripts, roughly 18% (or 126,000 hours) represents non-English audio and English transcripts, while the final 17% (or 117,000 hours) represents non-English audio and the corresponding transcript. This non-English data represents 98 different languages.""",,,,,"Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning.

Whisper was proposed in the paper Robust Speech Recognition via Large-Scale Weak Supervision by Alec Radford et al. from OpenAI. The original code repository can be found here.

Compared to the Whisper large model, the large-v2 model is trained for 2.5x more epochs with added regularization for improved performance.",Likely,Permissive license (depr.),0.0,,,,2024-03-07 20:22:50+00:00,Anonymous,United States of America,Industry,Industry,9302400000.0,"""When scaled to 680,000 hours of multilingual and multitask
supervision, the resulting models generalize well
to standard benchmarks and are often competitive
with prior fully supervised results but in a zeroshot transfer setting without the need for any finetuning.""


13,680 words/h (estimate) * 680,000h = 9,302,400,000 words",Self-supervised learning,7.5,,,,,,1550000000.0,1550M,,,,,,,,,,,,,,
Code Llama-7B,Language,Code generation,Meta AI,"Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Ellen Tan, Yossef (Yossi) Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Defossez, Jade Copet, Faisal Azhar, Hugo Touvron, Gabriel Synnaeve, Louis Martin, Nicolas Usunier, Thomas Scialom",2023-08-14,Code Llama: Open Foundation Models for Code,"https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/
https://arxiv.org/abs/2308.12950",463.0,,,1.1e+23,"2.5e22 finetune compute + 8.4e22 base compute for Llama 2-7B, for ~1.1e23 compute overall
","""As shown in Table 1, Code Llama is trained predominantly on a near-deduplicated dataset of
publicly available code. We also source 8% of our samples data from natural language datasets related to
code. This dataset contains many discussions about code and code snippets included in natural language
questions or answers. To help the model retain natural language understanding skills, we also sample a small
proportion of our batches from a natural language dataset""",,,NVIDIA A100 SXM4 80 GB,,"We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B and 34B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B and 13B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 53% and 55% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use.",Likely,Permissive license (depr.),0.0,,,,2024-04-01 14:02:06+00:00,Epoch AI,United States of America,Industry,Industry,,"""We train Code Llama on 500B tokens during the initial phase, starting from the 7B, 13B, and 34B versions
of Llama 2.""",,,,,,,,7000000000.0,7B,,,,,,Llama 2-7B,2.5e+22,,,"Code Llama-base is trained from Llama 2 with 500B tokens: ""We train Code Llama on 500B tokens during the initial phase, starting from the 7B, 13B, and 34B versions of Llama 2""

Code Llama-Python required an additional 100B tokens in fine-tuning: ""We train Code Llama on 500B additional tokens and Code Llama - Python further on 100B tokens.""
Code Llama-Instruct is fine-tuned on 5B tokens: ""For Code Llama - Instruct, we train with a batch size of 524,288 tokens and on approx. 5B tokens in total.""

7B * (500B+100B) * 6 = 2.5e22
",,,,
BlueLM 13B,Language,Chat,vivo AI lab,,2023-10-31,,https://github.com/vivo-ai-lab/BlueLM,,,,1.0920000000001e+23,"C = 6DN = 6 * 2.6T * 7B = 1.092*10^23 FLOP
https://www.wolframalpha.com/input?i=6*7+billion+*+2.6+trillion
(assuming 1 epoch)",,,,,,,Speculative,Fully open-source (depr.),0.0,,,,2024-03-07 20:22:50+00:00,Anonymous,China,Industry,Industry,1950000000000.0,"""Larger amounts of high-quality data : high-quality corpus for training, reaching a scale of 2.6 trillion tokens. The corpus includes Chinese, English and a small amount of Japanese and Korean data"" from GitHub",,,,,,,,7000000000000.0,"""BlueLM is a large-scale pre-trained language model independently developed by vivo AI Global Research Institute. This release includes 7B base (base) model and 7B conversation (chat) model. At the same time, we have open sourced the long text base (base) model that supports 32K and conversation (chat) model."" from GitHub https://github.com/vivo-ai-lab/BlueLM

",,,,,,,,,,,,,,
Qwen-7B,Language,,Alibaba,"Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, Tianhang Zhu",2023-09-28,Qwen Technical Report,"https://arxiv.org/abs/2309.16609, https://huggingface.co/Qwen/Qwen-7B",169.0,,,1.01e+23,"2.4T tokens per Table 1

7b*2.4T*6 = 1.01e23","""Our dataset is designed to meet these requirements and includes public
web documents, encyclopedia, books, codes, etc. Additionally, our dataset is multilingual, with a
significant portion of the data being in English and Chinese.""",,,,,"Large language models (LLMs) have revolutionized the field of artificial intelligence, enabling natural language processing tasks that were previously thought to be exclusive to humans. In this work, we introduce Qwen, the first installment of our large language model series. Qwen is a comprehensive language model series that encompasses distinct models with varying parameter counts. It includes Qwen, the base pretrained language models, and Qwen-Chat, the chat models finetuned with human alignment techniques. The base language models consistently demonstrate superior performance across a multitude of downstream tasks, and the chat models, particularly those trained using Reinforcement Learning from Human Feedback (RLHF), are highly competitive. The chat models possess advanced tool-use and planning capabilities for creating agent applications, showcasing impressive performance even when compared to bigger models on complex tasks like utilizing a code interpreter. Furthermore, we have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, as well as mathematics-focused models, Math-Qwen-Chat, which are built upon base language models. These models demonstrate significantly improved performance in comparison with open-source models, and slightly fall behind the proprietary models.",Likely,Permissive license (depr.),1.0,,,,2024-04-01 14:02:07+00:00,Epoch AI,China,Industry,Industry,,,,,,,,,,7000000000.0,7B,,,,4000000.0,Table 1,,,,,,,,,
