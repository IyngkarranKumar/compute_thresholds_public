{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.linear_model import LinearRegression #for linear regression\n",
    "from scipy.optimize import curve_fit #for exponential fit\n",
    "\n",
    "csv_path = '/Users/iyngkarrankumar/Documents/Misc/Tracking models/data/all_systems.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def half_year_bin(date):\n",
    "    #CHATGPT generated\n",
    "\n",
    "    if date.month <= 6:\n",
    "        return f'{date.year}-H1'\n",
    "    else: \n",
    "        return f'{date.year}-H2'\n",
    "\n",
    "def year_bin(date):\n",
    "    return date.year\n",
    "\n",
    "\n",
    "def exponential_model(x,a,b):\n",
    "    return a*np.exp(b*(x-2017))\n",
    "\n",
    "def geometric_model(x,a,r):\n",
    "    return a*r**(x-2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = pd.read_csv(csv_path)\n",
    "DATA['Publication date'] = pd.to_datetime(DATA['Publication date'])\n",
    "\n",
    "#filter based on compute\n",
    "DATA_ = DATA.dropna(subset=['Training compute (FLOP)'])\n",
    "\n",
    "\n",
    "#data filtering and binning\n",
    "start_year = 2017\n",
    "DATA_f1 = DATA_[DATA_['Publication date'] > f'{start_year}-01-01']\n",
    "\n",
    "#new column for binning\n",
    "bin_type = 'year' \n",
    "if bin_type=='year':\n",
    "    DATA_f1['Publication_Bin'] = DATA_['Publication date'].apply(year_bin)\n",
    "elif bin_type=='half year':\n",
    "    DATA_f1['Publication_Bin'] = DATA_['Publication date'].apply(half_year_bin)\n",
    "\n",
    "#new column for log flop\n",
    "DATA_f1['log10 Training compute (FLOP)'] = np.log10(DATA_f1['Training compute (FLOP)'])\n",
    "\n",
    "#finding means\n",
    "DATA_f1.groupby('Publication_Bin')['log10 Training compute (FLOP)'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(reversed(DATA_f1['Publication_Bin'].unique()))\n",
    "fig,axs = plt.subplots(nrows=len(years),ncols=1,figsize=(8,12),sharex=True)\n",
    "bin_range = (15,27)\n",
    "bins=np.arange(bin_range[0],bin_range[-1],1)\n",
    "\n",
    "for idx,year in enumerate(years):\n",
    "    ax = axs[idx]\n",
    "    filtered_df = DATA_f1[DATA_f1['Publication_Bin']==year] #year df\n",
    "    filtered_df['log10 Training compute (FLOP)'].plot(kind='hist',bins=bins,range=bin_range,edgecolor='black',ax=ax)\n",
    "    ax.set_xlabel('');ax.set_ylabel('')\n",
    "    ax.set_xlim(bin_range)\n",
    "    ax.tick_params(axis='y',labelsize=12)\n",
    "    if idx==0:     ax.yaxis.set_major_locator(mpl.ticker.MaxNLocator(integer=True))\n",
    "\n",
    "    ax.set_title(f'Year {year}, n={len(filtered_df)}',fontsize=15)\n",
    "    ax.grid(alpha=0.5)\n",
    "\n",
    "\n",
    "\n",
    "fig.text(0.5, -0.04, 'Log Compute ($10^X$)', ha='center', fontsize=15)\n",
    "fig.text(-0.04, 0.5, 'Frequency', va='center', rotation='vertical', fontsize=15)\n",
    "plt.xticks(bins,fontsize=15)\n",
    "plt.subplots_adjust(hspace=10)\n",
    "plt.tight_layout(rect=[0.04, 0.04, 1, 1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model number extrapolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOUBLE_2024 = False\n",
    "\n",
    "DATA = pd.read_csv(csv_path)\n",
    "DATA['Publication date'] = pd.to_datetime(DATA['Publication date']) \n",
    "\n",
    "#we don't filter for na training compute - that doesn't matter for models\n",
    "\n",
    "start_year = 2017\n",
    "DATA_f1 = DATA[DATA['Publication date'] > f'{start_year}-01-01'] #start after 2017\n",
    "\n",
    "bin_type = 'year' \n",
    "if bin_type=='year':\n",
    "    DATA_f1['Publication_Bin'] = DATA['Publication date'].apply(year_bin)\n",
    "elif bin_type=='half year':\n",
    "    DATA_f1['Publication_Bin'] = DATA['Publication date'].apply(half_year_bin)\n",
    "\n",
    "DATA_f1['log10 Training compute (FLOP)'] = np.log10(DATA_f1['Training compute (FLOP)'])\n",
    "\n",
    "if DOUBLE_2024: #doesn't work\n",
    "    years = (sorted(DATA_f1['Publication_Bin'].unique()))\n",
    "    model_counts = list(DATA_f1['Publication_Bin'].value_counts().sort_index())\n",
    "    print(years,model_counts)\n",
    "    model_counts[-1] = 2*model_counts[-1]\n",
    "    model_counts[-1] = model_counts[-1] + (model_counts[-1]-model_counts[-2])\n",
    "    future_years = np.arange(2025,2030)\n",
    "\n",
    "\n",
    "else:\n",
    "    years = (sorted(DATA_f1['Publication_Bin'].unique())); years.pop()\n",
    "    model_counts = list(DATA_f1['Publication_Bin'].value_counts().sort_index()); model_counts.pop() #remove 2024 count\n",
    "    future_years = np.arange(2024,2030)\n",
    "\n",
    "years = np.array(years)\n",
    "model_counts = np.array(model_counts)\n",
    "\n",
    "\n",
    "#Linear extrapolation\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(years.reshape(-1,1),model_counts)\n",
    "linear_pred = linear_model.predict(future_years.reshape(-1,1))\n",
    "\n",
    "#Polynomial extrapolation\n",
    "degree = 2 \n",
    "coefficients = np.polyfit(years, model_counts,degree)\n",
    "polynomial = np.poly1d(coefficients)\n",
    "poly_pred = polynomial(future_years)\n",
    "\n",
    "\n",
    "#geometric series\n",
    "popt_geometric, _ = curve_fit(geometric_model,years,model_counts)\n",
    "geometric_pred = geometric_model(future_years,*popt_geometric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "\n",
    "ax.bar(years,model_counts,color='tab:blue')\n",
    "ax.bar(future_years,geometric_pred,color='tab:red',alpha=0.8); ax.legend(fontsize=12)\n",
    "ax.grid(alpha=0.5)\n",
    "ax.tick_params(axis='x',labelsize=12)\n",
    "ax.tick_params(axis='y',labelsize=12)\n",
    "ax.set_ylabel('Models released',fontsize=15)\n",
    "ax.set_xlabel('Years',fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Extrapolating models that exceed thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SETUP AND REGRESSION\n",
    "\n",
    "DATA = pd.read_csv(csv_path)\n",
    "DATA['Publication date'] = pd.to_datetime(DATA['Publication date'])\n",
    "\n",
    "#filter based on compute\n",
    "DATA_ = DATA.dropna(subset=['Training compute (FLOP)'])\n",
    "\n",
    "\n",
    "#data filtering and binning\n",
    "start_year = 2017\n",
    "DATA_f1 = DATA_[DATA_['Publication date'] > f'{start_year}-01-01']\n",
    "\n",
    "#filter out AlphaGo and AlphaGo master\n",
    "SYSTEMS_TO_REMOVE = ['AlphaGo Zero','AlphaGo Master']\n",
    "DATA_f1 = DATA_f1[~DATA_f1['System'].isin(SYSTEMS_TO_REMOVE)]\n",
    "\n",
    "#new column for binning\n",
    "bin_type = 'year' \n",
    "if bin_type=='year':\n",
    "    DATA_f1['Publication_Bin'] = DATA_['Publication date'].apply(year_bin)\n",
    "elif bin_type=='half year':\n",
    "    DATA_f1['Publication_Bin'] = DATA_['Publication date'].apply(half_year_bin)\n",
    "\n",
    "#new column for log flop\n",
    "DATA_f1['log10 Training compute (FLOP)'] = np.log10(DATA_f1['Training compute (FLOP)'])\n",
    "\n",
    "#years\n",
    "years = (sorted(DATA_f1['Publication_Bin'].unique())); years.pop()\n",
    "years=np.array(years)\n",
    "\n",
    "#finding means\n",
    "historic_means = list(DATA_f1.groupby('Publication_Bin')['log10 Training compute (FLOP)'].mean()) #ignore 2024 (n=4)\n",
    "historic_means.pop()\n",
    "\n",
    "historic_var = list(DATA_f1.groupby('Publication_Bin')['log10 Training compute (FLOP)'].var())\n",
    "historic_var.pop()\n",
    "\n",
    "#finding growth rate (in terms of OOMs)\n",
    "historic_means = np.array(historic_means)\n",
    "mean_differences = np.diff(historic_means)\n",
    "log_training_compute_linear_model = LinearRegression()\n",
    "log_training_compute_linear_model.fit(years.reshape(-1,1),historic_means)\n",
    "\n",
    "\n",
    "\n",
    "#avg var - not fitting a linear regression\n",
    "var = np.mean(np.array(historic_var))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Â GENERATING SIMULATED DATA\n",
    "\n",
    "model_counts_fit = \"poly\" #exp, lin ,poly\n",
    "\n",
    "start_year = 2024\n",
    "end_year = 2029\n",
    "future_years = np.arange(start_year,end_year+1,dtype=int)\n",
    "if model_counts_fit==\"exp\":\n",
    "    predicted_model_counts = (geometric_model(future_years,*popt_geometric)).astype('int')\n",
    "elif model_counts_fit==\"lin\":\n",
    "    predicted_model_counts = (linear_model.predict(future_years.reshape(-1,1))).astype('int')\n",
    "elif model_counts_fit=='poly': \n",
    "    predicted_model_counts = (polynomial(future_years)).astype('int')\n",
    "predicted_mean_training_compute = log_training_compute_linear_model.predict(future_years.reshape(-1,1))\n",
    "\n",
    "assert len(future_years) == len(predicted_mean_training_compute) == len(predicted_model_counts)\n",
    "\n",
    "historical_data = DATA_f1['log10 Training compute (FLOP)'].values\n",
    "historical_data_years = DATA_f1['Publication_Bin'].values\n",
    "\n",
    "\n",
    "simulated_data = {}\n",
    "\n",
    "for idx,year in enumerate(future_years):\n",
    "\n",
    "    #bootstrapping\n",
    "    samples = np.random.choice(historical_data,size=int(predicted_model_counts[idx]),replace=True)\n",
    "    sample_mean = np.mean(samples)\n",
    "    sample_std = np.std(samples)\n",
    "\n",
    "    #adjusting mean and var\n",
    "    adjusted_sample_Z = (samples-sample_mean)/sample_std  #normalise sample\n",
    "    adjusted_sample = adjusted_sample_Z*np.sqrt(var) + predicted_mean_training_compute[idx] #adjust mean and var\n",
    "\n",
    "    simulated_data[year]=adjusted_sample\n",
    "\n",
    "\n",
    "\n",
    "#combine historical and simulated data\n",
    "combined_df = pd.DataFrame(columns=['year','log10 Training Compute'])\n",
    "df_1 = {\n",
    "    'year': historical_data_years,\n",
    "    'log10 Training Compute':historical_data\n",
    "}\n",
    "combined_df = pd.concat([combined_df,pd.DataFrame(df_1)],ignore_index=True)\n",
    "\n",
    "for idx,year in enumerate(future_years):\n",
    "    simulated_data_year = simulated_data[year]\n",
    "    year_data = {\n",
    "        'year': np.full(len(simulated_data_year),int(year)),\n",
    "        'log10 Training Compute':simulated_data_year\n",
    "    }\n",
    "    combined_df = pd.concat([combined_df,pd.DataFrame(year_data)],ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute distributions (for simulated data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_df = combined_df[combined_df['year']>=2024]\n",
    "\n",
    "years = sorted(list(simulated_df['year'].unique()))\n",
    "fig,axs = plt.subplots(nrows=len(years),figsize=(8,12),sharex=True)\n",
    "bin_range = (15,31)\n",
    "bins=np.arange(bin_range[0],bin_range[-1],1)\n",
    "\n",
    "for idx,year in enumerate(years):\n",
    "    ax = axs[idx]\n",
    "    filtered_df = simulated_df[simulated_df['year']==year] #year df\n",
    "    filtered_df['log10 Training Compute'].plot(kind='hist',bins=bins,range=bin_range,edgecolor='black',ax=ax)\n",
    "    ax.set_xlabel('');ax.set_ylabel('')\n",
    "    ax.set_xlim(bin_range)\n",
    "    ax.tick_params(axis='y',labelsize=12)\n",
    "    \n",
    "    ax.set_title(f'Year {year}, n={len(filtered_df)}',fontsize=15)\n",
    "    ax.grid(alpha=0.5)\n",
    "\n",
    "\n",
    "\n",
    "fig.text(0.5, -0.04, 'Log Compute ($10^X$)', ha='center', fontsize=15)\n",
    "fig.text(-0.04, 0.5, 'Frequency', va='center', rotation='vertical', fontsize=15)\n",
    "plt.xticks(bins,fontsize=15)\n",
    "plt.subplots_adjust(hspace=10)\n",
    "plt.tight_layout(rect=[0.04, 0.04, 1, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scatter\n",
    "SCATTER=True\n",
    "MODEL_COUNT=False\n",
    "\n",
    "if SCATTER:\n",
    "    plt.scatter(combined_df['year'],combined_df['log10 Training Compute'],alpha=0.5,marker='x')\n",
    "    plt.xticks(np.arange(2017,2030),rotation=45)\n",
    "    plt.ylabel('log10 Compute')\n",
    "    plt.grid(alpha=0.4)\n",
    "\n",
    "if MODEL_COUNT:\n",
    "    years = list(sorted(combined_df.year.unique()))\n",
    "    total_model_counts=np.concatenate([model_counts,predicted_model_counts],axis=0)\n",
    "    plt.bar(years,total_model_counts)\n",
    "    plt.xticks(np.arange(2017,2030),rotation=45)\n",
    "    plt.title(\"Number of models released each year\")\n",
    "    plt.grid(alpha=0.4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting models exceeding threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can now find 'past year X, how many models are these past threshold Y'\n",
    "threshold = 25 #threshold in log FLOPS\n",
    "years = np.array(sorted(combined_df['year'].unique()))\n",
    "plot_data_25 = []\n",
    "plot_data_26 = []\n",
    "for year in years:\n",
    "    #10^25\n",
    "    date_condition = combined_df['year'] <= year\n",
    "    compute_condition = combined_df['log10 Training Compute'] >= 25\n",
    "    filtered_df=combined_df[date_condition & compute_condition] #how many models exist up to that year and have compute > threshold\n",
    "    plot_data_25.append(len(filtered_df))\n",
    "\n",
    "    #10^26\n",
    "    compute_condition = combined_df['log10 Training Compute'] >= 26\n",
    "    filtered_df=combined_df[date_condition & compute_condition] #how many models exist up to that year and have compute > threshold\n",
    "    plot_data_26.append(len(filtered_df))\n",
    "\n",
    "\n",
    "plt.bar(years-0.1,plot_data_25,width=0.5,label='10^25 FLOP threshold',alpha=0.8)\n",
    "plt.bar(years+0.1,plot_data_26,width=0.5,label='10^26 FLOP threshold',alpha=0.8)\n",
    "plt.legend()\n",
    "plt.xticks(np.arange(2017,2030),rotation=45)\n",
    "plt.yticks(range(0,3001,250))\n",
    "plt.title(f'Number of models exceeding threshold')\n",
    "plt.grid(alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Frontier models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = pd.read_csv(csv_path)\n",
    "DATA['Publication date'] = pd.to_datetime(DATA['Publication date'])\n",
    "\n",
    "#filter based on compute\n",
    "DATA_ = DATA.dropna(subset=['Training compute (FLOP)'])\n",
    "\n",
    "\n",
    "#data filtering and binning\n",
    "start_year = 2017\n",
    "DATA_f1 = DATA_[DATA_['Publication date'] > f'{start_year}-01-01']\n",
    "\n",
    "#new column for binning\n",
    "bin_type = 'year' \n",
    "if bin_type=='year':\n",
    "    DATA_f1['Publication_Bin'] = DATA_['Publication date'].apply(year_bin)\n",
    "elif bin_type=='half year':\n",
    "    DATA_f1['Publication_Bin'] = DATA_['Publication date'].apply(half_year_bin)\n",
    "\n",
    "#new column for log flop\n",
    "DATA_f1['log10 Training compute (FLOP)'] = np.log10(DATA_f1['Training compute (FLOP)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frontier_depth_1 = 1 #in terms of OOMs\n",
    "frontier_depth_05 = 0.5\n",
    "\n",
    "#REMOVE_SYSTEMS = ['AlphaGo Zero','AlphaGo Master']\n",
    "REMOVE_SYSTEMS = []\n",
    "DATA_f1 = DATA_f1[~DATA_f1['System'].isin(REMOVE_SYSTEMS)]\n",
    "\n",
    "years = list(reversed(DATA_f1['Publication_Bin'].unique()))\n",
    "LARGEST_RUNS = []\n",
    "N_FRONTIER_SYSTEMS_1 = []\n",
    "N_FRONTIER_SYSTEMS_05 = []\n",
    "N_SYSTEMS = []\n",
    "\n",
    "DF = pd.DataFrame(columns=['Year','Name','0.5 OOM','1 OOM'])\n",
    "\n",
    "for year in years:\n",
    "    filtered_df = DATA_f1[DATA_f1['Publication_Bin']<=year] #training runs that have taken place up to $year ($year included)\n",
    "    largest_run_to_date = round(filtered_df['log10 Training compute (FLOP)'].max(),2)\n",
    "    largest_run_index = filtered_df['log10 Training compute (FLOP)'].idxmax()\n",
    "    largest_run_name = filtered_df['System'].loc[largest_run_index]\n",
    "    LARGEST_RUNS.append((largest_run_name,largest_run_to_date))\n",
    "    N_SYSTEMS.append(len(filtered_df))\n",
    "\n",
    "\n",
    "    frontier_filtering_condition_1 = (largest_run_to_date - filtered_df['log10 Training compute (FLOP)']) < frontier_depth_1\n",
    "    frontier_filtered_df_1 = filtered_df[frontier_filtering_condition_1]\n",
    "    N_FRONTIER_SYSTEMS_1.append(len(frontier_filtered_df_1))\n",
    "\n",
    "    frontier_filtering_condition_05  = (largest_run_to_date-filtered_df['log10 Training compute (FLOP)']) < frontier_depth_05\n",
    "    frontier_filtered_df_05 = filtered_df[frontier_filtering_condition_05]\n",
    "    N_FRONTIER_SYSTEMS_05.append(len(frontier_filtered_df_05))\n",
    "\n",
    "\n",
    "N_FRONTIER_SYSTEMS_1=np.array(N_FRONTIER_SYSTEMS_1)-1 #to account for largest run\n",
    "N_FRONTIER_SYSTEMS_05 = np.array(N_FRONTIER_SYSTEMS_05)-1 \n",
    "\n",
    "DF['Year'] = years\n",
    "DF['Name'] = LARGEST_RUNS\n",
    "DF['1 OOM'] = N_FRONTIER_SYSTEMS_1\n",
    "DF['0.5 OOM'] = N_FRONTIER_SYSTEMS_05\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 0 - Cumulative model counts for various thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prep\n",
    "\n",
    "DATA = pd.read_csv(csv_path)\n",
    "DATA['Publication date'] = pd.to_datetime(DATA['Publication date'])\n",
    "\n",
    "#filter based on compute\n",
    "DATA_ = DATA.dropna(subset=['Training compute (FLOP)'])\n",
    "\n",
    "\n",
    "#data filtering and binning\n",
    "start_year = 2017\n",
    "DATA_f1 = DATA_[DATA_['Publication date'] > f'{start_year}-01-01']\n",
    "\n",
    "#remove systems\n",
    "SYSTEMS_TO_REMOVE = ['AlphaGo Zero','AlphaGo Master']\n",
    "DATA_f1 = DATA_f1[~DATA_f1['System'].isin(SYSTEMS_TO_REMOVE)]\n",
    "\n",
    "#new column for binning\n",
    "bin_type = 'year' \n",
    "if bin_type=='year':\n",
    "    DATA_f1['Publication_Bin'] = DATA_['Publication date'].apply(year_bin)\n",
    "elif bin_type=='half year':\n",
    "    DATA_f1['Publication_Bin'] = DATA_['Publication date'].apply(half_year_bin)\n",
    "\n",
    "#new column for log flop\n",
    "DATA_f1['log10 Training compute (FLOP)'] = np.log10(DATA_f1['Training compute (FLOP)'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOUBLE_2024 = True\n",
    "\n",
    "years = np.array(list(reversed(DATA_f1['Publication_Bin'].unique())))\n",
    "years_str = [str(year) for year in years]\n",
    "thresholds = [23,24]\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(8,6),sharey=True)\n",
    "LARGEST_RUNS=[]\n",
    "\n",
    "PLOT_DATA = {key: None for key in thresholds}\n",
    "\n",
    "for idx,threshold in enumerate(thresholds):\n",
    "    ax.grid(alpha=0.4)\n",
    "    plot_data = []\n",
    "    for year in years:\n",
    "        date_condition = DATA_f1['Publication_Bin'] <= year\n",
    "        date_filtered_df = DATA_f1[date_condition]\n",
    "        largest_run = date_filtered_df['log10 Training compute (FLOP)'].max()\n",
    "        if idx==0:LARGEST_RUNS.append(round(largest_run,1))\n",
    "\n",
    "        threshold_condition = DATA_f1['log10 Training compute (FLOP)'] >= threshold\n",
    "        filtered_df = DATA_f1[date_condition & threshold_condition]\n",
    "\n",
    "        data_point = len(filtered_df)\n",
    "        if year==2024 and DOUBLE_2024:\n",
    "            exceeding_threshold_2024 = data_point-plot_data[-1]\n",
    "            data_point = data_point + 2*exceeding_threshold_2024 #to get a full year of 2024 data out\n",
    "\n",
    "        \n",
    "\n",
    "        plot_data.append(data_point)\n",
    "\n",
    "\n",
    "\n",
    "    PLOT_DATA[threshold] = np.array(plot_data)\n",
    "\n",
    "\n",
    "    if 0: \n",
    "        ax.bar(years,plot_data)\n",
    "        ax.set_title(f'Threshold: 10^{threshold} FLOPs')\n",
    "        ax.set_xticklabels(years,rotation=45,fontsize=12)\n",
    "        if idx==0: ax.set_ylabel('N_systems',fontsize=12,rotation=0)\n",
    "if 1: \n",
    "    ax.bar(years-0.1,PLOT_DATA[23], width=0.5, label=f'Threshold: 10^23 FLOPs',color='tab:blue',alpha=0.8)\n",
    "    ax.bar(years+0.1,PLOT_DATA[24], width=0.5, label=f'Threshold: 10^24 FLOPS',color='tab:orange',alpha=0.8)\n",
    "    ax.set_yticks(np.arange(0,130,10))\n",
    "    ax.legend()\n",
    "    fig.suptitle('Number of models exceeding thresholds')\n",
    "    #custom_xticks = [f'{year} \\n {run}' for year,run in zip(years,LARGEST_RUNS)]\n",
    "    #ax.set_xticklabels(custom_xticks,fontsize=10)\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=1)\n",
    "plt.tight_layout()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOUBLE_2024 = True\n",
    "REMOVE_2024 = False\n",
    "\n",
    "##threshold \n",
    "threshold=23\n",
    "\n",
    "## extrapolating\n",
    "years_arr =np.array(years)\n",
    "\n",
    "start_year=2025\n",
    "end_year=2029\n",
    "future_years = np.arange(start_year,end_year+1)\n",
    "##we go from the first year in which number of systems that exceeds is greater than 0\n",
    "nonzero_idxs = PLOT_DATA[threshold] != 0\n",
    "counts_filtered = PLOT_DATA[threshold][nonzero_idxs]\n",
    "years_filtered = years_arr[nonzero_idxs]\n",
    "\n",
    "\n",
    "if DOUBLE_2024: \n",
    "    assert not REMOVE_2024\n",
    "    exceeding_threshold_24 = counts_filtered[-1] - counts_filtered[-2] #how many models released so far in 2024 that exceed threshold\n",
    "    counts_filtered[-1] = counts_filtered[-1] + 2*exceeding_threshold_24\n",
    "    print(counts_filtered)\n",
    "\n",
    "if REMOVE_2024:\n",
    "    counts_filtered = counts_filtered[:-1]\n",
    "    years_filtered = years_filtered[:-1]\n",
    "\n",
    "X_0 = years_filtered[0]\n",
    "def geometric_model(x,a,r):\n",
    "    return a*r**(x-X_0)\n",
    "\n",
    "\n",
    "popt_geometric,_ = curve_fit(geometric_model,years_filtered,counts_filtered)\n",
    "geometric_pred = (geometric_model(future_years,*popt_geometric)).astype('int')\n",
    "\n",
    "\n",
    "fig,ax=plt.subplots()\n",
    "ax.bar(years_filtered,counts_filtered,color='tab:blue')\n",
    "ax.bar(future_years,geometric_pred,color='tab:red',alpha=0.8)\n",
    "ax.grid(alpha=0.5)\n",
    "ax.tick_params(axis='x',labelsize=12)\n",
    "ax.tick_params(axis='y',labelsize=12)\n",
    "ax.set_title(f'Models exceeding 10^{threshold} threshold',fontsize=15)\n",
    "ax.set_xlabel('Years',fontsize=15)\n",
    "ax.set_yscale('linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.linear_model import LinearRegression #for linear regression\n",
    "from scipy.optimize import curve_fit #for exponential fit\n",
    "import statsmodels\n",
    "\n",
    "csv_path = '/Users/iyngkarrankumar/Documents/Misc/Tracking models/data/all_systems.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = pd.read_csv(csv_path)\n",
    "DATA['Publication date'] = pd.to_datetime(DATA['Publication date'])\n",
    "\n",
    "DATA = DATA.dropna(subset=['Training compute (FLOP)'])\n",
    "\n",
    "start_year = 2017\n",
    "DATA = DATA[DATA['Publication date'] > f'{start_year}-01-01']\n",
    "\n",
    "#new column for binning\n",
    "bin_type = 'year' \n",
    "if bin_type=='year':\n",
    "    DATA['Publication_Bin'] = DATA['Publication date'].apply(year_bin)\n",
    "elif bin_type=='half year':\n",
    "    DATA['Publication_Bin'] = DATA['Publication date'].apply(half_year_bin)\n",
    "\n",
    "DATA['log10 Training compute (FLOP)'] = np.log10(DATA['Training compute (FLOP)'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "csv_path = '/Users/iyngkarrankumar/Documents/Misc/Tracking models/data/all_systems.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = pd.read_csv(csv_path)\n",
    "DATA['Publication date'] = pd.to_datetime(DATA['Publication date'])\n",
    "start_year = 2017\n",
    "DATA = DATA[DATA['Publication date'] > f'{start_year}-01-01']\n",
    "\n",
    "bin_type = 'year' \n",
    "if bin_type=='year':\n",
    "    DATA['Publication_Bin'] = DATA['Publication date'].apply(year_bin)\n",
    "elif bin_type=='half year':\n",
    "    DATA['Publication_Bin'] = DATA['Publication date'].apply(half_year_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "REMOVE_2024 = True\n",
    "\n",
    "years = sorted(DATA['Publication_Bin'].unique())\n",
    "years = years[:-1]\n",
    "\n",
    "STORE = {year:None for year in years}\n",
    "\n",
    "MODEL_COUNTS = []\n",
    "N_DEVELOPERS = []\n",
    "TOP_N = 10\n",
    "TOP_N_MED_RELEASE_FREQ = []\n",
    "MEAN_RELEASE_FREQ = []\n",
    "\n",
    "for year in years:\n",
    "    DATA_year_filtered = DATA[DATA['Publication_Bin']==year]\n",
    "    MODEL_COUNTS.append(len(DATA_year_filtered))\n",
    "    ORGANISATION_year = list(DATA_year_filtered['Organization'])\n",
    "    split_list = [item for sublist in ORGANISATION_year for item in sublist.split(',')]\n",
    "    organisation_frequency = dict(Counter(split_list))\n",
    "    STORE[year] = organisation_frequency\n",
    "    N_DEVELOPERS.append(len(organisation_frequency.keys()))\n",
    "\n",
    "    release_counts_sorted = np.sort(list(organisation_frequency.values()))\n",
    "    TOP_N_MED_RELEASE_FREQ.append(np.int32(np.median(release_counts_sorted[-TOP_N:])))\n",
    "    MEAN_RELEASE_FREQ.append(np.mean(release_counts_sorted))\n",
    "\n",
    "\n",
    "fig,axs = plt.subplots(nrows=3,sharex=True)\n",
    "axs[0].scatter(years,MODEL_COUNTS)\n",
    "axs[1].scatter(years,N_DEVELOPERS)\n",
    "axs[2].scatter(years,TOP_N_MED_RELEASE_FREQ)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('FTM')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f33f59546c507a35a4881afce9503208f6c8f0e8d914c07bf4768a8e3992010"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
