{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset setup and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from scipy import stats, optimize\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd #taking long to load here\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import copy,re, pdb, logging\n",
    "\n",
    "modules_to_import = [\n",
    "    ('numpy', 'np'),\n",
    "    ('scipy.stats', 'stats'),\n",
    "    ('scipy.optimize', 'optimize'),\n",
    "    ('matplotlib.pyplot', 'plt'),\n",
    "    ('pandas', 'pd'),\n",
    "    ('seaborn', 'sns'),\n",
    "    ('itertools', 'itertools'),\n",
    "    ('copy', 'copy'),\n",
    "    ('re', 're'),\n",
    "    ('pdb', 'pdb'),\n",
    "    ('logging', 'logging'),\n",
    "    ('sklearn.linear_model', 'linear_model'),\n",
    "]\n",
    "\n",
    "for module, alias in modules_to_import:\n",
    "    start_time = time.time()\n",
    "    exec(f\"import {module} as {alias}\")\n",
    "    end_time = time.time()\n",
    "    print(f\"{module} imported in {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "\n",
    "logging.getLogger().handlers.clear()\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "    handlers=[\n",
    "        #logging.FileHandler(log_filename),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feb 2025 dataset\n",
    "\n",
    "#path \n",
    "path=\"/Users/iyngkarrankumar/Documents/GovAI WF/EUAIA_thresholds_project/data/notable_ai_models_24_02_2025.csv\"\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "df = df[~df[\"Notability criteria\"].isna()]\n",
    "\n",
    "df[\"compute\"] = df[\"Training compute (FLOP)\"]\n",
    "df[\"date\"] = pd.to_datetime(df[\"Publication date\"])\n",
    "df[\"year\"] = pd.to_datetime(df[\"date\"]).dt.year\n",
    "df[\"model\"] = df[\"Model\"]\n",
    "df[\"cost\"] = df[\"Training compute cost (2023 USD)\"]\n",
    "df[\"cost\"] = df[\"cost\"].fillna(\"$0\")  # Handle NaN values\n",
    "df[\"cost\"] = df[\"cost\"].astype(str)  # Convert to string\n",
    "df[\"cost\"] = df[\"cost\"].str.replace(\",\", \"\").str.replace(\"$\", \"\").astype(float)\n",
    "df = df[[\"model\", \"compute\", \"date\", \"cost\",\"year\"]]\n",
    "\n",
    "# Models to remove\n",
    "to_remove = [\"AlphaGo Zero\", \"AlphaZero\"]\n",
    "df = df[~df[\"model\"].isin(to_remove)]\n",
    "\n",
    "\n",
    "\n",
    "# Print stats for full dataset\n",
    "logging.info(\"=== Full Dataset ===\")\n",
    "logging.info(\"Most recent date: %s\", df[\"date\"].max())\n",
    "logging.debug(\"Datapoints per year:\")\n",
    "for year in range(2017, 2025):\n",
    "    count = len(df[df[\"year\"] == year])\n",
    "    logging.debug(\"%d: %d\", year, count)\n",
    "\n",
    "max_compute_idx = df['compute'].idxmax()\n",
    "logging.info(\"Largest compute value: %.2e (%s)\", df.loc[max_compute_idx, 'compute'], df.loc[max_compute_idx, 'model'])\n",
    "# Create dataset without specified years\n",
    "years_to_exclude = [2025, 2024]  # List of years to exclude\n",
    "df_filtered = df[~df[\"year\"].isin(years_to_exclude)].copy()\n",
    "\n",
    "logging.info(\"\\n\\n=== Dataset excluding years %s ===\", years_to_exclude)\n",
    "logging.info(\"Most recent date: %s\", df_filtered[\"date\"].max())\n",
    "\n",
    "max_compute_idx = df_filtered['compute'].idxmax()\n",
    "logging.info(\"Largest compute value: %.2e (%s)\", df_filtered.loc[max_compute_idx, 'compute'], df_filtered.loc[max_compute_idx, 'model'])\n",
    "\n",
    "df = df_filtered\n",
    "\n",
    "# Report number of entries before removing NaN\n",
    "logging.info(\"\\n\\n Number of entries before removing rows with compute=NaN: %d\", len(df))\n",
    "\n",
    "# Remove rows with NaN in compute column\n",
    "df = df.dropna(subset=['compute'])\n",
    "\n",
    "# Report number of entries after removing rows with compute=NaN\n",
    "logging.info(\"Number of entries after removing rows with compute=NaN: %d\", len(df))\n",
    "\n",
    "logging.info(\"\\nDatapoints per year after removing rows with compute=NaN:\")\n",
    "for year in range(2017, df.year.max()+1):\n",
    "    count = len(df[df[\"year\"] == year])\n",
    "    logging.info(\"%d: %d\", year, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate basic scatterplot\n",
    "if 1:\n",
    "    fig = sns.scatterplot(data=df[df['date']>'2010-01-01'], x='date',y='compute')\n",
    "    fig.set(yscale='log')\n",
    "    plt.grid(alpha=0.5)\n",
    "\n",
    "    # Add line of best fit for historical data\n",
    "    historical_data = df[df['date']>'2010-01-01']\n",
    "    x = historical_data['date'].astype(np.int64) // 10**9  # Convert to unix timestamp\n",
    "    y = historical_data['compute']\n",
    "    z = np.polyfit(x, np.log(y), 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(historical_data['date'], np.exp(p(x)), 'b--', alpha=0.8)\n",
    "\n",
    "    future_dates = pd.date_range(start=f'{df.year.max()+1}-01-01', end='2029-12-31', periods=200)\n",
    "    base = 1e23  # Starting point based on 2024 level\n",
    "    noise = np.random.normal(0, 10, len(future_dates))\n",
    "    years_from_2025 = (future_dates.year - (df.year.max()+1))\n",
    "\n",
    "    growth_rate = 3.0  # Exponential growth rate\n",
    "    future_compute = base * np.exp(growth_rate * years_from_2025) * (1 + noise)\n",
    "    plt.scatter(future_dates, future_compute, alpha=0.3, color='red', label='Scenario A')\n",
    "\n",
    "    growth_rate = 0.4\n",
    "    future_compute = base * np.exp(growth_rate * years_from_2025) * (1 + noise)\n",
    "    plt.scatter(future_dates, future_compute, alpha=0.3, color='green', label='Scenario B')\n",
    "\n",
    "    growth_rate = 5.0  # Higher growth rate than Scenario A\n",
    "    future_compute = base * np.exp(growth_rate * years_from_2025) * (1 + noise)\n",
    "    plt.scatter(future_dates, future_compute, alpha=0.3, color='blue', label='Scenario C')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlim([pd.Timestamp('2020-01-01'),pd.Timestamp('2030-01-01')])\n",
    "\n",
    "    for exp in range(25,31):\n",
    "        plt.axhline(y=10**exp,color='gray',linestyle='--',alpha=0.6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#util funcs cell\n",
    "def norm_exp_func(x,a,b,k):\n",
    "    norm_factor=(1/k)*(np.exp(k*b)-np.exp(k*a))\n",
    "    return (1/norm_factor)*np.exp(k*x)\n",
    "\n",
    "def sample_from_exp_dist(a,b,k,spacing='linear'):\n",
    "    x=np.linspace(a,b,10000) #might need to change this to logspace\n",
    "    dx=x[1]-x[0] #differnt if logspace\n",
    "    pdf=norm_exp_func(x,a,b,k=k)\n",
    "    assert(round(sum(pdf*dx),2)==1), print(sum(pdf*dx)) #sanity check on probability dist\n",
    "    prob_dist=pdf*dx\n",
    "    prob_dist=prob_dist/np.sum(prob_dist) #ensure that sums exactly to 1 for use with np.random.choice\n",
    "\n",
    "    return np.random.choice(x,p=prob_dist)\n",
    "\n",
    "def decimal_year_to_date(decimal_year):\n",
    "    if isinstance(decimal_year, pd.Series):\n",
    "        return decimal_year.apply(lambda x: decimal_year_to_date(x))\n",
    "    if isinstance(decimal_year, (list, np.ndarray)):\n",
    "        return [decimal_year_to_date(x) for x in decimal_year]\n",
    "    year = int(decimal_year)\n",
    "    remainder = decimal_year-year\n",
    "    days_in_year = 366 if pd.Timestamp(year,1,1).is_leap_year else 365\n",
    "    days = int(remainder*days_in_year)\n",
    "    return pd.Timestamp(year,1,1)+pd.Timedelta(days=days)\n",
    "\n",
    "\n",
    "def tau_to_alloc(tau):\n",
    "    tau = np.array(tau)\n",
    "    train_alloc = tau/(tau+1)\n",
    "    inference_alloc = 1/(tau+1)\n",
    "    return train_alloc, inference_alloc\n",
    "\n",
    "def alloc_to_tau(train_alloc):\n",
    "        train_alloc=np.array(train_alloc)\n",
    "        tau = train_alloc/(1-train_alloc)\n",
    "        return tau\n",
    "\n",
    "\n",
    "def alloc_ratio_to_alloc(alloc_ratio):\n",
    "    #note - assumes alloc_rati = train/inf\n",
    "    alloc_ratio=np.array(alloc_ratio)\n",
    "    train_alloc=alloc_ratio/(1+alloc_ratio)\n",
    "    inference_alloc=1-train_alloc\n",
    "    return train_alloc, inference_alloc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training compute spending extrapolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total AI relevant compute extrapolations\n",
    "\n",
    "#extraps\n",
    "LINEAR_EXTRAP=True\n",
    "AI2027_EXTRAP=True\n",
    "method_choice=\"method 2027\" #['linear extrapolation', 'method 2027']\n",
    "assert method_choice in ['linear extrapolation','method 2027']\n",
    "\n",
    "#allocations\n",
    "hist_alloc=1/1\n",
    "hist_alloc_multiplier=1+(1/hist_alloc)\n",
    "FIXED_ALLOCATION=True\n",
    "fixed_alloc=40/60\n",
    "DYNAMIC_ALLOCATION=False #inference scaling continues improving\n",
    "assert(FIXED_ALLOCATION+DYNAMIC_ALLOCATION)==1\n",
    "pred_alloc_dict = {\n",
    "        2024: 40/60,\n",
    "        2025: 30/70,\n",
    "        2026: 30/70,\n",
    "        2027: 20/80,\n",
    "        2028: 20/80,\n",
    "    }\n",
    "g_global_AI_compute=2.25\n",
    "g_AI_workload_share=2.0 #assuming AI_compute_usage/AI_compute_capacity = const - 3.0 gets the two superposed!\n",
    "\n",
    "#plot\n",
    "PLOT=True\n",
    "\n",
    "LOG_AGGREGATE_COMPUTE_DATA={}\n",
    "\n",
    "year_grouped_df=df.groupby(df['date'][df['date']>'2010-01-01'].dt.year)\n",
    "aggregate_compute=year_grouped_df['compute'].sum()\n",
    "log_aggregate_compute=np.log10(aggregate_compute)\n",
    "\n",
    "recent_years = log_aggregate_compute[log_aggregate_compute.index.isin(range(2020,df.year.max()+1))]\n",
    "recent_log_compute_dict = {int(k): v for k, v in recent_years.items()}\n",
    "\n",
    "\n",
    "if 1: #do historical data\n",
    "    LOG_AGGREGATE_COMPUTE_DATA['historical aggregate training compute'] = {int(k): v for k, v in log_aggregate_compute.items()}\n",
    "    LOG_AGGREGATE_COMPUTE_DATA['historical aggregate total compute'] = {int(k): v+np.log10(hist_alloc_multiplier) for k, v in log_aggregate_compute.items()}\n",
    "\n",
    "if AI2027_EXTRAP:\n",
    "    g_global_AI_compute=2.25\n",
    "    g_AI_workload_share=2.0 #assuming AI_compute_usage/AI_compute_capacity = const - 3.0 gets the two superposed!\n",
    "    training_usage_2023 = 10**log_aggregate_compute.get(2023)\n",
    "    total_usage_2023 = 2 * training_usage_2023\n",
    "    \n",
    "    AI_compute_usage={}\n",
    "    for idx,year in enumerate(range(2024, 2029)):\n",
    "        AI_compute_usage[year] = total_usage_2023*(g_global_AI_compute+g_AI_workload_share)**(idx+1)\n",
    "    \n",
    "    log_aggregate_compute_predictions_dict = {year: np.log10(compute) for year, compute in AI_compute_usage.items()}\n",
    "    LOG_AGGREGATE_COMPUTE_DATA['Total-method 2027'] = log_aggregate_compute_predictions_dict\n",
    "    \n",
    "\n",
    "\n",
    "if LINEAR_EXTRAP:\n",
    "    # Fit exponential for extrapolation\n",
    "    # Linear regression\n",
    "    x = np.array(list(year_grouped_df.groups.keys())).reshape(-1, 1)\n",
    "    y = np.array(list(LOG_AGGREGATE_COMPUTE_DATA['historical aggregate total compute'].values())).reshape(-1, 1)\n",
    "    reg = linear_model.LinearRegression().fit(x, y)\n",
    "\n",
    "    # Generate future years for extrapolation\n",
    "    pred_years = np.arange(df.year.max()+1, 2029)\n",
    "    # Get predictions\n",
    "    log_aggregate_compute_predictions = reg.predict(pred_years.reshape(-1, 1))\n",
    "    log_aggregate_compute_predictions_dict = {int(year): pred for year, pred in zip(pred_years.flatten(), log_aggregate_compute_predictions)}\n",
    "\n",
    "    # Combine historical and predicted data\n",
    "    #combined_log_aggregate_compute_dict = dict(sorted({**recent_log_compute_dict, **log_aggregate_compute_predictions_dict}.items()))\n",
    "\n",
    "    LOG_AGGREGATE_COMPUTE_DATA['Total-linear extrapolation']=log_aggregate_compute_predictions_dict\n",
    "\n",
    "\n",
    "#do allocations\n",
    "if 1: \n",
    "    if FIXED_ALLOCATION:\n",
    "        train_alloc,inference_alloc=alloc_ratio_to_alloc(alloc_ratio=fixed_alloc)\n",
    "        LOG_AGGREGATE_COMPUTE_DATA['aggregate training compute'] = {year: val + np.log10(train_alloc) for year, val in LOG_AGGREGATE_COMPUTE_DATA[f\"Total-{method_choice}\"].items()}\n",
    "        LOG_AGGREGATE_COMPUTE_DATA['aggregate inference compute'] = {year: val + np.log10(inference_alloc) for year, val in LOG_AGGREGATE_COMPUTE_DATA[f\"Total-{method_choice}\"].items()}\n",
    "    \n",
    "    if DYNAMIC_ALLOCATION:\n",
    "        train_alloc_dict = {}\n",
    "        inference_alloc_dict = {}\n",
    "        \n",
    "        for year, val in LOG_AGGREGATE_COMPUTE_DATA[f'Total-{method_choice}'].items():\n",
    "            alloc_ratio=pred_alloc_dict.get(year,1.0)\n",
    "            train_alloc, inference_alloc = alloc_ratio_to_alloc(alloc_ratio=alloc_ratio)\n",
    "            train_alloc_dict[year] = val + np.log10(train_alloc)\n",
    "            inference_alloc_dict[year] = val + np.log10(inference_alloc)\n",
    "            \n",
    "        LOG_AGGREGATE_COMPUTE_DATA['aggregate training compute'] = train_alloc_dict\n",
    "        LOG_AGGREGATE_COMPUTE_DATA['aggregate inference compute'] = inference_alloc_dict\n",
    "\n",
    "\n",
    "if PLOT:\n",
    "    plt.figure(figsize=(10,6))\n",
    "    \n",
    "    # Plot extrapolations for each method\n",
    "    colors = {\n",
    "        'historical aggregate training compute': 'blue',\n",
    "        'historical aggregate total compute': 'cyan',\n",
    "        'Total-method 2027': 'purple', \n",
    "        'Total-linear extrapolation': 'orange',\n",
    "        'aggregate training compute': 'green',\n",
    "        'aggregate inference compute': 'red',\n",
    "    }\n",
    "    markers = {\n",
    "        'historical aggregate training compute': 'o',\n",
    "        'historical aggregate total compute': 'v',\n",
    "        'Total-method 2027': 's',\n",
    "        'Total-linear extrapolation': 'o',\n",
    "        'aggregate training compute': '.',\n",
    "        'aggregate inference compute': 'x',\n",
    "    }\n",
    "    for method, predictions in LOG_AGGREGATE_COMPUTE_DATA.items():\n",
    "        years = [y for y in predictions.keys()]\n",
    "        values = [predictions[y] for y in years]\n",
    "        plt.scatter(years, values, label=f'{method} (Projected)' if method!='historical data' else f'{method}',\n",
    "                   color=colors[method], marker=markers[method])\n",
    "    \n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Log10(Compute) [FLOP]')\n",
    "    plt.title(f'Compute Usage Over Time')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(np.arange(min(log_aggregate_compute.index), 2030, 2))\n",
    "\n",
    "\n",
    "    # Plot compute allocations for different tau values\n",
    "    plt.figure(figsize=(10,6))\n",
    "\n",
    "    years = sorted(pred_alloc_dict.keys())\n",
    "    alloc_ratios = [pred_alloc_dict[y] for y in years]\n",
    "\n",
    "    train_allocs = []\n",
    "    inference_allocs = []\n",
    "    if FIXED_ALLOCATION:\n",
    "        train_allocs,inference_allocs = alloc_ratio_to_alloc(np.ones(years.__len__())*fixed_alloc)\n",
    "    if DYNAMIC_ALLOCATION:\n",
    "        train_allocs,inference_allocs = alloc_ratio_to_alloc(np.array(list(pred_alloc_dict.values())))\n",
    "\n",
    "\n",
    "\n",
    "    plt.plot(years, train_allocs, 'g-', label='Training Allocation')\n",
    "    plt.plot(years, inference_allocs, 'r-', label='Inference Allocation')\n",
    "    plt.scatter(years, train_allocs, color='green', marker='o')\n",
    "    plt.scatter(years, inference_allocs, color='red', marker='o')\n",
    "    plt.ylim(0,1)\n",
    "\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Allocation Fraction') \n",
    "    plt.title('Compute Allocations Over Time')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate compute samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting f_M coefficients\n"
     ]
    }
   ],
   "source": [
    "#get compute_alloc fits\n",
    "fit_years=np.arange(2020,df.year.max()+1)\n",
    "FIT_DATA={year:None for year in fit_years}\n",
    "constraint_point=(1,1)\n",
    "\n",
    "print('Fitting f_M coefficients')\n",
    "\n",
    "for idx,year in enumerate(fit_years):\n",
    "    total_compute=aggregate_compute[aggregate_compute.index==year].values\n",
    "    datapoints_year=df[df['date'].dt.year==year]['compute']\n",
    "    mean_log_compute=np.log10(datapoints_year).mean()\n",
    "    norm_factor_model=datapoints_year.max()\n",
    "    norm_factor_total=total_compute[0]\n",
    "\n",
    "    sorted_computes=np.sort(datapoints_year)\n",
    "    norm_sorted_computes=sorted_computes/norm_factor_model\n",
    "    cumsum=np.cumsum(sorted_computes)\n",
    "    norm_cumsum=cumsum/norm_factor_total\n",
    "\n",
    "    #store data \n",
    "    FIT_DATA[year]={\n",
    "    'compute':sorted_computes,\n",
    "    'cumulative_sum':cumsum,\n",
    "    'norm_factor_total':norm_factor_total,\n",
    "    'norm_factor_model':norm_factor_model,\n",
    "    'f_m_coeffs':None,\n",
    "            }\n",
    "    \n",
    "    #fit data\n",
    "    X = np.log10(norm_sorted_computes).reshape(-1, 1)\n",
    "    y = np.log10(norm_cumsum)\n",
    "    X_trans,y_trans=X-constraint_point[0],y-constraint_point[1]\n",
    "    reg = linear_model.LinearRegression(fit_intercept=False).fit(X_trans, y_trans) #forcing X-a,y-b to go through (0,0) means X,y goes through (a,b)\n",
    "    FIT_DATA[year]['fit data'] = (X.ravel(),y.ravel())\n",
    "    FIT_DATA[year]['f_m_coeffs'] = [reg.coef_[0], reg.intercept_]\n",
    "\n",
    "# Log debug - Print f_m_coeffs for all years\n",
    "logging.info(\"f_m_coeffs for each year:\")\n",
    "for year in fit_years:\n",
    "    coeffs = FIT_DATA[year]['f_m_coeffs']\n",
    "    logging.info(f\"Year {year}: slope={coeffs[0]:.4f}, intercept={coeffs[1]:.4f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##generate compute samples\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "##compute allocation parameters\n",
    "\n",
    "CONST_FM=True\n",
    "LIN_EXTRAP_FM=False\n",
    "CUSTOM_FM=False\n",
    "default_fm_grad,default_fm_int=np.mean([FIT_DATA[year]['f_m_coeffs'][0] for year in FIT_DATA]),np.mean([FIT_DATA[year]['f_m_coeffs'][1] for year in FIT_DATA])\n",
    "if CUSTOM_FM:\n",
    "    fm_grad_dict={\n",
    "        2024:1.1,\n",
    "        2025:1.1,\n",
    "        2026:1.1,\n",
    "        2027:1.1,\n",
    "        2028:1.1,\n",
    "        2029:1.1\n",
    "    }\n",
    "    fm_int_dict={\n",
    "        2024:0.9,\n",
    "        2025:0.8,\n",
    "        2026:0.7,\n",
    "        2027:0.6,\n",
    "        2028:0.5,\n",
    "        2029:0.4\n",
    "    }\n",
    "assert(CONST_FM+LIN_EXTRAP_FM+CUSTOM_FM)==1, \"Only one of CONST_FM, LIN_EXTRAP_FM, or CUSTOM_FM can be True\"\n",
    "\n",
    "PLOT_KDES=True\n",
    "PLOT_SCATTER=True\n",
    "\n",
    "\n",
    "min_norm_m,max_norm_m = 10**-8, 1.0\n",
    "n_catgs=9\n",
    "norm_largest_model=np.mean([FIT_DATA[year]['norm_factor_model']/FIT_DATA[year]['norm_factor_total'] for year in FIT_DATA])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#bin sampling parameters\n",
    "bin_sampling_method='random'\n",
    "\n",
    "\n",
    "all_years=np.concatenate([fit_years, pred_years.astype(int).ravel()])\n",
    "\n",
    "COMPUTE_SAMPLE_DATA={int(year):{} for year in all_years} #all years because we're also retrodicting\n",
    "\n",
    "for year in all_years:\n",
    "\n",
    "    if year in fit_years:\n",
    "        log_agg_training_compute=LOG_AGGREGATE_COMPUTE_DATA[\"historical aggregate training compute\"][year]\n",
    "    if year in pred_years:\n",
    "        log_agg_training_compute=LOG_AGGREGATE_COMPUTE_DATA[\"aggregate training compute\"][year]\n",
    "    agg_training_compute=10**log_agg_training_compute #total compute used over the year\n",
    "\n",
    "    #model sizes (as fraction of largest_model)\n",
    "    norm_ms = np.logspace(np.log10(min_norm_m),np.log10(max_norm_m),num=n_catgs)\n",
    "    log_norm_ms = np.log10(norm_ms)\n",
    "\n",
    "    if CONST_FM: \n",
    "        fm_grad,fm_int=np.mean([FIT_DATA[year]['f_m_coeffs'][0] for year in FIT_DATA]),np.mean([FIT_DATA[year]['f_m_coeffs'][1] for year in FIT_DATA])\n",
    "    elif LIN_EXTRAP_FM:\n",
    "        raise NotImplementedError(\"Linear extrapolation of fm_grad and fm_int not implemented\")\n",
    "    elif CUSTOM_FM:\n",
    "        fm_grad,fm_int=fm_grad_dict.get(year,1.1),fm_int_dict.get(year,0.92)\n",
    "    if year in FIT_DATA.keys():\n",
    "        fm_grad,fm_int=FIT_DATA[year]['f_m_coeffs']\n",
    "\n",
    "    log_frac_cum_compute = fm_grad*log_norm_ms + fm_int\n",
    "    frac_cum_compute=10**log_frac_cum_compute\n",
    "\n",
    "    model_ctgs = [f'{norm_ms[i]:.2e}--{norm_ms[i+1]:.2e}' for i in range(len(norm_ms)-1)]\n",
    "    f_m = np.diff(frac_cum_compute) #we don't include compute alloc to models 1e-8 smaller than total compute\n",
    "    logging.info(f'Sum f_m: {np.sum(f_m)}')\n",
    "    bin_compute_allocs=f_m*agg_training_compute #array of how much compute allocated to each bin\n",
    "    DATA_alloc={model_ctgs[i]:\n",
    "                {'compute alloc':bin_compute_allocs[i]} for i in range(len(model_ctgs))}\n",
    "    \n",
    "    compute_samples_rand=[]\n",
    "\n",
    "    for idx,(ctg,alloc) in enumerate(list(zip(model_ctgs,bin_compute_allocs))):\n",
    "        #here alloc is the amount of alloc given to each individual bin\n",
    "        bounds = ctg.split('--')\n",
    "        largest_model=norm_largest_model*agg_training_compute\n",
    "        norm_model_bin_lb,norm_model_bin_ub = float(bounds[0]),float(bounds[1])\n",
    "        model_bin_lb,model_bin_ub = largest_model*norm_model_bin_lb, largest_model*norm_model_bin_ub #normalising factor is total training compute\n",
    "        assert(alloc>model_bin_ub)\n",
    "\n",
    "        #not generating multiple samples yet for CIs\n",
    "        allocnorm_model_bin_lb,allocnorm_model_bin_ub=model_bin_lb/alloc, model_bin_ub/alloc #this is purely just for samplign; no physical meaning\n",
    "        running_tot=0\n",
    "        allocnormed_samples=[] \n",
    "        while running_tot<1:\n",
    "            #SAMPLE\n",
    "            if bin_sampling_method=='random':\n",
    "                sample = np.random.uniform(allocnorm_model_bin_lb, allocnorm_model_bin_ub)\n",
    "                sample = float(sample) if isinstance(sample, np.ndarray) else sample\n",
    "\n",
    "            #SUM CHECK\n",
    "            if running_tot + sample > 1:\n",
    "                allocnormed_samples.append(1 - running_tot)\n",
    "                running_tot = 1\n",
    "            else:\n",
    "                allocnormed_samples.append(sample)\n",
    "                running_tot += sample\n",
    "\n",
    "        compute_samples_rand = compute_samples_rand + (list(alloc*np.array(allocnormed_samples)))\n",
    "\n",
    "\n",
    "    compute_samples_rand = [x for x in compute_samples_rand if x!=0]\n",
    "\n",
    "    COMPUTE_SAMPLE_DATA[year]['samples']=compute_samples_rand\n",
    "    COMPUTE_SAMPLE_DATA[year]['date']=[decimal_year_to_date(year+np.random.random()) for _ in compute_samples_rand] #conver to stand pd datetime format\n",
    "    COMPUTE_SAMPLE_DATA[year]['largest model']=largest_model\n",
    "\n",
    "\n",
    "print(\"\\nNumber of samples per year:\")\n",
    "for year in pred_years.ravel():\n",
    "    print(f\"{year}: {len(COMPUTE_SAMPLE_DATA[year]['samples'])} samples\")\n",
    "\n",
    "        \n",
    "\n",
    "if PLOT_KDES:\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(12, 8))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for idx, (year, value) in enumerate((y, s) for y, s in COMPUTE_SAMPLE_DATA.items() if y in pred_years):\n",
    "        sns.kdeplot(data=np.log10(value['samples']), ax=axes[idx])\n",
    "        axes[idx].set_title(f'Year {year}')\n",
    "        axes[idx].set_xlabel('log compute (FLOPs)')\n",
    "        axes[idx].set_ylabel('Density')\n",
    "        axes[idx].grid(alpha=0.5)\n",
    "        axes[idx].set_xlim([15,30])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if PLOT_SCATTER:\n",
    "    \n",
    "    # Create scatter plot\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.scatter(df[df['year'].isin(fit_years)]['date'], np.log10(df[df['year'].isin(fit_years)]['compute']), alpha=0.5, label='Historical')\n",
    "    for year in pred_years:\n",
    "        plt.scatter(COMPUTE_SAMPLE_DATA[year]['date'], np.log10(COMPUTE_SAMPLE_DATA[year]['samples']), alpha=0.5, label='Projected' if year==pred_years[0] else \"\", color='red')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Log Compute (FLOPs)')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## regular counts \n",
    "##\n",
    "\n",
    "thresholds = [25, 26, 27, 28, 29, 30]\n",
    "threshold_counts = {year: [] for year in pred_years.astype(int).ravel()}\n",
    "\n",
    "for year, samples in COMPUTE_SAMPLE_DATA.items():\n",
    "    if year in pred_years:\n",
    "        for threshold in thresholds:\n",
    "            count = sum(x >= 10**threshold for x in samples['samples'])\n",
    "            threshold_counts[year].append(count)\n",
    "\n",
    "df_counts = pd.DataFrame(threshold_counts,\n",
    "                        index=[f'>1e{t}' for t in thresholds])\n",
    "\n",
    "\n",
    "# Make cumulative across years\n",
    "df_counts_cumulative = df_counts.copy()\n",
    "for idx in df_counts.index:\n",
    "    df_counts_cumulative.loc[idx] = df_counts.loc[idx].cumsum()\n",
    "display(df_counts_cumulative)\n",
    "\n",
    "\n",
    "for year, samples in COMPUTE_SAMPLE_DATA.items():\n",
    "    if year in pred_years:\n",
    "        print(f\"Year {year}: {len(samples['samples'])} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t8/jzkwry8124n527hkww122ts00000gn/T/ipykernel_3700/2307097023.py:5: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  bins = pd.date_range(start=f\"{pred_years.ravel().min()}-01-01\", end=f\"{pred_years.ravel().max()+1}-01-01\", freq=period_freq).astype(f'period[{period_freq}]')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2024</th>\n",
       "      <th>2025</th>\n",
       "      <th>2026</th>\n",
       "      <th>2027</th>\n",
       "      <th>2028</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Within 0.5 OOM</th>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Within 1 OOM</th>\n",
       "      <td>21</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Within 1.5 OOM</th>\n",
       "      <td>26</td>\n",
       "      <td>20</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                2024  2025  2026  2027  2028\n",
       "Within 0.5 OOM    13     8    11     7     6\n",
       "Within 1 OOM      21    16    17    17    15\n",
       "Within 1.5 OOM    26    20    22    21    20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#frontier-connected threshold counts for samples\n",
    "threshold_widths = [0.5, 1, 1.5]  # List of threshold widths to analyze\n",
    "period_freq = '6M'  # Can be changed to any frequency like '1Y', '3M', '30D'\n",
    "\n",
    "bins = pd.date_range(start=f\"{pred_years.ravel().min()}-01-01\", end=f\"{pred_years.ravel().max()+1}-01-01\", freq=period_freq).astype(f'period[{period_freq}]')\n",
    "period_data=pd.Series(bins[bins.searchsorted(df.date.dt.to_period(period_freq)) - 1], index=df.index)\n",
    "\n",
    "# Initialize results dictionary\n",
    "frontier_counts = {width: {period: 0 for period in bins} for width in threshold_widths}\n",
    "\n",
    "# For each period\n",
    "for period in bins:\n",
    "    period_samples = []\n",
    "    \n",
    "    # Collect all samples from that period\n",
    "    for year in COMPUTE_SAMPLE_DATA:\n",
    "        #print(COMPUTE_SAMPLE_DATA[year]['date'])\n",
    "        pd_dt_dates = pd.to_datetime(COMPUTE_SAMPLE_DATA[year]['date'])\n",
    "        if period.year == year:\n",
    "            # Filter samples that fall within the period\n",
    "            period_start = period.start_time\n",
    "            period_end = period.end_time\n",
    "            period_mask = (pd_dt_dates >= period_start) & (pd_dt_dates < period_end)\n",
    "            period_samples.extend(np.array(COMPUTE_SAMPLE_DATA[year]['samples'])[period_mask])\n",
    "            \n",
    "    if period_samples:\n",
    "        # Find largest model in period\n",
    "        frontier = max(period_samples)\n",
    "        \n",
    "        # Count models within each threshold width\n",
    "        for width in threshold_widths:\n",
    "            threshold = 10**width\n",
    "            count = sum(abs(np.log10(model) - np.log10(frontier)) <= width for model in period_samples)\n",
    "            frontier_counts[width][period] = count\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_frontier = pd.DataFrame(frontier_counts)\n",
    "df_frontier.columns = [f'Within {w} OOM' for w in threshold_widths]\n",
    "\n",
    "# Sum up counts for each year\n",
    "yearly_counts = {}\n",
    "for width in threshold_widths:\n",
    "    col = f'Within {width} OOM'\n",
    "    yearly_counts[col] = df_frontier.groupby(df_frontier.index.year)[col].sum()\n",
    "\n",
    "\n",
    "df_frontier_yearly = pd.DataFrame(yearly_counts)\n",
    "df_frontier_yearly = df_frontier_yearly.transpose()\n",
    "\n",
    "\n",
    "display(df_frontier_yearly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backtesting the absolute thresholds\n",
    "\n",
    "retrodict_years=fit_years\n",
    "retrodict_thresholds=[1e23,1e24,1e25]\n",
    "\n",
    "#observed\n",
    "# Create DataFrame from observed counts\n",
    "df_observed = pd.DataFrame.from_dict({threshold: {year: sum(df[df['year'] == year]['compute'] > threshold)\n",
    "                                                for year in retrodict_years}\n",
    "                                    for threshold in retrodict_thresholds}, \n",
    "                                    orient='index')\n",
    "df_observed.index = [f'{threshold:.2e}' for threshold in retrodict_thresholds]\n",
    "df_observed.index.name = 'Threshold'\n",
    "\n",
    "# Create retrodict counts dictionary\n",
    "retrodict_counts = {year: [] for year in retrodict_years}\n",
    "\n",
    "for year, data in COMPUTE_SAMPLE_DATA.items():\n",
    "    samples = data['samples']\n",
    "    if year in retrodict_years:\n",
    "        for threshold in retrodict_thresholds:\n",
    "            count = sum(x >= threshold for x in samples)\n",
    "            retrodict_counts[year].append(count)\n",
    "\n",
    "df_retrodict = pd.DataFrame(retrodict_counts,\n",
    "                          index=[f'{t:.2e}' for t in retrodict_thresholds])\n",
    "df_retrodict.index.name = 'Threshold'\n",
    "\n",
    "# Take cumulative sum across years for both dataframes\n",
    "df_observed_cumulative = df_observed.cumsum(axis=1)\n",
    "df_retrodict_cumulative = df_retrodict.cumsum(axis=1)\n",
    "\n",
    "\n",
    "# Create dataframe with observed and retrodicted values\n",
    "combined_df = pd.DataFrame(index=df_observed_cumulative.index)\n",
    "\n",
    "# Fill in the values as tuples of (observed, retrodicted)\n",
    "for year in df_observed_cumulative.columns:\n",
    "    combined_df[year] = list(zip(df_observed_cumulative[year], df_retrodict_cumulative[year]))\n",
    "\n",
    "display(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## frontier counts\n",
    "\n",
    "\n",
    "threshold_widths = [0.5, 1, 1.5]  # List of threshold widths to analyze\n",
    "period_freq = '6M'  # Can be changed to any frequency like '1Y', '3M', '30D'\n",
    "\n",
    "# Group data into 6-month periods\n",
    "bins = pd.date_range(start=df.date.min(), end=df.date.max(), freq=period_freq).astype(f'period[{period_freq}]')\n",
    "df['period'] = pd.Series(bins[bins.searchsorted(df.date.dt.to_period(period_freq)) - 1], index=df.index)\n",
    "df['log_compute'] = np.log10(df['compute'])\n",
    "\n",
    "frontier_counts = []\n",
    "\n",
    "for period in df['period'].unique():\n",
    "    period_data = df[df['period'] == period]\n",
    "    if len(period_data) > 0:\n",
    "        largest_model = period_data['compute'].max()\n",
    "        \n",
    "        for width in threshold_widths:\n",
    "            count = (np.abs(np.log10(largest_model)-period_data['log_compute']) <= width).sum()\n",
    "            \n",
    "            frontier_counts.append({\n",
    "                'period': period.to_timestamp(),\n",
    "                'threshold_width': width,\n",
    "                'count': count,\n",
    "                'largest_model': largest_model\n",
    "            })\n",
    "\n",
    "frontier_df = pd.DataFrame(frontier_counts)\n",
    "\n",
    "# Filter for 2020-2023 and pivot to create summary dataframe\n",
    "summary_df = frontier_df[\n",
    "    (frontier_df['period'].dt.year >= 2020) & \n",
    "    (frontier_df['period'].dt.year <= 2023)\n",
    "].pivot(\n",
    "    index='period',\n",
    "    columns='threshold_width',\n",
    "    values='count'\n",
    ") #basically a reshaping operation\n",
    "summary_df.columns = [f'width: {w}' for w in threshold_widths]\n",
    "\n",
    "\n",
    "# Create similar table for COMPUTE_SAMPLE_DATA\n",
    "sample_frontier_counts = {width: {} for width in threshold_widths}\n",
    "\n",
    "# For each period\n",
    "for period in pd.date_range(start='2020', end='2024', freq=period_freq).astype(f'period[{period_freq}]'):\n",
    "    period_samples = []\n",
    "    \n",
    "    # Collect all samples from that period\n",
    "    for year in COMPUTE_SAMPLE_DATA:\n",
    "        pd_dt_dates = pd.to_datetime((COMPUTE_SAMPLE_DATA[year]['date']))\n",
    "        if period.year == year:\n",
    "            period_start = period.start_time\n",
    "            period_end = period.end_time\n",
    "            period_mask = (pd_dt_dates >= period_start) & (pd_dt_dates < period_end)\n",
    "            period_samples.extend(np.array(COMPUTE_SAMPLE_DATA[year]['samples'])[period_mask])\n",
    "            \n",
    "    if period_samples:\n",
    "        # Find largest model in period\n",
    "        frontier = max(period_samples)\n",
    "        \n",
    "        # Count models within each threshold width\n",
    "        for width in threshold_widths:\n",
    "            count = sum(abs(np.log10(model) - np.log10(frontier)) <= width for model in period_samples)\n",
    "            sample_frontier_counts[width][period] = count\n",
    "\n",
    "sample_summary_df = pd.DataFrame(sample_frontier_counts)\n",
    "sample_summary_df.columns = [f'width: {w}' for w in threshold_widths]\n",
    "\n",
    "\n",
    "# Group by year and sum\n",
    "summary_df = summary_df.groupby(summary_df.index.year).sum()\n",
    "sample_summary_df = sample_summary_df.groupby(sample_summary_df.index.year).sum()\n",
    "# Combine observed and retrodicted counts\n",
    "combined_df = pd.DataFrame()\n",
    "for col in summary_df.columns:\n",
    "    combined_df[col] = list(zip(summary_df[col], sample_summary_df[col]))\n",
    "combined_df.index = range(2020, 2024)\n",
    "\n",
    "print(\"\\nFrontier counts (observed, retrodicted):\")\n",
    "display(combined_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the largest model trained each year and divide by the total compute used for training\n",
    "largest_model_compute_ratio = {}\n",
    "for year in range(2020, 2024):\n",
    "    largest_model_index = df[df['year'] == year]['compute'].idxmax()\n",
    "    largest_model = df.loc[largest_model_index, 'compute']\n",
    "    largest_model_name = df.loc[largest_model_index, 'model']\n",
    "    total_training_compute = 10**(LOG_AGGREGATE_COMPUTE_DATA['historical aggregate training compute'][year])\n",
    "    largest_model_compute_ratio[year] = largest_model / total_training_compute\n",
    "    print(f\"Year: {year}\")\n",
    "    print(f\"Largest Model Name: {largest_model_name}\")\n",
    "    print(f\"Largest Model Size: {largest_model}\")\n",
    "    print(f\"Total Training Compute: {total_training_compute}\")\n",
    "    print(f\"Ratio: {largest_model_compute_ratio[year]}\")\n",
    "    print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sci_comp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
