{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pwlf #for colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset setup and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "modules = [\n",
    "    ('numpy', 'np'),\n",
    "    ('scipy.stats', 'stats'),\n",
    "    ('scipy.optimize', 'optimize'), \n",
    "    ('matplotlib.pyplot', 'plt'), \n",
    "    ('pandas', 'pd'),\n",
    "    ('seaborn', 'sns'),\n",
    "    ('itertools', 'itertools'),\n",
    "    ('copy', 'copy'),\n",
    "    ('re', 're'),\n",
    "    ('pdb', 'pdb'),\n",
    "    ('logging', 'logging')\n",
    "]\n",
    "\n",
    "for module, alias in modules:\n",
    "    start = time.time()\n",
    "    exec(f\"import {module} as {alias}\")\n",
    "    end = time.time()\n",
    "    print(f\"{module}: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats, optimize\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd #taking long to load here\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import copy,re, pdb, logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger=logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"https://epochai.org/data/epochdb/notable_systems.csv\")\n",
    "url = 'https://drive.google.com/file/d/1RLLKPU3bEYK65wlQlU0p20u9M8cHkLMl/view?usp=sharing'\n",
    "url = 'https://drive.google.com/uc?id=' + url.split('/')[-2]\n",
    "\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "df = df[~df[\"Notability criteria\"].isna()]\n",
    "\n",
    "df[\"compute\"] = df[\"Training compute (FLOP)\"]\n",
    "df[\"date\"] = df[\"Publication date\"]\n",
    "df[\"model\"] = df[\"System\"]\n",
    "df[\"poss1e23\"] = df[\"Possibly over 1e23 FLOP\"]\n",
    "df[\"poss1e25\"] = df[\"Estimated over 1e25 FLOP\"]\n",
    "df[\"cost\"] = df[\"Training compute cost (2023 USD)\"]\n",
    "df[\"cost\"] = df[\"cost\"].str.replace(\",\", \"\").str.replace(\"$\", \"\").astype(float)\n",
    "\n",
    "df = df[[\"model\", \"compute\", \"date\", \"cost\", \"poss1e23\", \"poss1e25\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = ['AlphaGo Zero','AlphaZero'] #outliers\n",
    "df = df[~df[\"model\"].isin(to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_append = [\n",
    "  [\"Claude 3.5 Sonnet\", 4.3e25, \"2024-06-21\", np.nan, np.nan, np.nan],\n",
    "  [\"GPT-4o Mini\", 1.2e25, \"2024-07-18\", np.nan, np.nan, np.nan],\n",
    "]\n",
    "\n",
    "for row in to_append:\n",
    "  if row[0] not in df[\"model\"].values:\n",
    "    df.loc[len(df)] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_add_compute = {\n",
    "    \"Claude 3 Opus\": 2.5e25,\n",
    "    \"Claude 3 Sonnet\": 1.1e25,\n",
    "    \"GPT-4o\": 2.9e25,\n",
    "    \"Gemini 1.0 Pro\": 2.8e24,\n",
    "    \"Gemini 1.5 Pro\": 1.9e25,\n",
    "    \"Reka Core\": 8.4e24,\n",
    "    \"GPT-4 Turbo\": 2.1e25,  # rough guess\n",
    "    \"GPT-4V\": 2.1e25,  # rough guess\n",
    "    \"Claude 2.1\": df[df[\"model\"]==\"Claude 2\"][\"compute\"].values,  # rough guess\n",
    "}\n",
    "\n",
    "logger.info('Can add more recent models here')\n",
    "\n",
    "\n",
    "for k, v in to_add_compute.items():\n",
    "  if df.loc[df[\"model\"] == k, \"compute\"].isna().values:\n",
    "    df.loc[df[\"model\"] == k, \"compute\"] = v\n",
    "  else:\n",
    "    print(f\"{k} already has a compute value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the ones we've set\n",
    "df.loc[~df[\"compute\"].isna(), \"poss1e23\"] = np.nan\n",
    "df.loc[~df[\"compute\"].isna(), \"poss1e25\"] = np.nan\n",
    "\n",
    "# Set some temporary placeholder values\n",
    "# TODO: revisit\n",
    "# df.loc[(df[\"poss1e25\"] == \"checked\"), \"compute\"] = 1.01e25  # placeholder\n",
    "# df.loc[((df[\"poss1e23\"] ==\"checked\") & (df[\"poss1e25\"] != \"checked\")), \"compute\"] = 1.01e23  # placeholder\n",
    "\n",
    "# We want to handle these leading models manually via the above compute estimates.\n",
    "assert df[(df[\"poss1e25\"] == \"checked\") & (df[\"compute\"].isna())].size == 0\n",
    "\n",
    "# We sample 1e23-1e25 models with unknown compute from the existing empirical distribution.\n",
    "# TODO: revisit\n",
    "poss1e23 = ((df[\"poss1e23\"] == \"checked\") & (df[\"poss1e25\"] != \"checked\"))\n",
    "df.loc[poss1e23, \"compute\"] = df[(df[\"compute\"] >= 1e23) & (df[\"compute\"] < 1e25)][\"compute\"].sample(poss1e23.sum(), random_state=0).values\n",
    "\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df[\"log_compute\"] = np.log10(df[\"compute\"])\n",
    "\n",
    "df[\"date_float\"] = df[\"date\"].dt.year + df[\"date\"].dt.month/12\n",
    "\n",
    "df['year'] = df['date'].dt.year\n",
    "\n",
    "df = df.sort_values(\"date\")\n",
    "df.dropna(subset=\"compute\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate basic scatterplot\n",
    "if 1:\n",
    "    fig = sns.scatterplot(data=df[df['date']>'2010-01-01'], x='date',y='compute')\n",
    "    fig.set(yscale='log')\n",
    "    plt.grid(alpha=0.5)\n",
    "\n",
    "    # Add line of best fit for historical data\n",
    "    historical_data = df[df['date']>'2010-01-01']\n",
    "    x = historical_data['date'].astype(np.int64) // 10**9  # Convert to unix timestamp\n",
    "    y = historical_data['compute']\n",
    "    z = np.polyfit(x, np.log(y), 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(historical_data['date'], np.exp(p(x)), 'b--', alpha=0.8)\n",
    "\n",
    "    future_dates = pd.date_range(start='2025-01-01', end='2029-12-31', periods=200)\n",
    "    base = 1e25  # Starting point based on 2024 level\n",
    "    noise = np.random.normal(0, 10, len(future_dates))\n",
    "    years_from_2025 = (future_dates.year - 2025)\n",
    "\n",
    "    growth_rate = 3.0  # Exponential growth rate\n",
    "    future_compute = base * np.exp(growth_rate * years_from_2025) * (1 + noise)\n",
    "    plt.scatter(future_dates, future_compute, alpha=0.3, color='red', label='Scenario A')\n",
    "\n",
    "    growth_rate = 0.4\n",
    "    future_compute = base * np.exp(growth_rate * years_from_2025) * (1 + noise)\n",
    "    plt.scatter(future_dates, future_compute, alpha=0.3, color='green', label='Scenario B')\n",
    "\n",
    "    growth_rate = 5.0  # Higher growth rate than Scenario A\n",
    "    future_compute = base * np.exp(growth_rate * years_from_2025) * (1 + noise)\n",
    "    plt.scatter(future_dates, future_compute, alpha=0.3, color='blue', label='Scenario C')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlim([pd.Timestamp('2020-01-01'),pd.Timestamp('2030-01-01')])\n",
    "\n",
    "    for exp in range(25,31):\n",
    "        plt.axhline(y=10**exp,color='gray',linestyle='--',alpha=0.6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute allocations experiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "FLOP_dollar_2024 = 2e25/100e6\n",
    "dollar_FLOP_2024 = 1/FLOP_dollar_2024\n",
    "year_grouped_df=df.groupby(df['date'][df['date']>'2010-01-01'].dt.year)\n",
    "aggregate_compute=year_grouped_df['compute'].sum()\n",
    "aggregate_compute_cost=aggregate_compute*dollar_FLOP_2024\n",
    "log_aggregate_compute=np.log10(aggregate_compute)\n",
    "log_aggregate_compute_cost=np.log10(aggregate_compute_cost)\n",
    "#plot\n",
    "# Plot historical data\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(year_grouped_df.groups.keys(), log_aggregate_compute_cost, label='Historical data')\n",
    "\n",
    "# Fit exponential for extrapolation\n",
    "# Linear regression\n",
    "x = np.array(list(year_grouped_df.groups.keys())).reshape(-1, 1)\n",
    "y = log_aggregate_compute_cost.values\n",
    "reg = LinearRegression().fit(x, y)\n",
    "\n",
    "# Generate future years for extrapolation\n",
    "future_years = np.arange(max(x), 2030).reshape(-1, 1)\n",
    "\n",
    "# Get predictions\n",
    "future_predictions = reg.predict(future_years)\n",
    "\n",
    "\n",
    "# Plot extrapolation\n",
    "plt.plot(future_years, future_predictions, '--', label='Extrapolation')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Log10(Total Compute)')\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_compute_2028 = 1e30\n",
    "cost_2024 = total_compute_2028 * dollar_FLOP_2024\n",
    "print(f\"With 2024 FLOP/dollar costs, the cost of {total_compute_2028} FLOP is approx {cost_2024/1e12:,.2f} trillion USD\")\n",
    "\n",
    "TM_plot=True\n",
    "TM_fit=True\n",
    "KDE_plot=True\n",
    "#case 1 - ~ 9 models with 1e29, 100 models with 1e27 \n",
    "#case 1 - ~ 9 models with \n",
    "\n",
    "#case 2 - ~10000 models with 1e26, 0 models above that\n",
    "\n",
    "#case 3 - 1 model 1e29, 10 models 1e28, 100 models 1e27, 1000 models 1e26 etc. \n",
    "\n",
    "years_to_iter=[2020,2021,2022,2023]\n",
    "DATA={year:None for year in years_to_iter}\n",
    "\n",
    "if TM_plot:\n",
    "        fig,axs=plt.subplots(nrows=2,ncols=2,figsize=(8,6)); axs_ravel=axs.ravel()\n",
    "if KDE_plot:\n",
    "        kde_fig,kde_axs=plt.subplots(nrows=2,ncols=2,figsize=(8,6)); kde_axs_ravel=kde_axs.ravel()\n",
    "\n",
    "def percentage_formatter(x,pos):\n",
    "        return f'{x:.6f}%'\n",
    "\n",
    "\n",
    "for idx,year in enumerate(years_to_iter):\n",
    "        ax,kde_ax=axs_ravel[idx], kde_axs_ravel[idx]\n",
    "        total_compute=aggregate_compute[aggregate_compute.index==year].values\n",
    "        cost_2023=total_compute*dollar_FLOP_2024\n",
    "        datapoints_year=df[df['date'].dt.year==year]['compute']\n",
    "        mean_log_compute=np.log10(datapoints_year).mean()\n",
    "\n",
    "        #prep data\n",
    "        sorted_computes=np.sort(datapoints_year)\n",
    "        norm_factor=total_compute[0]\n",
    "        norm_sorted_computes=sorted_computes/norm_factor\n",
    "        cumsum=np.cumsum(sorted_computes)\n",
    "        norm_cumsum=cumsum/norm_factor\n",
    "\n",
    "        #store data \n",
    "        DATA[year]={\n",
    "                'compute':sorted_computes,\n",
    "                'cumulative_sum':cumsum,\n",
    "                'norm_factor':norm_factor,\n",
    "                'T_m_coeffs':None,\n",
    "        }\n",
    "\n",
    "\n",
    "        if TM_plot:\n",
    "                #T-m plot\n",
    "                ax.plot(norm_sorted_computes,norm_cumsum)\n",
    "                ax.scatter(norm_sorted_computes, norm_cumsum, alpha=0.5, color='blue', s=30,marker='x')\n",
    "\n",
    "                ax.grid(True,alpha=0.3)\n",
    "                ax.set_xscale('log'); ax.set_yscale('log')\n",
    "                #ax.set_xlim([1e18,1e27])\n",
    "                ax.set_xlabel('individual model size'); ax.set_ylabel('Total training compute')\n",
    "                ax.set_title(f'Year: {year}')\n",
    "                ax.text(0.05, 0.95, f'Total compute: {total_compute[0]:.2e} FLOP', \n",
    "                        transform=ax.transAxes, verticalalignment='top')\n",
    "                ax.axhline(y=norm_cumsum[-1],color='r',linestyle='--')\n",
    "                ax.axvline(x=1,color='g',linestyle='--',alpha=0.5)\n",
    "                ax.text(1,ax.get_ylim()[0],f'{norm_factor:.2e}',\n",
    "                        rotation=90,fontsize=8,verticalalignment='top')\n",
    "                ax.yaxis.set_major_formatter(percentage_formatter)\n",
    "\n",
    "        #fit T-m\n",
    "        if TM_fit:\n",
    "                # Fit linear regression\n",
    "                X = np.log10(norm_sorted_computes).reshape(-1, 1)\n",
    "                y = np.log10(norm_cumsum)\n",
    "                reg = LinearRegression().fit(X, y)\n",
    "                DATA[year]['fit data'] = (X.ravel(),y.ravel())\n",
    "                DATA[year]['T_m_coeffs'] = [reg.coef_[0], reg.intercept_]\n",
    "                \n",
    "        if KDE_plot:\n",
    "                #KDE plot \n",
    "                kde=stats.gaussian_kde(np.log10(norm_sorted_computes))\n",
    "                x_range=np.logspace(np.log10(norm_sorted_computes).min(),np.log10(1))\n",
    "                kde_ax.plot(x_range,kde(np.log10(x_range)))\n",
    "                kde_ax.set_xscale('log')\n",
    "                kde_ax.set_title(f'Year: {year}')\n",
    "                kde_ax.grid(alpha=0.5)\n",
    "\n",
    "                kde_ax.axvline(x=1,color='g',linestyle='--',alpha=0.5)\n",
    "                kde_ax.text(1,ax.get_ylim()[0],f'{norm_factor:.2e}',\n",
    "                        rotation=90,fontsize=8,verticalalignment='top')\n",
    "                if idx>=2: kde_ax.set_xlabel('Model compute (normalised by total)')\n",
    "\n",
    "fig.tight_layout(pad=2.0)\n",
    "kde_fig.tight_layout(pad=2.0)\n",
    "\n",
    "\n",
    "if TM_fit:\n",
    "        #print gradients and intercepts\n",
    "        print('\\nT-m gradients and intercepts:')\n",
    "        for year in DATA:\n",
    "                grad, intercept = DATA[year]['T_m_coeffs']\n",
    "                print(f'{year}: gradient = {grad:.3f}, intercept = {intercept:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up compute-allocs\n",
    "avg_grad,avg_int = np.mean([DATA[year]['T_m_coeffs'][0] for year in DATA]),np.mean([DATA[year]['T_m_coeffs'][1] for year in DATA])\n",
    "fit_years=[2020,2021,2022,2023]\n",
    "extrap_years = np.arange(2024,2030)\n",
    "test_total_compute=1e29 #TEST - AI compute in 2028\n",
    "\n",
    "\n",
    "## fit aggregate compute model \n",
    "#get aggregate compute for fit years\n",
    "fit_agg_compute = np.array(aggregate_compute[aggregate_compute.index.isin(fit_years)])\n",
    "\n",
    "#fit linear regression\n",
    "X = np.array(fit_years).reshape(-1,1)\n",
    "y = np.log10(fit_agg_compute)\n",
    "aggregate_compute_reg = LinearRegression().fit(X, y)\n",
    "\n",
    "#store coefficients\n",
    "aggregate_grad, aggregate_int = aggregate_compute_reg.coef_[0], aggregate_compute_reg.intercept_\n",
    "\n",
    "#extrapolate aggregate compute for extrapolation years\n",
    "extrap_agg_compute = 10**(aggregate_compute_reg.predict(extrap_years.reshape(-1,1)))\n",
    "\n",
    "#model sizes (as fraction of T_tot)\n",
    "log_min_norm_m = np.log10(1e-8)\n",
    "log_max_norm_m = np.log10(1e-1) #free param - assume that largest model that year is no larger than 10% of total training compute (can find this from historic data and so sensitivity analysis)\n",
    "norm_ms = np.logspace(log_min_norm_m,log_max_norm_m,2*(int(log_max_norm_m)-int(log_min_norm_m))+1)\n",
    "log_norm_ms = np.log10(norm_ms)\n",
    "log_frac_cum_compute = avg_grad*log_norm_ms + avg_int\n",
    "cum_fm=10**log_frac_cum_compute\n",
    "\n",
    "#finding allocations for each compute bin \n",
    "model_ctgs = [f'{norm_ms[i]:.2e}--{norm_ms[i+1]:.2e}' for i in range(len(norm_ms)-1)]\n",
    "f_m = np.diff(cum_fm) #we don't include compute alloc to models 1e-8 smaller than total compute\n",
    "compute_alloc=f_m*test_total_compute\n",
    "DATA_alloc={model_ctgs[i]:\n",
    "            {'compute alloc':compute_alloc[i]} for i in range(len(model_ctgs))}\n",
    "\n",
    "#\n",
    "# (f'\\nCompute allocations for {test_total_compute:.2e} total compute:')\n",
    "#for i in range(len(model_ctgs)):\n",
    "#    print(f'Models in range {model_ctgs[i]} accounted for {10*f_m[i]}% share of compute, which is {compute_alloc[i]:.2e} FLOP')\n",
    "#print(f'Sum of f_m: {sum(f_m)*10}') #we can figure this out later, it's not a major block\n",
    "\n",
    "\n",
    "#generating a compute sample\n",
    "#assumption - we take n_a models from lower bound\n",
    "#in the limit we have allocation for arbitrarily small compute bins\n",
    "#one option - take compute alloc and divide by smallest possible model\n",
    "#another option - take compute alloc and divide by largest possible model\n",
    "#another option - take compute alloc and divide by average model size\n",
    "#another option - random samples models from range whilst constraining sum\n",
    "\n",
    "#low_bound method\n",
    "compute_samples_lb=[]\n",
    "compute_samples_ub=[]\n",
    "compute_samples_mp=[]\n",
    "compute_samples_rand=[]\n",
    "round_param=2 #FREE PARAM\n",
    "\n",
    "#perform allocation\n",
    "for idx,(ctg,alloc) in enumerate(list(zip(model_ctgs,compute_alloc))):\n",
    "    bounds = ctg.split('--')\n",
    "    ctg_lb,ctg_ub = float(bounds[0]),float(bounds[1])\n",
    "    sz_lb,sz_ub = test_total_compute*ctg_lb, test_total_compute*ctg_ub\n",
    "    norm_sz_lb,norm_sz_ub=round(sz_lb/alloc,round_param), round(sz_ub/alloc,round_param)\n",
    "\n",
    "    #lb\n",
    "    n_full=int(1/norm_sz_lb) #number of times smallest model goes into alloc \n",
    "    chunks = np.array([norm_sz_lb]*n_full); compute_samples_lb = compute_samples_lb + list(chunks*alloc) \n",
    "    remainder = 1-norm_sz_lb*n_full; compute_samples_lb.append(remainder*alloc)\n",
    "\n",
    "    #ub\n",
    "    n_full=int(1/norm_sz_ub) #number of times largest model goes into alloc \n",
    "    chunks = np.array([norm_sz_ub]*n_full); compute_samples_ub = compute_samples_ub + list(chunks*alloc) \n",
    "    remainder = 1-norm_sz_ub*n_full; compute_samples_ub.append(remainder*alloc)\n",
    "\n",
    "    #mp - arithmetic and geometric mean\n",
    "    norm_sz_mp = (norm_sz_lb + norm_sz_ub)/2 #arithmetic mean\n",
    "    norm_sz_mp_geom = np.sqrt(norm_sz_lb * norm_sz_ub) #geometric mean\n",
    "    n_full=int(1/norm_sz_mp_geom) #number of times mean model goes into alloc\n",
    "    chunks = np.array([norm_sz_mp_geom]*n_full); compute_samples_mp = compute_samples_mp + list(chunks*alloc)\n",
    "    remainder = 1-norm_sz_mp_geom*n_full; compute_samples_mp.append(remainder*alloc)\n",
    "\n",
    "    #random sample - sample randomly until exceeding 1, then last sample makes running_tot sum to 1\n",
    "    running_tot=0\n",
    "    normed_samples=[] \n",
    "    while running_tot<1:\n",
    "        sample = np.random.uniform(norm_sz_lb, norm_sz_ub)\n",
    "        if running_tot + sample > 1:\n",
    "            normed_samples.append(1 - running_tot)\n",
    "            running_tot = 1\n",
    "        else:\n",
    "            normed_samples.append(sample)\n",
    "            running_tot += sample\n",
    "\n",
    "    compute_samples_rand = compute_samples_rand + (list(alloc*np.array(normed_samples)))\n",
    "\n",
    "\n",
    "compute_samples_lb = [x for x in compute_samples_lb if x!=0]\n",
    "compute_samples_ub = [x for x in compute_samples_ub if x!=0]\n",
    "compute_samples_mp = [x for x in compute_samples_mp if x!=0]\n",
    "compute_samples_rand = [x for x in compute_samples_rand if x!=0]\n",
    "\n",
    "# Plot\n",
    "fig, (ax1,ax2,ax3,ax4) = plt.subplots(nrows=4, figsize=(8,8))\n",
    "sns.kdeplot(data=np.log10(compute_samples_lb), log_scale=False, ax=ax1)\n",
    "ax1.set_title('KDE - Lower bound approach')\n",
    "ax1.grid(alpha=0.5)\n",
    "\n",
    "sns.kdeplot(data=np.log10(compute_samples_ub), log_scale=False, ax=ax2) \n",
    "ax2.set_title('KDE - Upper bound approach')\n",
    "ax2.grid(alpha=0.5)\n",
    "\n",
    "sns.kdeplot(data=np.log10(compute_samples_mp), log_scale=False, ax=ax3)\n",
    "ax3.set_title('KDE - Midpoint approach')\n",
    "ax3.grid(alpha=0.5)\n",
    "\n",
    "sns.kdeplot(data=np.log10(compute_samples_rand), log_scale=False, ax=ax4)\n",
    "ax4.set_title('KDE - Random sampling approach')\n",
    "ax4.grid(alpha=0.5)\n",
    "\n",
    "\n",
    "\n",
    "for ax in (ax1,ax2,ax3,ax4):\n",
    "    ax.set_xlim(20,30)\n",
    "\n",
    "\n",
    "fig.supxlabel('log compute')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#allocate 1e24 FLOP in range 1e22-1e23 FLOP\n",
    "\n",
    "alloc=1e24\n",
    "sz_lb=1e22\n",
    "sz_ub=1e23\n",
    "norm_sz_lb,norm_sz_ub=sz_lb/alloc,sz_ub/alloc\n",
    "\n",
    "#random sample\n",
    "running_tot=0\n",
    "normed_samples=[] \n",
    "while running_tot<1:\n",
    "    sample = np.random.uniform(norm_sz_lb, norm_sz_ub)\n",
    "    if running_tot + sample > 1:\n",
    "        normed_samples.append(1 - running_tot)\n",
    "        running_tot = 1\n",
    "    else:\n",
    "        normed_samples.append(sample)\n",
    "        running_tot += sample\n",
    "\n",
    "samples=np.round(alloc*np.array(normed_samples),decimals=-21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up notable model distribution vs all models distribution\n",
    "\n",
    "year=2023\n",
    "\n",
    "log_compute_data=df[df.date.dt.year==2023]['log_compute']\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "sns.kdeplot(data=log_compute_data, log_scale=False, ax=ax,label='Notable models KDE')\n",
    "\n",
    "x=np.linspace(15,30)\n",
    "skewed_dist=stats.skewnorm.pdf(x,a=5,loc=20,scale=2.9) #a>0 for right skew\n",
    "ax.plot(x, skewed_dist, color='red', label='All models KDE')\n",
    "\n",
    "ax.set_title(f'{year}')\n",
    "ax.grid(alpha=0.5)\n",
    "ax.set_xlim(10,30)\n",
    "ax.set_xlabel('log compute')\n",
    "ax.legend(fontsize=15)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating samples with compute allocation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#util funcs cell\n",
    "def norm_exp_func(x,a,b,k):\n",
    "    norm_factor=(1/k)*(np.exp(k*b)-np.exp(k*a))\n",
    "    return (1/norm_factor)*np.exp(k*x)\n",
    "\n",
    "def sample_from_exp_dist(a,b,k,spacing='linear'):\n",
    "    x=np.linspace(a,b,10000) #might need to change this to logspace\n",
    "    dx=x[1]-x[0] #differnt if logspace\n",
    "    pdf=norm_exp_func(x,a,b,k=k)\n",
    "    assert(round(sum(pdf*dx),2)==1), print(sum(pdf*dx)) #sanity check on probability dist\n",
    "    prob_dist=pdf*dx\n",
    "    prob_dist=prob_dist/np.sum(prob_dist) #ensure that sums exactly to 1 for use with np.random.choice\n",
    "\n",
    "    return np.random.choice(x,p=prob_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training compute spending extrapolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_compute_growth_rate=2.25\n",
    "total_compute_2024=1e27\n",
    "train_compute_alloc=0.4\n",
    "inference_compute_alloc=1-train_compute_alloc\n",
    "\n",
    "# Extrapolate total compute for 2024-2029\n",
    "years = np.arange(2024,2030)\n",
    "total_compute = total_compute_2024 * total_compute_growth_rate**(years-2024)\n",
    "total_train_compute  = train_compute_alloc*total_compute\n",
    "total_inf_compute = inference_compute_alloc*total_train_compute\n",
    "log_total_compute = np.log10(total_compute)\n",
    "\n",
    "# Plot training and inference compute allocation fractions\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(years, np.ones_like(years)*train_compute_alloc, label='Training fraction', marker='x')\n",
    "plt.plot(years, np.ones_like(years)*inference_compute_alloc, label='Inference fraction', marker='x')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Fraction of total compute')\n",
    "plt.title('Training vs Inference Compute Allocation Fractions')\n",
    "plt.grid(alpha=0.5)\n",
    "plt.ylim(0,1)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrapolate total compute spending\n",
    "\n",
    "TEST_CASE=True\n",
    "LINEAR_EXTRAP=True\n",
    "PLOT=True\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "FLOP_dollar_2024 = 2e25/100e6\n",
    "dollar_FLOP_2024 = 1/FLOP_dollar_2024\n",
    "\n",
    "LOG_AGGREGATE_COMPUTE_DATA={}\n",
    "\n",
    "\n",
    "year_grouped_df=df.groupby(df['date'][df['date']>'2010-01-01'].dt.year)\n",
    "aggregate_compute=year_grouped_df['compute'].sum()\n",
    "aggregate_compute_cost=aggregate_compute*dollar_FLOP_2024\n",
    "log_aggregate_compute=np.log10(aggregate_compute)\n",
    "log_aggregate_compute_cost=np.log10(aggregate_compute_cost)\n",
    "\n",
    "\n",
    "if LINEAR_EXTRAP:\n",
    "    # Fit exponential for extrapolation\n",
    "    # Linear regression\n",
    "    x = np.array(list(year_grouped_df.groups.keys())).reshape(-1, 1)\n",
    "    y = log_aggregate_compute.values\n",
    "    reg = LinearRegression().fit(x, y)\n",
    "\n",
    "    # Generate future years for extrapolation\n",
    "    pred_years = np.arange(max(x), 2030).reshape(-1, 1)\n",
    "    # Get predictions\n",
    "    log_aggregate_compute_predictions = reg.predict(pred_years)\n",
    "    log_aggregate_compute_predictions_dict = {int(year): pred for year, pred in zip(pred_years.flatten(), aggregate_compute_predictions)}\n",
    "    LOG_AGGREGATE_COMPUTE_DATA['Linear']=log_aggregate_compute_predictions\n",
    "\n",
    "#test case\n",
    "if TEST_CASE: \n",
    "    test_total_compute=1e28\n",
    "    log_aggregate_compute_predictions = np.log10(test_total_compute) * np.ones(len(pred_years))\n",
    "    LOG_AGGREGATE_COMPUTE_DATA['Test']=log_aggregate_compute_predictions\n",
    "# Plot extrapolation\n",
    "\n",
    "if PLOT:\n",
    "    plt.figure(figsize=(10,6))\n",
    "    # Plot historical data\n",
    "    plt.scatter(year_grouped_df.groups.keys(), log_aggregate_compute.values, \n",
    "                label='Historical Data', color='blue')\n",
    "    \n",
    "    # Plot extrapolations\n",
    "    for scenario, predictions in LOG_AGGREGATE_COMPUTE_DATA.items():\n",
    "        plt.scatter(pred_years, predictions, \n",
    "                   label=f'{scenario} Extrapolation', marker='o')\n",
    "    \n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Log10(Compute) [FLOP]')\n",
    "    plt.title('Historical and Projected Compute Usage')\n",
    "    plt.legend()\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate compute samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get compute_alloc fits\n",
    "fit_years=[2020,2021,2022,2023]\n",
    "FIT_DATA={year:None for year in fit_years}\n",
    "\n",
    "\n",
    "print('Fitting f_M coefficients')\n",
    "for idx,year in enumerate(fit_years):\n",
    "    total_compute=aggregate_compute[aggregate_compute.index==year].values\n",
    "    datapoints_year=df[df['date'].dt.year==year]['compute']\n",
    "    mean_log_compute=np.log10(datapoints_year).mean()\n",
    "\n",
    "    sorted_computes=np.sort(datapoints_year)\n",
    "    norm_factor=total_compute[0]\n",
    "    norm_sorted_computes=sorted_computes/norm_factor\n",
    "    cumsum=np.cumsum(sorted_computes)\n",
    "    norm_cumsum=cumsum/norm_factor\n",
    "\n",
    "    #store data \n",
    "    FIT_DATA[year]={\n",
    "    'compute':sorted_computes,\n",
    "    'cumulative_sum':cumsum,\n",
    "    'norm_factor':norm_factor,\n",
    "    'f_m_coeffs':None,\n",
    "            }\n",
    "    \n",
    "    #fit data\n",
    "    X = np.log10(norm_sorted_computes).reshape(-1, 1)\n",
    "    y = np.log10(norm_cumsum)\n",
    "    reg = LinearRegression().fit(X, y)\n",
    "    FIT_DATA[year]['fit data'] = (X.ravel(),y.ravel())\n",
    "    FIT_DATA[year]['f_m_coeffs'] = [reg.coef_[0], reg.intercept_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##generate compute samples\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "CONST_FM=False\n",
    "LIN_EXTRAP_FM=False\n",
    "TEST_FM=True\n",
    "assert(CONST_FM+LIN_EXTRAP_FM+TEST_FM)==1, \"Only one of CONST_FM, LIN_EXTRAP_FM, or TEST_FM can be True\"\n",
    "\n",
    "PLOT_KDES=True\n",
    "PLOT_SCATTER=True\n",
    "\n",
    "\n",
    "#compute allocation parameters\n",
    "if CONST_FM:\n",
    "    fm_grad,fm_int = np.mean([FIT_DATA[year]['f_m_coeffs'][0] for year in FIT_DATA]),np.mean([FIT_DATA[year]['f_m_coeffs'][1] for year in FIT_DATA])\n",
    "if LIN_EXTRAP_FM:\n",
    "    pass\n",
    "if TEST_FM:\n",
    "    fm_grad,fm_int=1.1,1.0 #1.1,1.0 are arithmatic means from [2020,2023]\n",
    "\n",
    "\n",
    "\n",
    "#individual model size parameters\n",
    "log_min_norm_m = np.log10(1e-8) #the smallest model to allocate compute to is ~1e-8 the size of total compute spending that year\n",
    "log_max_norm_m = np.log10(1e-1) #free param - assume that largest model that year is no larger than 10% of total training compute (can find this from historic data and so sensitivity analysis)\n",
    "\n",
    "#bin sampling parameters\n",
    "bin_sampling_method='random'\n",
    "k=-100 #for exponential dist sampling\n",
    "\n",
    "#misc parameters\n",
    "round_param=2\n",
    "\n",
    "\n",
    "COMPUTE_SAMPLE_DATA={int(year):None for year in pred_years}\n",
    "\n",
    "\n",
    "\n",
    "for year in pred_years.astype(int).ravel():\n",
    "\n",
    "    log_agg_training_compute=log_aggregate_compute_predictions_dict[year]\n",
    "    agg_training_compute=10**log_agg_training_compute #total compute used over the year\n",
    "\n",
    "    #model sizes (as fraction of T_tot)\n",
    "    norm_ms = np.logspace(log_min_norm_m,log_max_norm_m,2*(int(log_max_norm_m)-int(log_min_norm_m))+1)\n",
    "    log_norm_ms = np.log10(norm_ms)\n",
    "    log_frac_cum_compute = fm_grad*log_norm_ms + fm_int\n",
    "    cum_fm=10**log_frac_cum_compute\n",
    "\n",
    "    model_ctgs = [f'{norm_ms[i]:.2e}--{norm_ms[i+1]:.2e}' for i in range(len(norm_ms)-1)]\n",
    "    f_m = np.diff(cum_fm) #we don't include compute alloc to models 1e-8 smaller than total compute\n",
    "    bin_compute_allocs=f_m*agg_training_compute #array of how much compute allocated to each bin\n",
    "    DATA_alloc={model_ctgs[i]:\n",
    "                {'compute alloc':bin_compute_allocs[i]} for i in range(len(model_ctgs))}\n",
    "    \n",
    "    compute_samples_rand=[]\n",
    "\n",
    "    for idx,(ctg,alloc) in enumerate(list(zip(model_ctgs,bin_compute_allocs))):\n",
    "        #here alloc is the amount of alloc given to each individual bin\n",
    "\n",
    "        bounds = ctg.split('--')\n",
    "        norm_model_bin_lb,norm_model_bin_ub = float(bounds[0]),float(bounds[1])\n",
    "        model_bin_lb,model_bin_ub = agg_training_compute*norm_model_bin_lb, agg_training_compute*norm_model_bin_ub #normalising factor is total training compute\n",
    "        allocnorm_model_bin_lb,allocnorm_model_bin_ub=model_bin_lb/alloc, model_bin_ub/alloc\n",
    "\n",
    "        #not generating multiple samples yet for CIs\n",
    "        running_tot=0\n",
    "        allocnormed_samples=[] \n",
    "        while running_tot<1:\n",
    "            #SAMPLE\n",
    "            if bin_sampling_method=='random':\n",
    "                sample = np.random.uniform(allocnorm_model_bin_lb, allocnorm_model_bin_ub)\n",
    "            elif bin_sampling_method=='exp':\n",
    "                sample  = sample_from_exp_dist(a=allocnorm_model_bin_lb,b=allocnorm_model_bin_ub,k=k)\n",
    "\n",
    "            #SUM CHECK\n",
    "            if running_tot + sample > 1:\n",
    "                allocnormed_samples.append(1 - running_tot)\n",
    "                running_tot = 1\n",
    "            else:\n",
    "                allocnormed_samples.append(sample)\n",
    "                running_tot += sample\n",
    "\n",
    "        #print(f\"Model category {ctg} adds {len(allocnormed_samples)} models\")\n",
    "        compute_samples_rand = compute_samples_rand + (list(alloc*np.array(allocnormed_samples)))\n",
    "        \n",
    "        '''\n",
    "        print(f\"\"\"\n",
    "        Sampling for:\n",
    "        Year: {year}\n",
    "        Model category: {ctg}\n",
    "        n_models: {len(allocnormed_samples)}\n",
    "        \"\"\")\n",
    "        '''\n",
    "\n",
    "    compute_samples_rand = [x for x in compute_samples_rand if x!=0]\n",
    "\n",
    "    COMPUTE_SAMPLE_DATA[year]=compute_samples_rand\n",
    "        \n",
    "\n",
    "if PLOT_KDES:\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(12, 8))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for idx, (year, samples) in enumerate(COMPUTE_SAMPLE_DATA.items()):\n",
    "        sns.kdeplot(data=np.log10(samples), ax=axes[idx])\n",
    "        axes[idx].set_title(f'Year {year}')\n",
    "        axes[idx].set_xlabel('log compute (FLOPs)')\n",
    "        axes[idx].set_ylabel('Density')\n",
    "        axes[idx].grid(alpha=0.5)\n",
    "        axes[idx].set_xlim([15,30])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if PLOT_SCATTER:\n",
    "    #Get historical data\n",
    "    historical_data = {\n",
    "        'year': [],\n",
    "        'compute': []\n",
    "    }\n",
    "    for year in range(2020, 2024):\n",
    "        models = df[df['year'] == year]['compute'].values\n",
    "        for compute in models:\n",
    "            # Add random month offset\n",
    "            year_frac = year + np.random.random()\n",
    "            historical_data['year'].append(year_frac)\n",
    "            historical_data['compute'].append(compute)\n",
    "\n",
    "    # Get projected data\n",
    "    projected_data = {\n",
    "        'year': [],\n",
    "        'compute': []\n",
    "    }\n",
    "    for year, samples in COMPUTE_SAMPLE_DATA.items():\n",
    "        for compute in samples:\n",
    "            # Add random month offset\n",
    "            year_frac = year + np.random.random()\n",
    "            projected_data['year'].append(year_frac)\n",
    "            projected_data['compute'].append(compute)\n",
    "\n",
    "    # Create scatter plot\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.scatter(historical_data['year'], np.log10(historical_data['compute']), alpha=0.5, label='Historical')\n",
    "    plt.scatter(projected_data['year'], np.log10(projected_data['compute']), alpha=0.5, label='Projected',color='red')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Log Compute (FLOPs)')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "thresholds = [25, 26, 27, 28, 29, 30]\n",
    "threshold_counts = {year: [] for year in COMPUTE_SAMPLE_DATA.keys()}\n",
    "\n",
    "for year, samples in COMPUTE_SAMPLE_DATA.items():\n",
    "    for threshold in thresholds:\n",
    "        count = sum(x >= 10**threshold for x in samples)\n",
    "        threshold_counts[year].append(count)\n",
    "\n",
    "df_counts = pd.DataFrame(threshold_counts, \n",
    "                        index=[f'>1e{t}' for t in thresholds])\n",
    "display(df_counts)\n",
    "\n",
    "\n",
    "for year, samples in COMPUTE_SAMPLE_DATA.items():\n",
    "    print(f\"Year {year}: {len(samples)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments with allocation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot 2020-2023 m_alloc,c_alloc\n",
    "gradients = [FIT_DATA[year]['f_m_coeffs'][0] for year in fit_years]\n",
    "intercepts = [FIT_DATA[year]['f_m_coeffs'][1] for year in fit_years]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=2, figsize=(8,5))\n",
    "\n",
    "# Plot gradients\n",
    "ax1.plot(fit_years, gradients, 'o-')\n",
    "ax1.set_xlabel('Year')\n",
    "ax1.set_ylabel('Value')\n",
    "ax1.set_title('m_alloc trend')\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Plot intercepts\n",
    "ax2.plot(fit_years, intercepts, 'o-')\n",
    "ax2.set_xlabel('Year')\n",
    "ax2.set_ylabel('Value') \n",
    "ax2.set_title('c_alloc trend')\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# Set same y limits\n",
    "ymin = min(min(gradients), min(intercepts))\n",
    "ymax = max(max(gradients), max(intercepts))\n",
    "ax1.set_ylim(ymin, ymax)\n",
    "ax2.set_ylim(ymin, ymax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TS for fm_m,fm_c effect\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "CONST_FM=False\n",
    "LIN_EXTRAP_FM=False\n",
    "TEST_FM=True\n",
    "assert(CONST_FM+LIN_EXTRAP_FM+TEST_FM)==1, \"Only one of CONST_FM, LIN_EXTRAP_FM, or TEST_FM can be True\"\n",
    "\n",
    "PLOT_KDES=True\n",
    "PLOT_SCATTER=True\n",
    "\n",
    "\n",
    "#compute allocation parameters\n",
    "if CONST_FM:\n",
    "    fm_grad,fm_int = np.mean([FIT_DATA[year]['f_m_coeffs'][0] for year in FIT_DATA]),np.mean([FIT_DATA[year]['f_m_coeffs'][1] for year in FIT_DATA])\n",
    "if LIN_EXTRAP_FM:\n",
    "    pass\n",
    "if TEST_FM:\n",
    "    fm_grad,fm_int=1.1,0.5 #1.1,1.0 are arithmatic means from [2020,2023]\n",
    "    \n",
    "fm_m_choices=[1.1]\n",
    "fm_c_choices=[0.5,1.0,2.0]\n",
    "FM_combos=[(m,c) for m in fm_m_choices for c in fm_c_choices]\n",
    "\n",
    "#individual model size parameters\n",
    "log_min_norm_m = np.log10(1e-8) #the smallest model to allocate compute to is ~1e-8 the size of total compute spending that year\n",
    "log_max_norm_m = np.log10(1e-1) #free param - assume that largest model that year is no larger than 10% of total training compute (can find this from historic data and so sensitivity analysis)\n",
    "\n",
    "#bin sampling parameters\n",
    "bin_sampling_method='random'\n",
    "k=-100 #for exponential dist sampling\n",
    "\n",
    "#misc parameters\n",
    "round_param=2\n",
    "\n",
    "\n",
    "FM_DATA={(fm_m,fm_c):None for fm_m,fm_c in FM_combos}\n",
    "\n",
    "\n",
    "for fm_m,fm_c in FM_combos:\n",
    "    COMPUTE_SAMPLE_DATA={int(year):None for year in pred_years}\n",
    "    for year in pred_years.astype(int).ravel():\n",
    "\n",
    "        log_agg_training_compute=aggregate_compute_predictions_dict[year]\n",
    "        agg_training_compute=10**log_agg_training_compute #total compute used over the year\n",
    "\n",
    "        #model sizes (as fraction of T_tot)\n",
    "        norm_ms = np.logspace(log_min_norm_m,log_max_norm_m,2*(int(log_max_norm_m)-int(log_min_norm_m))+1)\n",
    "        log_norm_ms = np.log10(norm_ms)\n",
    "        log_frac_cum_compute = fm_m*log_norm_ms + fm_c\n",
    "        cum_fm=10**log_frac_cum_compute\n",
    "\n",
    "        model_ctgs = [f'{norm_ms[i]:.2e}--{norm_ms[i+1]:.2e}' for i in range(len(norm_ms)-1)]\n",
    "        f_m = np.diff(cum_fm) #we don't include compute alloc to models 1e-8 smaller than total compute\n",
    "        bin_compute_allocs=f_m*agg_training_compute #array of how much compute allocated to each bin\n",
    "        DATA_alloc={model_ctgs[i]:\n",
    "                    {'compute alloc':bin_compute_allocs[i]} for i in range(len(model_ctgs))}\n",
    "        \n",
    "        compute_samples_rand=[]\n",
    "\n",
    "        for idx,(ctg,alloc) in enumerate(list(zip(model_ctgs,bin_compute_allocs))):\n",
    "            #here alloc is the amount of alloc given to each individual bin\n",
    "\n",
    "            bounds = ctg.split('--')\n",
    "            norm_model_bin_lb,norm_model_bin_ub = float(bounds[0]),float(bounds[1])\n",
    "            model_bin_lb,model_bin_ub = agg_training_compute*norm_model_bin_lb, agg_training_compute*norm_model_bin_ub #normalising factor is total training compute\n",
    "            allocnorm_model_bin_lb,allocnorm_model_bin_ub=model_bin_lb/alloc, model_bin_ub/alloc\n",
    "\n",
    "            #not generating multiple samples yet for CIs\n",
    "            running_tot=0\n",
    "            allocnormed_samples=[] \n",
    "            while running_tot<1:\n",
    "                #SAMPLE\n",
    "                if bin_sampling_method=='random':\n",
    "                    sample = np.random.uniform(allocnorm_model_bin_lb, allocnorm_model_bin_ub)\n",
    "                elif bin_sampling_method=='exp':\n",
    "                    sample  = sample_from_exp_dist(a=allocnorm_model_bin_lb,b=allocnorm_model_bin_ub,k=k)\n",
    "\n",
    "                #SUM CHECK\n",
    "                if running_tot + sample > 1:\n",
    "                    allocnormed_samples.append(1 - running_tot)\n",
    "                    running_tot = 1\n",
    "                else:\n",
    "                    allocnormed_samples.append(sample)\n",
    "                    running_tot += sample\n",
    "\n",
    "            #print(f\"Model category {ctg} adds {len(allocnormed_samples)} models\")\n",
    "            compute_samples_rand = compute_samples_rand + (list(alloc*np.array(allocnormed_samples)))\n",
    "            \n",
    "            '''\n",
    "            print(f\"\"\"\n",
    "            Sampling for:\n",
    "            Year: {year}\n",
    "            Model category: {ctg}\n",
    "            n_models: {len(allocnormed_samples)}\n",
    "            \"\"\")\n",
    "            '''\n",
    "\n",
    "        compute_samples_rand = [x for x in compute_samples_rand if x!=0]\n",
    "\n",
    "        COMPUTE_SAMPLE_DATA[year]=compute_samples_rand\n",
    "    FM_DATA[(fm_m,fm_c)]=COMPUTE_SAMPLE_DATA\n",
    "\n",
    "\n",
    "if PLOT_SCATTER:\n",
    "    n_plots = len(FM_DATA.keys())\n",
    "    fig, axes = plt.subplots(1, n_plots, figsize=(5*n_plots, 5))\n",
    "    if n_plots == 1:\n",
    "        axes = [axes]\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, n_plots))\n",
    "    \n",
    "    # Plot historical data first\n",
    "    historical_years = df[df['year'].between(2020, 2023)]['year']\n",
    "    historical_compute = df[df['year'].between(2020, 2023)]['compute']\n",
    "    for ax in axes:\n",
    "        # Add random jitter to historical years\n",
    "        jittered_hist_years = np.random.uniform(historical_years-0.4, historical_years+0.4, len(historical_years))\n",
    "        ax.scatter(jittered_hist_years, historical_compute, alpha=0.3, c='gray', s=10, label='Historical')\n",
    "        ax.set_xlim(2020, 2029)\n",
    "\n",
    "    # Plot model data\n",
    "    for (fm_m, fm_c), color, ax in zip(FM_DATA.keys(), colors, axes):\n",
    "        years = []\n",
    "        compute_values = []\n",
    "        for year, samples in FM_DATA[(fm_m,fm_c)].items():\n",
    "            # Add random jitter to years to spread points out\n",
    "            jittered_years = np.random.uniform(year-0.4, year+0.4, len(samples))\n",
    "            years.extend(jittered_years)\n",
    "            compute_values.extend(samples)\n",
    "            \n",
    "        ax.scatter(years, compute_values, alpha=0.3, c=color, s=10, label='Model')\n",
    "        ax.set_yscale('log')\n",
    "        ax.set_xlabel('Year')\n",
    "        ax.set_ylabel('Compute (FLOP)')\n",
    "        ax.set_title(f'm_alloc={fm_m}, c_alloc={fm_c}')\n",
    "        ax.grid(alpha=0.5)\n",
    "        ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if PLOT_KDES:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, len(FM_DATA.keys())))\n",
    "    \n",
    "    for (fm_m, fm_c), color in zip(FM_DATA.keys(), colors):\n",
    "        # Get samples for 2026\n",
    "        samples_2026 = FM_DATA[(fm_m,fm_c)][2026]\n",
    "            \n",
    "        # Create KDE\n",
    "        sns.kdeplot(data=np.log10(samples_2026), \n",
    "                   label=f'fm_m={fm_m}, fm_c={fm_c}',\n",
    "                   color=color)\n",
    "    \n",
    "    ax.set_xlabel('Log Compute (FLOP)')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title('Log compute KDE for 2026')\n",
    "    ax.grid(alpha=0.5)\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sci_comp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
