{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import scipy.optimize as optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in and process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('data/save_file.xlsx')\n",
    "print(df['compute'].isna().sum())\n",
    "df.replace('',np.nan,inplace=True)\n",
    "print(df['compute'].isna().sum())\n",
    "\n",
    "##### Add columns and sort out 'poss 1e23/1e25'\n",
    "\n",
    "# Reset the ones we've set\n",
    "df.loc[~df[\"compute\"].isna(), \"poss1e23\"] = np.nan\n",
    "df.loc[~df[\"compute\"].isna(), \"poss1e25\"] = np.nan\n",
    "\n",
    "print(df['compute'].isna().sum())\n",
    "\n",
    "# Set some temporary placeholder values\n",
    "# TODO: revisit\n",
    "# df.loc[(df[\"poss1e25\"] == \"checked\"), \"compute\"] = 1.01e25  # placeholder\n",
    "# df.loc[((df[\"poss1e23\"] == \"checked\") & (df[\"poss1e25\"] != \"checked\")), \"compute\"] = 1.01e23  # placeholder\n",
    "\n",
    "# We want to handle these leading models manually via the above compute estimates.\n",
    "assert df[(df[\"poss1e25\"] == \"checked\") & (df[\"compute\"].isna())].size == 0\n",
    "\n",
    "# We sample 1e23-1e25 models with unknown compute from the existing empirical distribution.\n",
    "# TODO: revisit\n",
    "poss1e23 = ((df[\"poss1e23\"] == \"checked\") & (df[\"poss1e25\"] != \"checked\"))\n",
    "df.loc[poss1e23, \"compute\"] = df[(df[\"compute\"] >= 1e23) & (df[\"compute\"] < 1e25)][\"compute\"].sample(poss1e23.sum(), random_state=0).values\n",
    "\n",
    "\n",
    "\n",
    "##setting columns\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "df['compute'] = pd.to_numeric(df['compute'],errors='coerce')\n",
    "df[\"log_compute\"] = np.log10(df[\"compute\"])\n",
    "\n",
    "df[\"date_float\"] = df[\"date\"].dt.year + df[\"date\"].dt.month/12\n",
    "\n",
    "df = df.sort_values(\"date\")\n",
    "\n",
    "df['year'] = df['date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "systems_to_remove = ['AlphaGo Zero', 'AlphaZero']\n",
    "print(len(df))\n",
    "df = df[~df['model'].isin(systems_to_remove)]\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## regressions\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#filtering\n",
    "start_date = '2017-01-01'\n",
    "end_date = '2024-01-01'\n",
    "filtered_df = df[(df['date']>start_date) & (df['date']<end_date)]\n",
    "filtered_df = filtered_df[~filtered_df['log_compute'].isna()] #remove Nan compute rows\n",
    "\n",
    "\n",
    "#lin regress for means\n",
    "year_grouped_df = filtered_df.groupby(['year'])\n",
    "grouped_df_mean = year_grouped_df['log_compute'].mean()\n",
    "aggregate_compute = year_grouped_df['compute'].sum()\n",
    "log_aggregate_compute = np.log10(aggregate_compute)\n",
    "\n",
    "X = grouped_df_mean.index.values\n",
    "\n",
    "\n",
    "mean_log_compute_model = LinearRegression()\n",
    "mean_log_compute_model.fit(X.reshape(-1,1),grouped_df_mean)\n",
    "\n",
    "log_aggregate_compute_model = LinearRegression()\n",
    "log_aggregate_compute_model.fit(X.reshape(-1,1),log_aggregate_compute)\n",
    "\n",
    "del filtered_df #remove filtered df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exponential counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## exp counts fit\n",
    "\n",
    "from scipy.stats import t\n",
    "\n",
    "def exp_pred_counts(years,year_counts,future_years,alpha=0.10):\n",
    "    mapped_years = np.arange(0,len(year_counts)).astype('float')\n",
    "\n",
    "    def exp_fit(x,a,b):\n",
    "        return a*np.exp(b*x)\n",
    "    \n",
    "    popt,pcov = optimize.curve_fit(exp_fit,mapped_years.astype(float),year_counts.values.astype(float))\n",
    "    pred_counts = exp_fit(future_years-years[0],*popt).astype(int) \n",
    "\n",
    "    #calculatiing confidence bounds\n",
    "    #assuming log normal uncertainty, 90% CI\n",
    "    pred_counts_fit = exp_fit(mapped_years,*popt)\n",
    "    log_pred_counts_fit = np.log(pred_counts_fit)\n",
    "    log_obs_counts = np.log(year_counts.values.astype(float))\n",
    "    residuals = log_pred_counts_fit - log_obs_counts #we're calculating residuals of log counts \n",
    "    SEP = np.sqrt(np.sum(residuals**2)/(len(year_counts-2)))\n",
    "\n",
    "\n",
    "    alpha = alpha #90% conf interval\n",
    "    dof = len(year_counts) - 2 #apparenlty for linear function the dof is n-2\n",
    "    crit_t_value = stats.t.ppf(1-alpha/2,dof)\n",
    "    pred_delta = crit_t_value*SEP \n",
    "\n",
    "    years_all = np.concatenate([years,future_years])\n",
    "    preds_all = np.concatenate([pred_counts_fit,pred_counts])\n",
    "    log_pred_UB = np.log(preds_all)+pred_delta\n",
    "    log_pred_LB = np.log(preds_all)-pred_delta\n",
    "    pred_counts_UB = np.exp(log_pred_UB)\n",
    "    pred_counts_LB = np.exp(log_pred_LB)\n",
    "\n",
    "    return years_all,preds_all,pred_counts_UB,pred_counts_LB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from scipy.stats import gaussian_kde,norm,linregress\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fit = None #var place holder\n",
    "mu_0 = None\n",
    "def trunc_norm_NLL(sigma): #NLL assuming we're sampling from  truncated normal \n",
    "    ll = norm.logpdf(data_fit.to_numpy(),mu_0,sigma) - np.log(1-norm.cdf(data_fit.to_numpy().min(),mu_0,sigma))\n",
    "    return -np.sum(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SCATTER PLOT - just to visualise\n",
    "\n",
    "#filtering\n",
    "start_date = '2017-01-01'\n",
    "end_date = '2024-01-01'\n",
    "filtered_df = df[(df['date']>start_date) & (df['date']<end_date)]\n",
    "\n",
    "\n",
    "\n",
    "filtered_df['year'] = filtered_df['date'].dt.year\n",
    "filtered_df = filtered_df[~filtered_df['log_compute'].isna()] #remove Nan compute rows\n",
    "\n",
    "#lin regress for means\n",
    "grouped_df_mean = filtered_df.groupby(['year'])['log_compute'].mean().reset_index()\n",
    "\n",
    "X = grouped_df_mean['year'].values\n",
    "mean_log_compute = grouped_df_mean['log_compute'].values\n",
    "\n",
    "mean_log_compute_model = LinearRegression()\n",
    "mean_log_compute_model.fit(X.reshape(-1,1),mean_log_compute)\n",
    "\n",
    "#plot\n",
    "fig_S,ax_S=plt.subplots()\n",
    "#sns.regplot(data=filtered_df,x='date_float',y='log_compute',color='tab:blue')\n",
    "sns.stripplot(x='year',y='log_compute',data=filtered_df,jitter=True,ax=ax_S)\n",
    "sns.regplot(x=pd.Categorical(filtered_df['year']).codes,y='log_compute',data=filtered_df,scatter=False,ax=ax_S)\n",
    "ymin,ymax = ax_S.get_yticks().min(),ax_S.get_yticks().max()\n",
    "ax_S.set_yticks(np.arange(ymin,ymax,1))\n",
    "#ax_S.axhline(y=23.7)\n",
    "\n",
    "del filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### KDEs\n",
    "\n",
    "#filtering\n",
    "start_date = '2017-01-01'\n",
    "filtered_df = df[df['date']>start_date]\n",
    "\n",
    "\n",
    "PLOT=True\n",
    "FIT_2024_RT = True\n",
    "mean_2024 = mean_log_compute_model.predict(np.array(2024).reshape(-1,1))[0]\n",
    "\n",
    "\n",
    "start_year=2017\n",
    "end_year = 2024 \n",
    "\n",
    "years = np.arange(start_year,end_year+1)\n",
    "log_compute_min = 10\n",
    "log_compute_max = 28\n",
    "x = np.linspace(log_compute_min,log_compute_max,1000)\n",
    "\n",
    "\n",
    "nrows = 4\n",
    "ncols = 2\n",
    "fig,axs = plt.subplots(nrows=nrows,ncols=ncols,figsize=(8,10))\n",
    "axs_ravel = np.ravel(axs)\n",
    "MEAN,VAR= [],[]\n",
    "\n",
    "\n",
    "\n",
    "for idx,year in enumerate(years):\n",
    "    try:ax=axs_ravel[idx]\n",
    "    except: pass\n",
    "\n",
    "\n",
    "    year_filtered_df = filtered_df[filtered_df['year']==year]\n",
    "    nan_frac =(year_filtered_df['log_compute'].isna().sum())/len(year_filtered_df)\n",
    "    year_filtered_df = year_filtered_df[~year_filtered_df['log_compute'].isna()]\n",
    "    log_compute_data = year_filtered_df['log_compute']\n",
    "\n",
    "    if year==2024:\n",
    "        #find sigma from right tail\n",
    "        if FIT_2024_RT:\n",
    "            RT_filtered_data = log_compute_data[log_compute_data>mean_2024]\n",
    "            #print(len(RT_filtered_data)/len(log_compute_data)) #sanity check\n",
    "            init_sigma = np.std(RT_filtered_data)\n",
    "            data_fit = RT_filtered_data\n",
    "            mu_0 = mean_2024\n",
    "            sigma = (minimize(trunc_norm_NLL,[init_sigma])).x\n",
    "\n",
    "            mean,sigma=mu_0,sigma #variable renaming\n",
    "            MEAN.append(mean)\n",
    "            VAR.append(sigma[0]**2)\n",
    "\n",
    "    #find means in standard way\n",
    "    else: \n",
    "        mean,std = np.mean(log_compute_data),np.std(log_compute_data)\n",
    "        MEAN.append(mean); VAR.append(std**2)\n",
    "        kde = gaussian_kde(log_compute_data)\n",
    "        norm_pdf = norm.pdf(x,mean,std)\n",
    "\n",
    "\n",
    "    sns.kdeplot(log_compute_data,fill=True,ax=ax,label='KDE')\n",
    "    ax.plot(x,norm_pdf,label='norm fit')\n",
    "    ax.set_title(f'{year},nan_frac={np.round(nan_frac,1)}')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "'''\n",
    "##predicted pdfs\n",
    "CONST_VAR=True\n",
    "tmp_year = 2021\n",
    "\n",
    "if CONST_VAR:\n",
    "    idx_tmp_year = np.where(years==tmp_year)[0][0]\n",
    "    dist_var = np.mean(VAR[idx_tmp_year:])\n",
    "else: \n",
    "    dist_var = None #not yet implemened\n",
    "\n",
    "future_years = np.arange(2024,2028+1)\n",
    "pred_means = mean_log_compute_model.predict(future_years.reshape(-1,1))\n",
    "\n",
    "fig,axs=plt.subplots(nrows=3,ncols=2,figsize=(8,10))\n",
    "axs_ravel = np.ravel(axs)\n",
    "x_ = np.linspace(15,35)\n",
    "\n",
    "for idx,year in enumerate(future_years):\n",
    "    ax=axs_ravel[idx]\n",
    "    norm_pdf = norm.pdf(x_,pred_means[idx],dist_var)\n",
    "    ax.plot(norm_pdf)\n",
    "\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "'''\n",
    "\n",
    "del filtered_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Counting models above thresholds\n",
    "\n",
    "\n",
    "\n",
    "CONST_VAR = True #take variance from year y onwards\n",
    "tmp_year = 2021\n",
    "\n",
    "years = np.arange(2017,2023+1)\n",
    "future_years = np.arange(2024,2028+1)\n",
    "\n",
    "#filtering\n",
    "start_date = '2017-01-01'\n",
    "end_date = '2024-01-01'\n",
    "filtered_df = df[(df['date']>start_date) & (df['date']<end_date)]\n",
    "\n",
    "\n",
    "year_counts = filtered_df['year'].value_counts().sort_index()\n",
    "year_counts = year_counts.loc[2017:2023]\n",
    "\n",
    "years_all,pred_counts,pred_counts_UB,pred_counts_LB = exp_pred_counts(years,year_counts,future_years)\n",
    "\n",
    "if CONST_VAR:\n",
    "    idx_tmp_year = np.where(years==tmp_year)[0][0]\n",
    "    dist_var = np.mean(VAR[idx_tmp_year:])\n",
    "    dist_var = 1.1\n",
    "else: \n",
    "    dist_var = None #not yet implemened\n",
    "\n",
    "\n",
    "threshold_count = 0\n",
    "threshold = 25\n",
    "stt_bin,stop_bin,num = 23,30,1000\n",
    "x=np.linspace(start=stt_bin,stop=stop_bin,num=num)\n",
    "bins = np.arange(stt_bin,stop_bin+1)\n",
    "\n",
    "predicted_bin_counts_df = pd.DataFrame(index=bins[:-1],columns=future_years)\n",
    "\n",
    "\n",
    "for idx,year in enumerate(future_years):\n",
    "    fmt_year = np.array(year).reshape(-1,1)\n",
    "\n",
    "    mean,sigma = mean_log_compute_model.predict(fmt_year),np.sqrt(dist_var)\n",
    "    norm_pdf = norm.pdf(x,mean,sigma)\n",
    "    norm_cdf = norm.cdf(bins,loc=mean,scale=sigma) #cdf at bin edges\n",
    "    bin_pmfs = np.diff(norm_cdf)\n",
    "\n",
    "    counts_idx = np.where(years_all==year)[0][0]\n",
    "    model_count = pred_counts[counts_idx]\n",
    "\n",
    "    freq_dist = (model_count*bin_pmfs).astype(int)\n",
    "\n",
    "    predicted_bin_counts_df[year]=freq_dist\n",
    "\n",
    "\n",
    "del filtered_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training compute budget verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "aggregate_log_compute_pred = log_aggregate_compute_model.predict(future_years.reshape(-1,1))\n",
    "\n",
    "method = 'bin_median'\n",
    "bin_index = (predicted_bin_counts_df.index.values[:,None]).astype(float) #formating, float to stop overflow\n",
    "if method=='bin_median':\n",
    "    index_mult_factor = 10**(bin_index+0.5) #midpoint vals\n",
    "if method=='bin_lb':\n",
    "    index_mult_factor = 10**(bin_index+0)\n",
    "    \n",
    "distribution_aggregate_log_compute = np.log10((index_mult_factor*predicted_bin_counts_df).sum(axis=0))\n",
    "#predicted_bin_counts_df.loc['Aggregate log flop'] = new_row\n",
    "\n",
    "\n",
    "##plot\n",
    "years = predicted_bin_counts_df.columns.values\n",
    "dist_to_reg_ratio = 10**(distribution_aggregate_log_compute-aggregate_log_compute_pred)\n",
    "\n",
    "plt.bar(years,dist_to_reg_ratio)\n",
    "plt.title('Ratio of total compute (distribution v.s. raw extrapolation)')\n",
    "\n",
    "\n",
    "\n",
    "mean_log_compute_pred = mean_log_compute_model.predict(future_years.reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Empirical training compute distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "start_date = '2017-01-01'\n",
    "end_date = '2024-01-01'\n",
    "filtered_df = df[(df['date']>start_date) & (df['date']<end_date)]\n",
    "filtered_df = filtered_df[~filtered_df['compute'].isna()]\n",
    "\n",
    "years = filtered_df.year.unique()\n",
    "\n",
    "year_grouped_df = filtered_df.groupby(['year'])\n",
    "total_compute = year_grouped_df['compute'].sum()\n",
    "log_total_compute = np.log10(total_compute)\n",
    "frontier_models = year_grouped_df['compute'].max()\n",
    "\n",
    "cols = years\n",
    "index = ['f0-1','f1-2','f2-3','other']\n",
    "df_store = pd.DataFrame(columns=cols,index=index)\n",
    "\n",
    "for year in years:\n",
    "    year_tmp_df = filtered_df[filtered_df['year']==year]\n",
    "    year_total_compute = year_tmp_df['compute'].sum()\n",
    "    frontier_model = frontier_models[year]\n",
    "    \n",
    "    bins_ = (np.log10(frontier_model) - np.arange(0,len(index)))\n",
    "    bins_ = np.append(bins_,-0.1)\n",
    "    year_tmp_df['binned'] = pd.cut(df['log_compute'],bins=bins_[::-1],labels=index[::-1])\n",
    "    total_compute_bins = year_tmp_df.groupby(['binned'])['compute'].sum()\n",
    "    frac_compute = round(total_compute_bins/year_total_compute,2)\n",
    "    df_store[year]=frac_compute\n",
    "\n",
    "df_store = df_store.cumsum()\n",
    "\n",
    "new_index = ['Within 1 OOM', 'Within 2 OOM','Within 3 OOM','All']\n",
    "df_store.index=new_index\n",
    "#del filtered_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
