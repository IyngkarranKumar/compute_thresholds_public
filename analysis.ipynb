{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from scipy import stats, optimize\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd #taking long to load here\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import copy,re, pdb, logging\n",
    "from sklearn import linear_model\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().handlers.clear()\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "    handlers=[\n",
    "        #logging.FileHandler(log_filename),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils \n",
    "\n",
    "#util funcs cell\n",
    "def norm_exp_func(x,a,b,k):\n",
    "    norm_factor=(1/k)*(np.exp(k*b)-np.exp(k*a))\n",
    "    return (1/norm_factor)*np.exp(k*x)\n",
    "\n",
    "def sample_from_exp_dist(a,b,k,spacing='linear'):\n",
    "    x=np.linspace(a,b,10000) #might need to change this to logspace\n",
    "    dx=x[1]-x[0] #differnt if logspace\n",
    "    pdf=norm_exp_func(x,a,b,k=k)\n",
    "    assert(round(sum(pdf*dx),2)==1), print(sum(pdf*dx)) #sanity check on probability dist\n",
    "    prob_dist=pdf*dx\n",
    "    prob_dist=prob_dist/np.sum(prob_dist) #ensure that sums exactly to 1 for use with np.random.choice\n",
    "\n",
    "    return np.random.choice(x,p=prob_dist)\n",
    "\n",
    "def decimal_year_to_date(decimal_year):\n",
    "    if isinstance(decimal_year, pd.Series):\n",
    "        return decimal_year.apply(lambda x: decimal_year_to_date(x))\n",
    "    if isinstance(decimal_year, (list, np.ndarray)):\n",
    "        return [decimal_year_to_date(x) for x in decimal_year]\n",
    "    year = int(decimal_year)\n",
    "    remainder = decimal_year-year\n",
    "    days_in_year = 366 if pd.Timestamp(year,1,1).is_leap_year else 365\n",
    "    days = int(remainder*days_in_year)\n",
    "    return pd.Timestamp(year,1,1)+pd.Timedelta(days=days)\n",
    "\n",
    "\n",
    "def alloc_ratio_to_alloc(alloc_ratio):\n",
    "    #note - assumes alloc_rati = train/inf\n",
    "    alloc_ratio=np.array(alloc_ratio)\n",
    "    train_alloc=alloc_ratio/(1+alloc_ratio)\n",
    "    inference_alloc=1-train_alloc\n",
    "    return train_alloc, inference_alloc\n",
    "\n",
    "def round_dates(dates, freq):\n",
    "    #from Claude; unsure how this works\n",
    "    if freq == '6M':\n",
    "        return dates.map(lambda d: d.replace(day=1) + pd.offsets.MonthEnd(6 - (d.month - 1) % 6))\n",
    "    elif freq == '3M':\n",
    "        return dates.map(lambda d: d.replace(day=1) + pd.offsets.MonthEnd(3 - (d.month - 1) % 3))\n",
    "    elif freq == '1M':\n",
    "        return dates.map(lambda d: d.replace(day=1) + pd.offsets.MonthEnd(1))\n",
    "    elif freq == '1Y':\n",
    "        return dates.map(lambda d: d.replace(month=1, day=1) + pd.offsets.YearEnd())\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported frequency\")\n",
    "\n",
    "def truncated_normal(mean,std_dev,min_lms=None,max_lms=None,size=1):\n",
    "    if min_lms is None: min_lms=mean-3*std_dev\n",
    "    if max_lms is None: max_lms=mean+3*std_dev\n",
    "    samples = np.random.normal(mean, std_dev, size)\n",
    "    return np.clip(samples, min_lms, max_lms) \n",
    "\n",
    "\n",
    "def sample_array_catg_summary(samples,ranges):\n",
    "    compute_summary = {}\n",
    "    for start,end in ranges:\n",
    "        compute_summary[f\"1e{start}-1e{end}\"] = None #create all keys \n",
    "    for start, end in ranges:\n",
    "        count = sum(1 for x in samples if 10**start <= x < 10**end)\n",
    "        if count > 0:\n",
    "            compute_summary[f\"1e{start}-1e{end}\"] = count\n",
    "    return compute_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feb 2025 dataset\n",
    "\n",
    "#path \n",
    "path=\"/Users/iyngkarrankumar/Documents/GovAI WF/EUAIA_thresholds_project/data/notable_ai_models_24_02_2025.csv\"\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "df = df[~df[\"Notability criteria\"].isna()]\n",
    "\n",
    "df[\"compute\"] = df[\"Training compute (FLOP)\"]\n",
    "df[\"date\"] = pd.to_datetime(df[\"Publication date\"])\n",
    "df[\"year\"] = pd.to_datetime(df[\"date\"]).dt.year\n",
    "df[\"model\"] = df[\"Model\"]\n",
    "df[\"cost\"] = df[\"Training compute cost (2023 USD)\"]\n",
    "df[\"cost\"] = df[\"cost\"].fillna(\"$0\")  # Handle NaN values\n",
    "df[\"cost\"] = df[\"cost\"].astype(str)  # Convert to string\n",
    "df[\"cost\"] = df[\"cost\"].str.replace(\",\", \"\").str.replace(\"$\", \"\").astype(float)\n",
    "df = df[[\"model\", \"compute\", \"date\", \"cost\",\"year\"]]\n",
    "\n",
    "# Models to remove\n",
    "to_remove = [\"AlphaGo Zero\", \"AlphaZero\", \"AlphaGo Master\"] #remove compute outliers\n",
    "df = df[~df[\"model\"].isin(to_remove)]\n",
    "\n",
    "# Count total non-NaN compute values\n",
    "compute_df = df[~df['compute'].isna()]\n",
    "total_compute = len(compute_df)\n",
    "\n",
    "# Create dataset without specified years\n",
    "years_to_exclude = [2025]  # List of years to exclude\n",
    "df_filtered = df[~df[\"year\"].isin(years_to_exclude)].copy()\n",
    "\n",
    "df = df_filtered\n",
    "\n",
    "# Remove rows with NaN in compute column\n",
    "df = df.dropna(subset=['compute'])\n",
    "\n",
    "# Plot distribution of models 2020-2023\n",
    "# Plot models from 2020-2023\n",
    "df_plot = df[(df['year'] >= 2017) & (df['year'] <= 2025)]\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df_plot, x='date', y='compute', marker='x', hue='year', \n",
    "                alpha=1, palette='deep', s=100)\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.title('Model Training Compute (2017-2024)', pad=15)\n",
    "plt.xlabel('Date', labelpad=10)\n",
    "plt.ylabel('Compute (FLOP)', labelpad=10)\n",
    "plt.grid(alpha=0.5)\n",
    "plt.legend(title='Year', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##FIT YEARS PLOTS - decide which years for fit years\n",
    "min_absl_year=2017\n",
    "\n",
    "# Analyze data year by year\n",
    "# Calculate number of years for subplot layout\n",
    "n_years = df.year.max() - min_absl_year + 1\n",
    "n_cols = 3\n",
    "n_rows = (n_years + n_cols - 1) // n_cols  # Ceiling division to handle non-divisible cases\n",
    "nrows,ncols=3,2\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(8, 8))\n",
    "axes = axes.flatten()  # Flatten to make indexing easier\n",
    "fig.suptitle('Distribution of Model Compute by Year')\n",
    "\n",
    "year_data_dict = {}\n",
    "\n",
    "# Second pass to create actual subplots with consistent y-axis\n",
    "for i, year in enumerate(range(min_absl_year, df.year.max()+1)):\n",
    "    year_data = df[df['year'] == year]\n",
    "    \n",
    "    if not year_data.empty:\n",
    "        # Find largest model that year\n",
    "        max_compute_idx = year_data['compute'].idxmax()\n",
    "        max_model = year_data.loc[max_compute_idx, 'model']\n",
    "        max_compute = year_data.loc[max_compute_idx, 'compute']\n",
    "        \n",
    "        # Calculate total compute for the year\n",
    "        total_compute = year_data['compute'].sum()\n",
    "        n_datapoints = len(year_data)\n",
    "        \n",
    "        year_data_dict[year] = {\n",
    "            'n_datapoints': n_datapoints,\n",
    "            'largest_model': max_model,\n",
    "            'max_compute': max_compute,\n",
    "            'total_compute': total_compute,\n",
    "            'LMS': max_compute/total_compute\n",
    "        }\n",
    "        \n",
    "        # Create KDE plot for this year\n",
    "        sns.kdeplot(data=np.log10(year_data['compute'].values), ax=axes[i])\n",
    "        axes[i].set_title(f'Year {year}')\n",
    "        axes[i].set_ylim(0, 0.6)\n",
    "        axes[i].set_xlim(15,30)\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "        # Add text annotation for number of datapoints\n",
    "        axes[i].text(0.05, 0.95, f'n={n_datapoints}', \n",
    "                    transform=axes[i].transAxes,\n",
    "                    verticalalignment='top',\n",
    "                    fontsize=15)\n",
    "\n",
    "plt.xlabel('Log10(Compute) (FLOP)')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global params\n",
    "n_simulations = 10 #for bootstrappng, sampling parameters etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training compute extrapolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params\n",
    "fit_years = np.arange(2017,2024)\n",
    "pred_years = np.arange(fit_years.max()+1,2029)\n",
    "all_years = np.concatenate([fit_years,pred_years])\n",
    "\n",
    "\n",
    "#extrap\n",
    "AI2027_EXTRAP=True\n",
    "method_choice=\"method 2027\" #['linear extrapolation', 'method 2027']\n",
    "\n",
    "\n",
    "hist_alloc=40/60\n",
    "hist_alloc_multiplier=1+(1/hist_alloc)\n",
    "FIXED_ALLOCATION=False\n",
    "fixed_alloc=40/60\n",
    "DYNAMIC_ALLOCATION=True #inference scaling continues improving\n",
    "assert(FIXED_ALLOCATION+DYNAMIC_ALLOCATION)==1\n",
    "pred_alloc_dict = {\n",
    "        2024: 40/60,\n",
    "        2025: 40/60,\n",
    "        2026: 40/60,\n",
    "        2027: 30/70,\n",
    "        2028: 30/70,\n",
    "    }\n",
    "\n",
    "g_historical=6.3 #from fit years \n",
    "g_global_AI_compute_mean=2.25\n",
    "g_AI_workload_share_mean=1.4 #assuming AI_compute_usage/AI_compute_capacity = const - 3.0 gets the two superposed!\n",
    "g_total_AI_2027 = g_global_AI_compute_mean * g_AI_workload_share_mean #\n",
    "growth_rate_weights = [0.25,0.75] #historical, AI_2027\n",
    "g_total = growth_rate_weights[0]*g_historical + growth_rate_weights[1]*g_total_AI_2027\n",
    "g_stdev=0.5 #get more reasonable values by fixing rather than computing from historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total AI relevant compute extrapolations\n",
    "\n",
    "###DATA STRUCTURE INIT\n",
    "LOG_AGGREGATE_COMPUTE_DATA={}\n",
    "\n",
    "\n",
    "for sim in range(n_simulations):\n",
    "    LOG_AGGREGATE_COMPUTE_DATA[sim] = {}\n",
    "\n",
    "    year_grouped_df=df.groupby(df['date'][df['date']>'2010-01-01'].dt.year)\n",
    "    aggregate_compute=year_grouped_df['compute'].sum()\n",
    "    log_aggregate_compute=np.log10(aggregate_compute)\n",
    "\n",
    "    recent_years = log_aggregate_compute[log_aggregate_compute.index.isin(fit_years)]\n",
    "    recent_log_compute_dict = {int(k): v for k, v in recent_years.items()}\n",
    "\n",
    "\n",
    "    if 1: #do historical data\n",
    "        LOG_AGGREGATE_COMPUTE_DATA[sim]['historical aggregate training compute'] = {int(k): v for k, v in log_aggregate_compute.items()}\n",
    "        LOG_AGGREGATE_COMPUTE_DATA[sim]['historical aggregate total compute'] = {int(k): v+np.log10(hist_alloc_multiplier) for k, v in log_aggregate_compute.items()}\n",
    "\n",
    "    if AI2027_EXTRAP:\n",
    "        previous_year_training_usage = 10**log_aggregate_compute.get(fit_years[-1])\n",
    "        total_usage_previous_year = hist_alloc_multiplier * previous_year_training_usage\n",
    "\n",
    "        AI_compute_usage={}\n",
    "        sim_noise_term=np.random.normal(0,g_stdev) #set noise term for each sim \n",
    "        for idx,year in enumerate(pred_years):\n",
    "            AI_compute_usage[year] = total_usage_previous_year * (g_total+sim_noise_term) ** (idx + 1)\n",
    "\n",
    "        log_aggregate_compute_predictions_dict = {year: np.log10(compute) for year, compute in AI_compute_usage.items()}\n",
    "        LOG_AGGREGATE_COMPUTE_DATA[sim]['Total-method 2027'] = log_aggregate_compute_predictions_dict\n",
    "\n",
    "\n",
    "    #do allocations\n",
    "    if 1: \n",
    "        if FIXED_ALLOCATION:\n",
    "            train_alloc,inference_alloc=alloc_ratio_to_alloc(alloc_ratio=fixed_alloc)\n",
    "            LOG_AGGREGATE_COMPUTE_DATA[sim]['aggregate training compute'] = {year: val + np.log10(train_alloc) for year, val in LOG_AGGREGATE_COMPUTE_DATA[sim][f\"Total-{method_choice}\"].items()}\n",
    "            LOG_AGGREGATE_COMPUTE_DATA[sim]['aggregate inference compute'] = {year: val + np.log10(inference_alloc) for year, val in LOG_AGGREGATE_COMPUTE_DATA[sim][f\"Total-{method_choice}\"].items()}\n",
    "\n",
    "        if DYNAMIC_ALLOCATION:\n",
    "            train_alloc_dict = {}\n",
    "            inference_alloc_dict = {}\n",
    "\n",
    "            for year, val in LOG_AGGREGATE_COMPUTE_DATA[sim][f'Total-{method_choice}'].items():\n",
    "                alloc_ratio=pred_alloc_dict.get(year,1.0)\n",
    "                train_alloc, inference_alloc = alloc_ratio_to_alloc(alloc_ratio=alloc_ratio)\n",
    "                train_alloc_dict[year] = val + np.log10(train_alloc)\n",
    "                inference_alloc_dict[year] = val + np.log10(inference_alloc)\n",
    "\n",
    "            LOG_AGGREGATE_COMPUTE_DATA[sim]['aggregate training compute'] = train_alloc_dict\n",
    "            LOG_AGGREGATE_COMPUTE_DATA[sim]['aggregate inference compute'] = inference_alloc_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot extrapolations for each method\n",
    "colors = {\n",
    "    'historical aggregate training compute': 'blue',\n",
    "    'historical aggregate total compute': 'cyan',\n",
    "    'aggregate compute': 'purple',\n",
    "    'aggregate training compute': 'green',\n",
    "    'aggregate inference compute': 'red',\n",
    "}\n",
    "markers = {\n",
    "    'historical aggregate training compute': 'o',\n",
    "    'historical aggregate total compute': 'v',\n",
    "    'aggregate compute': 's',\n",
    "    'aggregate training compute': '.',\n",
    "    'aggregate inference compute': 'x',\n",
    "}\n",
    "\n",
    "for method in colors.keys():\n",
    "    all_sim_values = defaultdict(list)\n",
    "    \n",
    "    for sim in range(n_simulations):\n",
    "        predictions = LOG_AGGREGATE_COMPUTE_DATA[sim].get(method, {})\n",
    "        for year, value in predictions.items() :\n",
    "            all_sim_values[year].append(value)\n",
    "    \n",
    "    years = sorted(all_sim_values.keys())\n",
    "    medians = [np.median(all_sim_values[year]) for year in years]\n",
    "    lower_bounds = [np.percentile(all_sim_values[year], 5) for year in years]\n",
    "    upper_bounds = [np.percentile(all_sim_values[year], 95) for year in years]\n",
    "\n",
    "    plt.plot(years, medians, label=f'{method} (Median)', color=colors[method], marker=markers[method])\n",
    "    if \"historical\" not in method:\n",
    "        plt.fill_between(years, lower_bounds, upper_bounds, color=colors[method], alpha=0.2, label=f'{method} (90% CI)')\n",
    "\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Log10(Compute) [FLOP]')\n",
    "plt.title(f'Compute Usage Over Time')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(np.arange(min(log_aggregate_compute.index), 2030, 2))\n",
    "\n",
    "# Plot compute allocations\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "years = sorted(pred_alloc_dict.keys())\n",
    "alloc_ratios = [pred_alloc_dict[y] for y in years]\n",
    "\n",
    "train_allocs = []\n",
    "inference_allocs = []\n",
    "if FIXED_ALLOCATION:\n",
    "    train_allocs, inference_allocs = alloc_ratio_to_alloc(np.ones(years.__len__()) * fixed_alloc)\n",
    "if DYNAMIC_ALLOCATION:\n",
    "    train_allocs, inference_allocs = alloc_ratio_to_alloc(np.array(list(pred_alloc_dict.values())))\n",
    "\n",
    "plt.plot(years, train_allocs, 'g-', label='Training Allocation')\n",
    "plt.plot(years, inference_allocs, 'r-', label='Inference Allocation')\n",
    "plt.scatter(years, train_allocs, color='green', marker='o')\n",
    "plt.scatter(years, inference_allocs, color='red', marker='o')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Allocation Fraction')\n",
    "plt.title('Compute Allocations Over Time')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fits for 2017-2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get compute_alloc fits\n",
    "fit_years=fit_years\n",
    "pred_years = pred_years\n",
    "\n",
    "FIT_DATA={year:None for year in fit_years}\n",
    "constraint_point=(1,1)\n",
    "filter_thresholds=1e-20 #ignore models smaller than this\n",
    "\n",
    "\n",
    "logging.info('Fitting f_M coefficients')\n",
    "\n",
    "for idx,year in enumerate(fit_years):\n",
    "    total_compute=aggregate_compute[aggregate_compute.index==year].values\n",
    "    datapoints_year=df[df['date'].dt.year==year]['compute']\n",
    "    mean_log_compute=np.log10(datapoints_year).mean()\n",
    "    largest_model=datapoints_year.max()\n",
    "    smallest_model=datapoints_year.min()\n",
    "    norm_factor_total=total_compute[0]\n",
    "\n",
    "    #ms\n",
    "    sorted_computes=np.sort(datapoints_year)\n",
    "    norm_sorted_computes=sorted_computes/largest_model\n",
    "    \n",
    "    #cum_alloc\n",
    "    cumsum=np.cumsum(sorted_computes)\n",
    "    norm_cum_alloc=cumsum/norm_factor_total\n",
    "\n",
    "    #catg_alloc (derived from cum_alloc)\n",
    "    norm_catg_alloc = np.diff(norm_cum_alloc)\n",
    "    residual_catg_alloc = 1-np.sum(norm_catg_alloc)\n",
    "    norm_catg_alloc = np.concatenate((np.array([residual_catg_alloc]),norm_catg_alloc))\n",
    "\n",
    "    #store data \n",
    "    FIT_DATA[year]={\n",
    "    'compute':sorted_computes,\n",
    "    'cumulative_sum':cumsum,\n",
    "    'norm_factor_total':norm_factor_total,\n",
    "    'largest_model':largest_model,\n",
    "    'norm_smallest_model':smallest_model/largest_model,\n",
    "    'norm_cum_alloc fits':None,\n",
    "    'norm_catg_alloc fits':None,\n",
    "            }\n",
    "    \n",
    "    #fit data\n",
    "    X = np.log10(norm_sorted_computes).reshape(-1, 1)\n",
    "    y = np.log10(norm_cum_alloc)\n",
    "    X_trans,y_trans=X-constraint_point[0],y-constraint_point[1]\n",
    "    reg_cum_alloc = linear_model.LinearRegression(fit_intercept=False).fit(X_trans, y_trans) #forcing X-a,y-b to go through (0,0) means X,y goes through (a,b)\n",
    "    FIT_DATA[year]['cum_alloc_fits'] = [reg_cum_alloc.coef_[0], reg_cum_alloc.intercept_]\n",
    "\n",
    "\n",
    "    X,y = np.log10(norm_sorted_computes).reshape(-1,1),np.log10(norm_catg_alloc).reshape(-1,1)\n",
    "    reg_catg_alloc = linear_model.LinearRegression(fit_intercept=True).fit(X,y) #we claim that there is a linear relationship between log(norm_computes) and log(catg_alloc)\n",
    "    FIT_DATA[year]['catg_alloc_fits'] = [reg_catg_alloc.coef_[0][0], reg_catg_alloc.intercept_[0]]\n",
    "    FIT_DATA[year]['norm_catg_alloc']=norm_catg_alloc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#allocation plots\n",
    "\n",
    "PLOT_NORMALIZED=True\n",
    "\n",
    "years_to_plot = fit_years\n",
    "years_to_plot = [2020,2021,2022,2023]\n",
    "#years_to_plot = [2017,2018,2019,20005]\n",
    "n_years = len(fit_years)\n",
    "\n",
    "nrows,ncols,figsize=2,2,(8,8)\n",
    "nrows,ncols,figsize=2,2,(14,14)\n",
    "fig, axes = plt.subplots(nrows,ncols,figsize=figsize)\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot parameters\n",
    "LABEL_SIZE = 20\n",
    "TICK_SIZE = 20 \n",
    "LEGEND_SIZE = 14\n",
    "TEXT_SIZE = 20\n",
    "\n",
    "for i, year in enumerate(years_to_plot):\n",
    "    if year in FIT_DATA:\n",
    "        data = FIT_DATA[year]\n",
    "        sorted_computes = data['compute']\n",
    "        norm_sorted_computes = sorted_computes / data['largest_model']\n",
    "        norm_cum_alloc = data['cumulative_sum'] / data['norm_factor_total']\n",
    "        m, b = data['cum_alloc_fits']\n",
    "        \n",
    "        # Choose which x values to plot\n",
    "        x_values = sorted_computes if not PLOT_NORMALIZED else norm_sorted_computes\n",
    "        \n",
    "        axes[i].scatter(x_values, norm_cum_alloc, marker='x', c='b', s=50, label='Data points')\n",
    "        axes[i].plot(x_values, norm_cum_alloc, c='b')\n",
    "        \n",
    "        # Add linear fit\n",
    "        if PLOT_NORMALIZED:\n",
    "            x_range = np.logspace(np.log10(np.min(norm_sorted_computes)), np.log10(np.max(norm_sorted_computes)), 100)\n",
    "            y_fit = 10**(m * np.log10(x_range) + b)\n",
    "            axes[i].plot(x_range, y_fit, 'r-', linewidth=2, label=f'Linear fit (m={m:.2f}, b={b:.2f})')\n",
    "        axes[i].set_xlabel('Individual model size' if not PLOT_NORMALIZED else 'Normalized model size', fontsize=LABEL_SIZE)\n",
    "\n",
    "\n",
    "        axes[i].set_title(f'Year {year}',fontsize=LABEL_SIZE)\n",
    "        axes[i].legend(fontsize=LEGEND_SIZE)\n",
    "        axes[i].grid(True)\n",
    "        axes[i].set_xscale('log')\n",
    "        axes[i].set_yscale('log')\n",
    "        axes[i].set_yticks([10**n for n in range(int(np.log10(min(norm_cum_alloc))), int(np.log10(max(norm_cum_alloc))) + 1)])\n",
    "\n",
    "        # Set major ticks at powers of 10\n",
    "        if PLOT_NORMALIZED:\n",
    "            axes[i].set_xticks([10**n for n in range(int(np.log10(min(x_values))), int(np.log10(max(x_values))) + 1)])\n",
    "        else:\n",
    "            min_exp = int(np.log10(min(x_values)))\n",
    "            max_exp = int(np.log10(max(x_values)))\n",
    "            #axes[i].set_xticks(np.array([10**n for n in range(min_exp, max_exp + 1)]))\n",
    "\n",
    "        # Add red dotted lines at x=1 and y=1 with annotations\n",
    "        axes[i].axhline(y=1, color='r', linestyle='--', alpha=0.7)\n",
    "        if PLOT_NORMALIZED:\n",
    "            axes[i].axvline(x=1, color='r', linestyle='--', alpha=0.7)\n",
    "        else:\n",
    "            axes[i].axvline(x=max(x_values), color='r', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Add annotations for the lines\n",
    "        # Add text annotations in the corner of the plot\n",
    "        axes[i].text(0.05, 0.95, f'Total compute: {data[\"norm_factor_total\"]:.2e}', \n",
    "                    transform=axes[i].transAxes, color='r', va='top',fontsize=TEXT_SIZE)\n",
    "        \n",
    "        axes[i].text(0.90, 0.25, f'{data[\"largest_model\"]:.2e}', \n",
    "                    transform=axes[i].transAxes, color='r', va='top',rotation=90,fontsize=TEXT_SIZE)\n",
    "\n",
    "        axes[i].tick_params(axis='both', which='major', labelsize=TICK_SIZE)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating compute samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(None) #no seeding\n",
    "import logging\n",
    "\n",
    "\n",
    "##allocation fits\n",
    "ALLOC_FIT_TYPE='cumulative'\n",
    "DISTRIBUTION_CUM_ALLOC_PARAMS=True\n",
    "grad_cum_alloc_min, grad_cum_alloc_max = 0.9, 1.1 #for setting up uncertainty modelling\n",
    "\n",
    "#IMPORTANT PARAMETER - largest model share\n",
    "LMS_SAMPLING=\"log_normal\"\n",
    "assert LMS_SAMPLING in ['log_normal', 'uniform']\n",
    "min_lms,max_lms=0.05,0.50\n",
    "SET_2024_LMS=False\n",
    "\n",
    "#min m sampling\n",
    "min_norm_m_min,min_norm_m_max = 10**(-8.0), 10**-(6.0) #wacky variable names\n",
    "\n",
    "#n_catg setting (higher the better, up to a point where delta M gets too small)\n",
    "n_catgs = 20\n",
    "\n",
    "PLOT_KDES=False\n",
    "PLOT_SCATTER=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##INIT DATA STRUCTURE\n",
    "COMPUTE_SAMPLE_DATA = {sim: {int(year): {} for year in all_years} for sim in range(n_simulations)} #init data structure \n",
    "\n",
    "for sim in range(n_simulations):\n",
    "\n",
    "    for year in all_years:\n",
    "        #get total compute\n",
    "        if year in fit_years:\n",
    "            log_agg_training_compute = LOG_AGGREGATE_COMPUTE_DATA[sim][\"historical aggregate training compute\"][year]\n",
    "        if year in pred_years:\n",
    "            log_agg_training_compute = LOG_AGGREGATE_COMPUTE_DATA[sim][\"aggregate training compute\"][year]\n",
    "        agg_training_compute = 10**log_agg_training_compute \n",
    "\n",
    "        #set largest model that year \n",
    "\n",
    "\n",
    "        #generate compute bin allocations (catg_alloc)\n",
    "        VALID_ALLOCATION=False\n",
    "        while not VALID_ALLOCATION:\n",
    "\n",
    "            #LMS sampling\n",
    "            if year in fit_years: #uniform sampling for historical  data \n",
    "                norm_largest_model = np.random.uniform(min_lms, max_lms)\n",
    "            else: \n",
    "                if year==2024 and SET_2024_LMS:\n",
    "                    gpt_4o_size = 3.80*10**25\n",
    "                    norm_largest_model = gpt_4o_size/agg_training_compute\n",
    "                else: \n",
    "                    if LMS_SAMPLING=='uniform':\n",
    "                        norm_largest_model = np.random.uniform(min_lms, max_lms)\n",
    "                    elif LMS_SAMPLING=='log_normal':\n",
    "                        log_mean = np.mean([np.log(min_lms), np.log(max_lms)])\n",
    "                        log_stdev = np.abs((np.log(max_lms) - np.log(min_lms))/4)\n",
    "                        norm_largest_model = np.random.lognormal(mean=log_mean, sigma=log_stdev) #initial sample\n",
    "                        while norm_largest_model < min_lms or norm_largest_model > max_lms: #resample until within bounds\n",
    "                            norm_largest_model = np.random.lognormal(mean=log_mean, sigma=log_stdev)\n",
    "\n",
    "\n",
    "            largest_model = norm_largest_model * agg_training_compute\n",
    "            #assert largest_model <= 0.5*agg_training_compute, print(f\"Year: {year}, Largest Model: {largest_model}, Total Training Compute: {agg_training_compute}\")\n",
    "\n",
    "            #sample smallest model that year\n",
    "            min_norm_m = 10**(np.random.uniform(np.log10(min_norm_m_min),np.log10(min_norm_m_max)))\n",
    "            smallest_model = min_norm_m*agg_training_compute #just for logging \n",
    "            # model sizes (as fraction of largest_model)\n",
    "            norm_ms = np.logspace(np.log10(min_norm_m), np.log10(1.0), num=n_catgs)\n",
    "            log_norm_ms = np.log10(norm_ms)\n",
    "\n",
    "\n",
    "            if DISTRIBUTION_CUM_ALLOC_PARAMS:\n",
    "                grad_cum_alloc, int_cum_alloc = np.random.uniform(grad_cum_alloc_min,grad_cum_alloc_max), 0\n",
    "            else:\n",
    "                raise ValueError(\"Invalid choice of cumulative alloc params\")\n",
    "\n",
    "            log_cum_alloc = grad_cum_alloc*log_norm_ms + int_cum_alloc\n",
    "            cum_alloc = 10**log_cum_alloc\n",
    "            catg_alloc = np.diff(cum_alloc)\n",
    "            sum_condition = abs(np.sum(catg_alloc) - 1) < 1e-5\n",
    "            #assert sum_condition, f\"Sum of category allocations {np.sum(catg_alloc)} not equal to 1\" #stop code if not equal to 1\n",
    "\n",
    "            residual_catg_alloc = 1-np.sum(catg_alloc)\n",
    "            catg_alloc = np.concatenate(([residual_catg_alloc],catg_alloc))\n",
    "\n",
    "            absl_ms = norm_ms*largest_model\n",
    "            absl_model_catgs = [(absl_ms[i], absl_ms[i+1]) for i in range(len(absl_ms) - 1)]\n",
    "            absl_model_catgs = [(min_norm_m*largest_model,min_norm_m*largest_model)] + absl_model_catgs\n",
    "            absl_allocs = catg_alloc * agg_training_compute\n",
    "            alloc_ub_check_var = [ctg[-1] for ctg in absl_model_catgs] < absl_allocs\n",
    "            alloc_ub_condition = np.all(alloc_ub_check_var) #ensure all allocs are above the ctg_ub\n",
    "            #assert(alloc_ub_condition), print(f'{(~alloc_ub_check_var).sum()/len(alloc_ub_check_var)*100} of alloc-ctg pairs exceed ctg_ub')\n",
    "\n",
    "            if alloc_ub_condition: \n",
    "                VALID_ALLOCATION=True\n",
    "            else:\n",
    "                VALID_ALLOCATION=False\n",
    "                #print(f'{(~alloc_ub_check_var).sum()/len(alloc_ub_check_var)*100} of alloc-ctg pairs exceed ctg_ub')\n",
    "                #print(f\"Invalid allocation for year {year}, gradient: {grad_cum_alloc}\")\n",
    "\n",
    "        model_ctgs = [(norm_ms[i], norm_ms[i+1]) for i in range(len(norm_ms) - 1)]\n",
    "        model_ctgs = [(min_norm_m,min_norm_m)] + model_ctgs #for first ctg bin - we sample just the smallest model\n",
    "        ctgs_lbs, ctgs_ubs =[ctg[0] for ctg in model_ctgs], [ctg[-1] for ctg in model_ctgs] #useful vars\n",
    "\n",
    "        bin_compute_allocs = catg_alloc * agg_training_compute  # array of how much compute allocated to each bin\n",
    "\n",
    "        compute_samples_rand = []\n",
    "\n",
    "        #draw samples\n",
    "        for idx, (ctg, alloc) in enumerate(list(zip(model_ctgs, bin_compute_allocs))):\n",
    "            if idx==0: continue\n",
    "            \n",
    "            # set initial bounds\n",
    "            bounds = ctg\n",
    "            norm_model_bin_lb, norm_model_bin_ub = float(bounds[0]), float(bounds[1])\n",
    "            model_bin_lb, model_bin_ub = largest_model * norm_model_bin_lb, largest_model * norm_model_bin_ub  # normalising factor is total training compute\n",
    "            assert alloc > model_bin_ub\n",
    "            if alloc==0:  # skip bins which have no compute allocated to them - occurs when allocation gradient large \n",
    "                continue \n",
    "\n",
    "            #perform sampling \n",
    "            allocnorm_model_bin_lb, allocnorm_model_bin_ub = model_bin_lb / alloc, model_bin_ub / alloc  # this is purely just for sampling; no physical meaning\n",
    "            running_tot = 0\n",
    "            allocnormed_samples = []\n",
    "            while running_tot < 1:\n",
    "                # SAMPLE\n",
    "                sample = np.random.uniform(allocnorm_model_bin_lb, allocnorm_model_bin_ub)\n",
    "                sample = float(sample) if isinstance(sample, np.ndarray) else sample\n",
    "                assert sample <= 1 #sample should be smaller than alloc OR equal to it\n",
    "\n",
    "\n",
    "                # SUM CHECK\n",
    "                if running_tot + sample > 1:\n",
    "                    allocnormed_samples.append(1 - running_tot)\n",
    "                    running_tot = 1\n",
    "                else:\n",
    "                    allocnormed_samples.append(sample)\n",
    "                    running_tot += sample\n",
    "\n",
    "            bin_samples = alloc*np.array(allocnormed_samples) # un-normalise\n",
    "            compute_samples_rand = compute_samples_rand + (list(bin_samples)) #add to sample list\n",
    "\n",
    "            #print(f\"Number of samples drawn for model category: {len(allocnormed_samples)}\")\n",
    "\n",
    "    \n",
    "        compute_samples_rand = [x for x in compute_samples_rand if x != 0]\n",
    "        # Create summary table of compute samples\n",
    "\n",
    "        #if year==2024:\n",
    "            #ranges = [(23,24), (24,25), (25,26),(26,27)]\n",
    "            #print(sample_array_catg_summary(compute_samples_rand,ranges))\n",
    "\n",
    "        \n",
    "\n",
    "        COMPUTE_SAMPLE_DATA[sim][year]['samples'] = compute_samples_rand\n",
    "        COMPUTE_SAMPLE_DATA[sim][year]['date'] = [decimal_year_to_date(year + np.random.random()) for _ in compute_samples_rand]  # convert to standard pd datetime format\n",
    "        COMPUTE_SAMPLE_DATA[sim][year]['largest model'] = largest_model\n",
    "\n",
    "\n",
    "logging.debug(\"\\nNumber of samples per year:\")\n",
    "for year in pred_years.ravel():\n",
    "    logging.debug(f\"{year}: {len(COMPUTE_SAMPLE_DATA[0][year]['samples'])} samples\") #take first sim\n",
    "\n",
    "        \n",
    "\n",
    "if PLOT_KDES:\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(12, 8))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for idx, year in enumerate(pred_years):\n",
    "        all_samples = [np.log10(COMPUTE_SAMPLE_DATA[sim][year]['samples']) for sim in range(n_simulations)]\n",
    "        \n",
    "        # Define a common set of x-points for KDE evaluation\n",
    "        x_points = np.linspace(15, 30, 1000)\n",
    "        \n",
    "        # Evaluate KDE for each simulation\n",
    "        from scipy.stats import gaussian_kde\n",
    "        kde_values = [gaussian_kde(samples)(x_points) for samples in all_samples]\n",
    "        \n",
    "        # Calculate median and 90th percentile KDE values at each x-point\n",
    "        median_kde = np.median(kde_values, axis=0)\n",
    "        lower_bound_kde = np.percentile(kde_values, 5, axis=0)\n",
    "        upper_bound_kde = np.percentile(kde_values, 95, axis=0)\n",
    "        \n",
    "        # Plot the median KDE\n",
    "        axes[idx].plot(x_points, median_kde, label='Median KDE')\n",
    "        \n",
    "        # Fill between the 5th and 95th percentile KDE values\n",
    "        axes[idx].fill_between(x_points, lower_bound_kde, upper_bound_kde, alpha=0.3, label='90% CI')\n",
    "        \n",
    "        axes[idx].set_title(f'Year {year}')\n",
    "        axes[idx].set_xlabel('log compute (FLOPs)')\n",
    "        axes[idx].set_ylabel('Density')\n",
    "        axes[idx].grid(alpha=0.5)\n",
    "        axes[idx].set_xlim([15, 30])\n",
    "        axes[idx].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrodict_thresholds=[23,24,25]\n",
    "threshold_widths = [0.5,1.0,1.5]  # List of threshold widths to analyze\n",
    "period_freq = '3M'  # Can be changed to any frequency like '1Y', '3M', '4M', '6M'\n",
    "thresholds = [25, 26, 27, 28, 29]\n",
    "CI_percentiles=[5,50,95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backtesting the absolute thresholds\n",
    "\n",
    "\n",
    "retrodict_years=fit_years\n",
    "\n",
    "#observed\n",
    "# Create DataFrame from observed counts``\n",
    "df_observed = pd.DataFrame.from_dict({threshold: {year: sum(df[df['year'] == year]['compute'] > 10**threshold)\n",
    "                                                for year in retrodict_years}\n",
    "                                    for threshold in retrodict_thresholds}, \n",
    "                                    orient='index')\n",
    "df_observed.index = [f'{10**threshold:.2e}' for threshold in retrodict_thresholds]\n",
    "df_observed.index.name = 'Threshold'\n",
    "\n",
    "# Create retrodict counts dictionary\n",
    "retrodict_counts = {year: {threshold: [] for threshold in retrodict_thresholds} for year in retrodict_years}\n",
    "\n",
    "for sim, sim_data in COMPUTE_SAMPLE_DATA.items():\n",
    "    for year, year_data in sim_data.items():\n",
    "        if year in retrodict_years:\n",
    "            for threshold in retrodict_thresholds:\n",
    "                count = (sum(x >= 10**threshold for x in year_data['samples'])).astype(int)\n",
    "                retrodict_counts[year][threshold].append(count)\n",
    "\n",
    "# Calculate percentiles for each year and threshold\n",
    "\n",
    "retrodict_percentile_counts = {year: {percentile: [] for percentile in CI_percentiles} for year in retrodict_years}\n",
    "for year in retrodict_years:\n",
    "    for threshold in retrodict_thresholds:\n",
    "        for percentile in CI_percentiles:\n",
    "            percentile_count = (np.percentile(retrodict_counts[year][threshold], percentile)).astype(int)\n",
    "            retrodict_percentile_counts[year][percentile].append(percentile_count)\n",
    "\n",
    "dfs_retrodict = {}\n",
    "for percentile in CI_percentiles:\n",
    "    dfs_retrodict[percentile] = pd.DataFrame(\n",
    "        {year: retrodict_percentile_counts[year][percentile] for year in retrodict_years},\n",
    "        index=[f'{10**t:.2e}' for t in retrodict_thresholds]\n",
    "    )\n",
    "    dfs_retrodict[percentile].index.name = 'Threshold'\n",
    "\n",
    "# Take cumulative sum across years for both dataframes\n",
    "df_observed_cumulative = df_observed.cumsum(axis=1)\n",
    "dfs_retrodict_cumulative = {percentile: df.cumsum(axis=1) for percentile, df in dfs_retrodict.items()}\n",
    "\n",
    "\n",
    "# Create dataframe with observed and retrodicted values for each percentile\n",
    "combined_df = pd.DataFrame(index=df_observed_cumulative.index)\n",
    "\n",
    "for year in df_observed_cumulative.columns:\n",
    "    combined_df[year] = [f\"{obs} ({','.join(str(x) for x in ret)})\" for obs, ret in zip(\n",
    "        df_observed_cumulative[year],\n",
    "        zip(*[dfs_retrodict_cumulative[percentile][year] for percentile in CI_percentiles])\n",
    "    )]\n",
    "\n",
    "display(combined_df)\n",
    "# Calculate the difference between observed and retrodicted values for each percentile\n",
    "difference_df = pd.DataFrame(index=df_observed_cumulative.index)\n",
    "\n",
    "for year in df_observed_cumulative.columns:\n",
    "    differences = []\n",
    "    for obs, *rets in zip(df_observed_cumulative[year], \n",
    "                         *[dfs_retrodict_cumulative[percentile][year] for percentile in CI_percentiles]):\n",
    "        differences.append(f\"{obs-rets[0]}, {obs-rets[1]}, {obs-rets[2]}\")\n",
    "    difference_df[year] = differences\n",
    "\n",
    "#display(difference_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## backtesting the frontier counts\n",
    "\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "from pprint import pprint\n",
    "\n",
    "# Group data into specified periods\n",
    "df['period'] = round_dates(df['date'], period_freq)\n",
    "df['log_compute'] = np.log10(df['compute'])\n",
    "\n",
    "# Calculate observed frontier counts\n",
    "frontier_counts = {}\n",
    "for year in fit_years:\n",
    "    year_filtered_df = df[df['date'].dt.year == year]\n",
    "    frontier_counts[year] = {}\n",
    "    for width in threshold_widths:\n",
    "        width_year_counts = 0\n",
    "        for idx, period in enumerate(sorted(year_filtered_df['period'].unique())):\n",
    "            largest_model = df[df['period'] < period]['compute'].max()  # get largest model before this period\n",
    "            period_data = df[df.period == period]\n",
    "            within_threshold_condition = ((np.log10(largest_model) - np.log10(period_data['compute'])) <= width) & ((np.log10(largest_model) - np.log10(period_data['compute'])) > 0)\n",
    "            above_frontier_condition = period_data['compute'] > largest_model\n",
    "            count = within_threshold_condition.sum() + above_frontier_condition.sum()\n",
    "            width_year_counts += count\n",
    "        frontier_counts[year][width] = width_year_counts\n",
    "\n",
    "sample_frontier_counts = {year: {width: [] for width in threshold_widths} for year in fit_years}\n",
    "\n",
    "# Process each simulation\n",
    "for sim, sim_data in COMPUTE_SAMPLE_DATA.items():\n",
    "    \n",
    "    # Pre-compute dates and periods for all years to avoid repeated conversions\n",
    "    for year, year_data in sim_data.items():\n",
    "        year_data['period'] = round_dates(pd.to_datetime(year_data['date']), period_freq)\n",
    "        year_data['log_compute'] = np.log10(year_data['samples'])\n",
    "    \n",
    "    # Pre-compute largest models for each period across all years\n",
    "    all_periods = sorted(set(period for data in sim_data.values() for period in data['period'].unique()))\n",
    "    largest_models = {}\n",
    "    all_samples = np.concatenate([np.array(data['samples']) for data in sim_data.values()])\n",
    "    all_dates = np.concatenate([np.array(data['date']) for data in sim_data.values()])\n",
    "    for period in all_periods:\n",
    "        largest_models[period] = np.max(all_samples[all_dates < period])\n",
    "    \n",
    "    # Process each year\n",
    "    for year in fit_years:\n",
    "        year_data = sim_data[year]\n",
    "        year_samples = np.array(year_data['samples'])\n",
    "        year_periods = year_data['period'].unique()\n",
    "        \n",
    "        # Process each threshold width\n",
    "        for width in threshold_widths:\n",
    "            width_year_counts = 0\n",
    "            \n",
    "            # Vectorized operations for each period\n",
    "            for period in sorted(year_periods):\n",
    "                period_mask = year_data['period'] == period\n",
    "                period_samples = year_samples[period_mask]\n",
    "                largest_model = largest_models[period]\n",
    "                \n",
    "                # Combine conditions in single vectorized operation\n",
    "                log_ratio = np.log10(largest_model) - np.log10(period_samples)\n",
    "                counts = np.sum((log_ratio <= width) & (log_ratio > 0)) + np.sum(period_samples > largest_model)\n",
    "                width_year_counts += counts\n",
    "                \n",
    "            sample_frontier_counts[year][width].append(width_year_counts)\n",
    "\n",
    "            \n",
    "\n",
    "# Calculate percentile counts\n",
    "percentile_frontier_counts = {year: {width: {percentile: [] for percentile in CI_percentiles} for width in threshold_widths} for year in fit_years}\n",
    "for year in fit_years:\n",
    "    for width in threshold_widths:\n",
    "        for percentile in CI_percentiles:\n",
    "            percentile_count = (np.percentile(sample_frontier_counts[year][width], percentile)).astype(int)\n",
    "            percentile_frontier_counts[year][width][percentile] = percentile_count\n",
    "\n",
    "# Create combined dataframe\n",
    "combined_df = pd.DataFrame(index=threshold_widths)\n",
    "combined_df.index.name = 'width'\n",
    "\n",
    "for year in fit_years:\n",
    "    combined_df[year] = [f\"{frontier_counts[year][width]} ({','.join(str(percentile_frontier_counts[year][width][p]) for p in CI_percentiles)})\" for width in threshold_widths]\n",
    "\n",
    "# Calculate differences\n",
    "difference_df = pd.DataFrame(index=threshold_widths)\n",
    "difference_df.index.name = 'width'\n",
    "\n",
    "for year in fit_years:\n",
    "    differences = []\n",
    "    for width in threshold_widths:\n",
    "        obs = frontier_counts[year][width]\n",
    "        rets = [percentile_frontier_counts[year][width][p] for p in CI_percentiles]\n",
    "        differences.append(f\"{obs-rets[0]}, {obs-rets[1]}, {obs-rets[2]}\")\n",
    "    difference_df[year] = differences\n",
    "\n",
    "display(combined_df)\n",
    "#display(difference_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## regular counts\n",
    "\n",
    "threshold_counts_all_simulations = {year: {threshold: [] for threshold in thresholds} for year in pred_years.astype(int).ravel()}\n",
    "\n",
    "# Iterate over each simulation\n",
    "for sim in range(len(COMPUTE_SAMPLE_DATA)):\n",
    "    for year, samples in COMPUTE_SAMPLE_DATA[sim].items():\n",
    "        if year in pred_years:\n",
    "            for threshold in thresholds:\n",
    "                count = sum(x >= 10**threshold for x in samples['samples'])\n",
    "                threshold_counts_all_simulations[year][threshold].append(count)\n",
    "\n",
    "# Calculate counts for each percentile in CI_percentiles\n",
    "threshold_counts_summary = {year: [] for year in pred_years.astype(int).ravel()}\n",
    "for year in pred_years.astype(int).ravel():\n",
    "    for threshold in thresholds:\n",
    "        counts = threshold_counts_all_simulations[year][threshold]\n",
    "        percentile_counts = [np.percentile(counts, p) for p in CI_percentiles]\n",
    "        threshold_counts_summary[year].append(f\"{percentile_counts[1]:.0f} ({percentile_counts[0]:.0f}-{percentile_counts[2]:.0f})\")\n",
    "\n",
    "# Create DataFrames for each percentile\n",
    "percentile_dfs = {}\n",
    "for percentile in CI_percentiles:\n",
    "    percentile_dfs[percentile] = pd.DataFrame(\n",
    "        {year: [int(round(np.percentile(threshold_counts_all_simulations[year][threshold], percentile))) \n",
    "                for threshold in thresholds] \n",
    "         for year in pred_years.astype(int).ravel()},\n",
    "        index=[f'>1e{t}' for t in thresholds]\n",
    "    )\n",
    "\n",
    "# Make cumulative across years\n",
    "percentile_dfs_cumulative = {\n",
    "    percentile: df.cumsum(axis=1) \n",
    "    for percentile, df in percentile_dfs.items()\n",
    "}\n",
    "\n",
    "# Combine into a single DataFrame\n",
    "df_combined_cumulative = pd.DataFrame()\n",
    "for year in percentile_dfs_cumulative[50].columns:\n",
    "    for idx in percentile_dfs_cumulative[50].index:\n",
    "        values = [str(percentile_dfs_cumulative[p].loc[idx, year]) for p in CI_percentiles]\n",
    "        df_combined_cumulative.loc[idx, year] = \", \".join(values)\n",
    "display(df_combined_cumulative)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frontier-connected threshold counts for samples\n",
    "\n",
    "\n",
    "\n",
    "# Generate period data for years 2024-2029 (2029 not inclusive)\n",
    "period_data = pd.date_range(start='2024-01-01', end='2029-01-01', freq=period_freq).strftime('%Y-%m-%d %H:%M:%S').tolist()\n",
    "\n",
    "frontier_counts_all_simulations = {year: {width: [] for width in threshold_widths} for year in pred_years}\n",
    "\n",
    "for sim in range(len(COMPUTE_SAMPLE_DATA)):\n",
    "    for year in pred_years:\n",
    "        year_data = COMPUTE_SAMPLE_DATA[sim][year]\n",
    "        year_data['period'] = round_dates(pd.to_datetime(year_data['date']), period_freq)\n",
    "        year_data['log_compute'] = np.log10(year_data['samples'])\n",
    "        \n",
    "        for width in threshold_widths:\n",
    "            width_year_counts = 0\n",
    "            for period in sorted(year_data['period'].unique()):\n",
    "                largest_model = max(np.concatenate([np.array(data['samples'])[np.array(data['date']) < period] for data in COMPUTE_SAMPLE_DATA[sim].values()])) #get largest model until this period\n",
    "                period_sample_data = np.array(year_data['samples'])[year_data['period'] == period] #get models released in this period\n",
    "                within_threshold_condition = (np.log10(largest_model) - np.log10(period_sample_data) <= width) & (np.log10(largest_model) - np.log10(period_sample_data) > 0) #0 condition makes sure we don't catch models larger than frontier\n",
    "                above_frontier_condition  = period_sample_data > largest_model\n",
    "                count = within_threshold_condition.sum() + above_frontier_condition.sum() #how many models released this period within thresholds of largest model seen so far.\n",
    "                width_year_counts += count\n",
    "            frontier_counts_all_simulations[year][width].append(width_year_counts)\n",
    "\n",
    "\n",
    "\n",
    "# Create DataFrames for each percentile\n",
    "frontier_percentile_dfs = {}\n",
    "for percentile in CI_percentiles:\n",
    "    frontier_percentile_dfs[percentile] = pd.DataFrame(\n",
    "        {year: [int(round(np.percentile(frontier_counts_all_simulations[year][width], percentile)))\n",
    "                for width in threshold_widths]\n",
    "         for year in pred_years},\n",
    "        index=[f'Within {width} OOM' for width in threshold_widths]\n",
    "    )\n",
    "\n",
    "# Combine into a single DataFrame\n",
    "df_frontier_combined = pd.DataFrame()\n",
    "for year in frontier_percentile_dfs[50].columns:\n",
    "    for idx in frontier_percentile_dfs[50].index:\n",
    "        values = [str(frontier_percentile_dfs[p].loc[idx, year]) for p in CI_percentiles]\n",
    "        df_frontier_combined.loc[idx, year] = \", \".join(values)\n",
    "display(df_frontier_combined)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
