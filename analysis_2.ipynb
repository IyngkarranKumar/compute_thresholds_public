{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and preprocess df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats, optimize\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import copy,re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"https://epochai.org/data/epochdb/notable_systems.csv\")\n",
    "url = 'https://drive.google.com/file/d/1RLLKPU3bEYK65wlQlU0p20u9M8cHkLMl/view?usp=sharing'\n",
    "url = 'https://drive.google.com/uc?id=' + url.split('/')[-2]\n",
    "\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "df = df[~df[\"Notability criteria\"].isna()]\n",
    "\n",
    "df[\"compute\"] = df[\"Training compute (FLOP)\"]\n",
    "df[\"date\"] = df[\"Publication date\"]\n",
    "df[\"model\"] = df[\"System\"]\n",
    "df[\"poss1e23\"] = df[\"Possibly over 1e23 FLOP\"]\n",
    "df[\"poss1e25\"] = df[\"Estimated over 1e25 FLOP\"]\n",
    "df[\"cost\"] = df[\"Training compute cost (2023 USD)\"]\n",
    "df[\"cost\"] = df[\"cost\"].str.replace(\",\", \"\").str.replace(\"$\", \"\").astype(float)\n",
    "\n",
    "df = df[[\"model\", \"compute\", \"date\", \"cost\", \"poss1e23\", \"poss1e25\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = ['AlphaGo Zero','AlphaZero']\n",
    "df = df[~df[\"model\"].isin(to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_append = [\n",
    "  [\"Claude 3.5 Sonnet\", 4.3e25, \"2024-06-21\", np.nan, np.nan, np.nan],\n",
    "  [\"GPT-4o Mini\", 1.2e25, \"2024-07-18\", np.nan, np.nan, np.nan],\n",
    "]\n",
    "\n",
    "for row in to_append:\n",
    "  if row[0] not in df[\"model\"].values:\n",
    "    df.loc[len(df)] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_add_compute = {\n",
    "    \"Claude 3 Opus\": 2.5e25,\n",
    "    \"Claude 3 Sonnet\": 1.1e25,\n",
    "    \"GPT-4o\": 2.9e25,\n",
    "    \"Gemini 1.0 Pro\": 2.8e24,\n",
    "    \"Gemini 1.5 Pro\": 1.9e25,\n",
    "    \"Reka Core\": 8.4e24,\n",
    "    \"GPT-4 Turbo\": 2.1e25,  # rough guess\n",
    "    \"GPT-4V\": 2.1e25,  # rough guess\n",
    "    \"Claude 2.1\": df[df[\"model\"]==\"Claude 2\"][\"compute\"].values,  # rough guess\n",
    "}\n",
    "\n",
    "for k, v in to_add_compute.items():\n",
    "  if df.loc[df[\"model\"] == k, \"compute\"].isna().values:\n",
    "    df.loc[df[\"model\"] == k, \"compute\"] = v\n",
    "  else:\n",
    "    print(f\"{k} already has a compute value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the ones we've set\n",
    "df.loc[~df[\"compute\"].isna(), \"poss1e23\"] = np.nan\n",
    "df.loc[~df[\"compute\"].isna(), \"poss1e25\"] = np.nan\n",
    "\n",
    "# Set some temporary placeholder values\n",
    "# TODO: revisit\n",
    "# df.loc[(df[\"poss1e25\"] == \"checked\"), \"compute\"] = 1.01e25  # placeholder\n",
    "# df.loc[((df[\"poss1e23\"] == \"checked\") & (df[\"poss1e25\"] != \"checked\")), \"compute\"] = 1.01e23  # placeholder\n",
    "\n",
    "# We want to handle these leading models manually via the above compute estimates.\n",
    "assert df[(df[\"poss1e25\"] == \"checked\") & (df[\"compute\"].isna())].size == 0\n",
    "\n",
    "# We sample 1e23-1e25 models with unknown compute from the existing empirical distribution.\n",
    "# TODO: revisit\n",
    "poss1e23 = ((df[\"poss1e23\"] == \"checked\") & (df[\"poss1e25\"] != \"checked\"))\n",
    "df.loc[poss1e23, \"compute\"] = df[(df[\"compute\"] >= 1e23) & (df[\"compute\"] < 1e25)][\"compute\"].sample(poss1e23.sum(), random_state=0).values\n",
    "\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df[\"log_compute\"] = np.log10(df[\"compute\"])\n",
    "\n",
    "df[\"date_float\"] = df[\"date\"].dt.year + df[\"date\"].dt.month/12\n",
    "\n",
    "df['year'] = df['date'].dt.year\n",
    "\n",
    "df = df.sort_values(\"date\")\n",
    "df.dropna(subset=\"compute\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.scatterplot(data=df[df['date']>'2010-01-01'], x='date',y='compute')\n",
    "fig.set(yscale='log')\n",
    "plt.grid(alpha=0.5)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## utils\n",
    "\n",
    "\n",
    "def exponential_fit(x,a,b):\n",
    "    return a*np.exp(b*x)\n",
    "\n",
    "def x_transform_for_exp_fit(ref_x,x,inverse=False):\n",
    "    \n",
    "    '''\n",
    "    Transform timestamps to ~ interval [0,50] for stable exp fit.\n",
    "    Can also do inverse\n",
    "    '''\n",
    "\n",
    "    norm_const = ref_x.min()\n",
    "\n",
    "    if not inverse:\n",
    "        transformed_x = (x-norm_const)/1e7 #normalising x values\n",
    "    else:\n",
    "        transformed_x = 1e7*x + norm_const \n",
    "\n",
    "    return transformed_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture; random_seed=42\n",
    "from scipy.stats import norm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.optimize import curve_fit\n",
    "import pwlf\n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class AnalysisConfig:\n",
    "    fit_start_date: str = '2017-01-01'\n",
    "    fit_stop_date: str = '2024-01-01'\n",
    "    predict_start_date: str = '2024-01-01'\n",
    "    predict_stop_date: str = '2030-01-01'\n",
    "\n",
    "class DataAnalysis():\n",
    "\n",
    "    def __init__(self,df,window_freq='quarter'):\n",
    "\n",
    "        self.df = df\n",
    "        self.df['date'] = pd.to_datetime(self.df['date'])\n",
    "        self.working_df = None\n",
    "\n",
    "        self.start_time = '2017-01-01'\n",
    "        self.stop_time = '2024-01-01'\n",
    "        self.predict_start_time = '2024-01-01'\n",
    "        self.predict_stop_time = '2030-01-01'\n",
    "        self.window_freq = window_freq\n",
    "        self.window_size = 'year'\n",
    "\n",
    "        if self.window_freq=='quarter':\n",
    "            times = pd.date_range(self.start_time,self.stop_time,freq='QS')\n",
    "            predict_times = pd.date_range(self.predict_start_time,self.predict_stop_time,freq='QS') \n",
    "        elif self.window_freq=='biannual':\n",
    "            times = pd.date_range(start=self.start_time,end=self.stop_time,freq='6MS')[1:-1] #indexing filters out startyear-01-01, endyear-01-01\n",
    "            predict_times = pd.date_range(self.predict_start_time,self.predict_stop_time,freq='6M')[1:-1] \n",
    "        elif self.window_freq=='year':\n",
    "            times = pd.date_range(start=self.start_time,end=self.stop_time,freq='AS-JUL')\n",
    "            predict_times = pd.date_range(self.predict_start_time,self.predict_stop_time,freq='AS-JUL')\n",
    "        else:\n",
    "            raise ValueError('')\n",
    "        \n",
    "        if self.window_size=='year':\n",
    "            times_lb = times - pd.DateOffset(months=6)\n",
    "            times_ub = times + pd.DateOffset(months=6)\n",
    "\n",
    "        #can use these to quickly filter df and get \n",
    "        self.window_times = times\n",
    "        self.window_times_lb = times_lb\n",
    "        self.window_times_ub = times_ub\n",
    "\n",
    "        self.predict_times = predict_times\n",
    "\n",
    "    def time_truncate_df(self,start='2017-01-01',end='2024-01-01'):\n",
    "\n",
    "        self.working_df = self.df[(self.df['date']>'2017-01-01') & (self.df['date']<'2024-01-01')]\n",
    "\n",
    "    def fit_distributions(self,fit_type,plot=False):\n",
    "\n",
    "        '''\n",
    "        May want to look at doing a fit to the rolling windows\n",
    "        \n",
    "        '''\n",
    "\n",
    "        FIT_TYPES = ['gaussian','gaussian mixture']\n",
    "        if fit_type not in FIT_TYPES:\n",
    "            raise ValueError(f'Invalid fit_type. Types: {FIT_TYPES}') \n",
    "        self.fit_type=fit_type\n",
    "\n",
    "        params = {t:None for t in self.window_times}\n",
    "\n",
    "\n",
    "        if fit_type=='gaussian':\n",
    "            \n",
    "\n",
    "            for t,t_lb,t_ub in list(zip(self.window_times,self.window_times_lb,self.window_times_ub)):\n",
    "                date_filt_condition = (self.working_df['date']>=t_lb) & (self.working_df['date'] < t_ub)\n",
    "                date_filt_df = self.working_df[date_filt_condition]\n",
    "                log_compute_data = date_filt_df['log_compute']\n",
    "\n",
    "                mean = log_compute_data.mean()\n",
    "                std = 1 #simple for now\n",
    "                params[t] = {'mean':mean,'std':std}\n",
    "\n",
    "                if plot: \n",
    "                    fig,ax=plt.subplots()\n",
    "                    plus_minus = \"\\u00B1\"\n",
    "                    sns.kdeplot(log_compute_data,label=f'timestamp: {t.date()} {plus_minus} 6mo ',linewidth=2,ax=ax)\n",
    "  \n",
    "                    mean = log_compute_data.mean()\n",
    "                    std = np.sqrt(log_compute_data.var()) #simple for now\n",
    "                    x=np.linspace(10,30,1000)\n",
    "                    ax.plot(x,norm.pdf(x,loc=mean,scale=std))\n",
    "                    ax.grid(); ax.legend(loc='upper left')\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        if fit_type == 'gaussian mixture':\n",
    "            params = {year:{} for year in years_to_fit}\n",
    "            for year in years_to_fit:\n",
    "                log_compute_data = self.working_df[self.working_df['year']==year]['log_compute']\n",
    "                gmm = GaussianMixture(n_components=2,random_state=random_seed)\n",
    "                gmm.fit(log_compute_data.to_numpy().reshape(-1,1))\n",
    "                means,covariances = gmm.means_,gmm.covariances_\n",
    "                params[year]['means']=means\n",
    "                params[year]['covars']=covariances\n",
    "        \n",
    "        self.fitted_params = params\n",
    "\n",
    "        return params\n",
    "    \n",
    "    def extrapolate_distributions(self):\n",
    "        \n",
    "        if self.fit_type=='gaussian':\n",
    "            \n",
    "            #linear extrap means\n",
    "            fit_dates = [t for t in self.fitted_params.keys()]\n",
    "            fit_dates_float = np.array([t.timestamp() for t in fit_dates])\n",
    "            means = np.array([self.fitted_params[t]['mean'] for t in self.fitted_params.keys()])\n",
    "            predicted_dates_float = np.array([t.timestamp() for t in self.predict_times])\n",
    "\n",
    "\n",
    "            model=LinearRegression()\n",
    "            model.fit(fit_dates_float.reshape(-1,1),means)\n",
    "            predicted_means = model.predict(predicted_dates_float.reshape(-1,1))\n",
    "\n",
    "            #sample std\n",
    "            std_bounds = (1.1,1.6)\n",
    "            predicted_stds = np.random.uniform(low=std_bounds[0],high=std_bounds[1],size=(predicted_means.shape))\n",
    "\n",
    "            predicted_params = {t:(mu,std) for t,mu,std in list(zip(self.predict_times,predicted_means,predicted_stds))}\n",
    "\n",
    "        elif self.fit_type=='gaussian mixture':\n",
    "            pass\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        self.predicted_params = predicted_params\n",
    "        self.distribution_parameter_model = model\n",
    "\n",
    "        return predicted_params\n",
    "    \n",
    "    \n",
    "    def model_counts(self,counts_fit_type):\n",
    "\n",
    "        COUNT_FIT_TYPES=['linear','exponential','kinked linear']\n",
    "\n",
    "        if counts_fit_type not in COUNT_FIT_TYPES: raise ValueError(f'Expected fit in {COUNT_FIT_TYPES}')\n",
    "        \n",
    "        if 0:\n",
    "            #get model counts\n",
    "            start_time = '2017-01-01'\n",
    "            stop_time = '2024-01-01'\n",
    "            if window_freq=='quarter':\n",
    "                times = pd.date_range(start_time,stop_time,freq='QE') + pd.Timedelta(days=1)\n",
    "            elif window_freq=='biannual':\n",
    "                times = pd.date_range(start_time,stop_time,freq='2QE') + pd.Timedelta(days=1)\n",
    "            elif window_freq=='year':\n",
    "                times = pd.date_range(start_time,stop_time,freq='4QE') + pd.Timedelta(days=1)\n",
    "            else:\n",
    "                raise ValueError('')\n",
    "            \n",
    "            if window_size=='year':\n",
    "                times_lb = times - pd.DateOffset(months=6)\n",
    "                times_ub = times + pd.DateOffset(months=6)\n",
    "\n",
    "        \n",
    "        #time_data is bad var name but leftover from old code\n",
    "        time_data = {t:{'size':None,\n",
    "                   } \n",
    "                   for t in self.window_times}\n",
    "        \n",
    "        for t,t_lb,t_ub in list(zip(self.window_times,self.window_times_lb,self.window_times_ub)):\n",
    "\n",
    "            if t_lb < pd.Timestamp(self.start_time) or t_ub > pd.Timestamp(self.stop_time): \n",
    "                print(f'Skipping {t} - window not in range')\n",
    "                time_data[t]['size']=None\n",
    "\n",
    "                continue\n",
    "            else:\n",
    "                date_filt_condition = (self.working_df['date']>=t_lb) & (self.working_df['date'] < t_ub)\n",
    "                date_tmp_df = self.working_df[date_filt_condition] #filtered df\n",
    "                time_data[t]['size']=date_tmp_df.shape[0]\n",
    "\n",
    "        #perform fitting\n",
    "    \n",
    "        fit_counts = np.array([t['size'] for t in time_data.values()])\n",
    "        fit_times_float = np.array([t.timestamp() for t in self.window_times])\n",
    "        predict_times_float = np.array([t.timestamp() for t in self.predict_times])\n",
    " \n",
    "        if counts_fit_type=='linear':\n",
    "            model = LinearRegression()\n",
    "            model.fit(fit_times_float.reshape(-1,1),fit_counts)\n",
    "            predicted_counts = model.predict(predict_times_float.reshape(-1,1))\n",
    "            retr_counts = model.predict(fit_times_float.reshape(-1,1)).astype('int')\n",
    "\n",
    "        elif counts_fit_type=='exponential':\n",
    "            transformed_fit_x = x_transform_for_exp_fit(ref_x=fit_times_float,x=fit_times_float)\n",
    "            popt,pcov = curve_fit(exponential_fit,transformed_fit_x,fit_counts)    \n",
    "            a,b = popt\n",
    "            transformed_pred_x = x_transform_for_exp_fit(ref_x=fit_times_float,x=predict_times_float)\n",
    "            predict_counts = exponential_fit(transformed_pred_x,a=a,b=b)\n",
    "            retr_counts = exponential_fit(transformed_fit_x,a=a,b=b)\n",
    "        elif counts_fit_type=='kinked linear':\n",
    "            model = pwlf.PiecewiseLinFit(fit_times_float,fit_counts)\n",
    "            breakpoints = model.fit(2)\n",
    "            predict_counts = model.predict(predict_times_float)\n",
    "            retr_counts = model.predict(fit_times_float)\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        \n",
    "        #set state vars\n",
    "        self.fit_counts = fit_counts\n",
    "        self.predicted_counts = predicted_counts.astype('int')\n",
    "        self.count_fit_type = counts_fit_type\n",
    "        self.retr_counts = retr_counts\n",
    "        self.counts_model = model\n",
    "\n",
    "        return predicted_counts\n",
    "    \n",
    "    def count_threshold_models(self):\n",
    "\n",
    "        thresholds = np.arange(23,30+1)\n",
    "        future_years = list(self.predict_times.year)\n",
    "        threshold_counts_df = pd.DataFrame(columns=thresholds,index=future_years)\n",
    "\n",
    "        #not doing rollouts yet\n",
    "        for pred_year,params,counts in list(zip(future_years,self.predicted_params.values(),self.predicted_counts)):\n",
    "            mu,sigma = params\n",
    "            if self.fit_type == 'gaussian':\n",
    "                log_compute_samples = norm.rvs(loc=mu,scale=sigma,size=counts)\n",
    "                for thr in thresholds:\n",
    "                    n_exceed = log_compute_samples[log_compute_samples>=thr].size\n",
    "                    threshold_counts_df.at[pred_year,thr] = n_exceed\n",
    "\n",
    "        self.threshold_counts = threshold_counts_df\n",
    "        return threshold_counts_df\n",
    "    \n",
    "    \n",
    "    def verify_with_retrodiction(self,n_years_retr):\n",
    "\n",
    "        '''\n",
    "        Params:\n",
    "            n_years_retr: Retrodict n years back\n",
    "\n",
    "        Return:\n",
    "\n",
    "        Notes:\n",
    "            - Don't think this is adapted for rolling windows yet (?)\n",
    "        \n",
    "        '''\n",
    "\n",
    "        past_years = self.window_times[-1*n_years_retr:].year\n",
    "        thresholds = [23,24]\n",
    "        predicted_past_counts = pd.DataFrame(index=past_years,columns=thresholds)\n",
    "        observed_past_counts = pd.DataFrame(index=past_years,columns=thresholds)\n",
    "        percent_error_df = pd.DataFrame(np.nan,index=past_years,columns=thresholds)\n",
    "\n",
    "\n",
    "        retrodict_times = self.window_times[-1*n_years_retr:]\n",
    "        retrodict_times_float = np.array([t.timestamp() for t in retrodict_times])\n",
    "        retr_counts = self.retr_counts\n",
    "\n",
    "        #pretty inefficient way to do it right now\n",
    "        for idx,t in enumerate(retrodict_times):\n",
    "\n",
    "            ##generate distributions and counts\n",
    "            if self.fit_type=='gaussian':\n",
    "                mean = self.distribution_parameter_model.predict(retrodict_times_float[idx].reshape(-1,1))\n",
    "                std = self.working_df[self.working_df['year']==t.year]['log_compute'].std() #just get empirical std\n",
    "            if self.count_fit_type=='linear':\n",
    "                count = self.counts_model(retrodict_times_float[idx].reshape(-1,1))\n",
    "                count = int(count)\n",
    "\n",
    "            ##generate pred log compute data\n",
    "            log_compute_data = norm.rvs(loc=mean,scale=std,size=count)\n",
    "\n",
    "            ##get obs log compute data\n",
    "            obs_log_compute_data = self.working_df[self.working_df['year']==t.year]['log_compute']\n",
    "\n",
    "            ##do threshold counts\n",
    "            for thr in thresholds:\n",
    "                #pred\n",
    "                thr_count_pr = log_compute_data[log_compute_data>=thr].size\n",
    "                predicted_past_counts.at[t.year,thr] = thr_count_pr\n",
    "\n",
    "                #obs\n",
    "                thr_count_ob = obs_log_compute_data[obs_log_compute_data>=thr].size\n",
    "                observed_past_counts.at[t.year,thr] = thr_count_ob\n",
    "\n",
    "        abs_diff = np.abs(observed_past_counts-predicted_past_counts)\n",
    "        obs_df_safe = observed_past_counts.replace(0,np.nan) #for safe division\n",
    "        percent_error_df = (abs_diff/obs_df_safe)*100\n",
    "             \n",
    "             \n",
    "        self.predicted_past_counts = predicted_past_counts\n",
    "        self.observed_past_counts = observed_past_counts\n",
    "\n",
    "\n",
    "\n",
    "        return predicted_past_counts,observed_past_counts,percent_error_df\n",
    "    \n",
    "    def verify_with_training_compute(self):\n",
    "         \n",
    "         return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = df[(df['date']>'2017-01-01') & (df['date']<'2024-01-01')]\n",
    "tmp_df['date'] = pd.to_datetime(tmp_df['date'])\n",
    "\n",
    "window_freq = 'year'\n",
    "n_year_retr=4\n",
    "\n",
    "analysis = DataAnalysis(df=tmp_df,window_freq=window_freq)\n",
    "analysis.time_truncate_df()\n",
    "params = analysis.fit_distributions(fit_type='gaussian',plot=False)\n",
    "predicted_params = analysis.extrapolate_distributions()\n",
    "predicted_counts = analysis.model_counts(counts_fit_type='linear')\n",
    "threshold_counts = analysis.count_threshold_models()\n",
    "pred_past_counts,obs_past_counts,percent_error_df = analysis.verify_with_retrodiction(n_years_retr=n_year_retr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_years_retr = 4 \n",
    "\n",
    "past_years = analysis.window_times[-1*n_years_retr:].year\n",
    "thresholds = [23,24]\n",
    "predicted_past_counts = pd.DataFrame(index=past_years,columns=thresholds)\n",
    "observed_past_counts = pd.DataFrame(index=past_years,columns=thresholds)\n",
    "\n",
    "retrodict_times = analysis.window_times[-1*n_years_retr:]\n",
    "retrodict_times_float = np.array([t.timestamp() for t in retrodict_times])\n",
    "\n",
    "retrodict_params = {t:None for t in retrodict_times}\n",
    "\n",
    "\n",
    "for idx,t in enumerate(retrodict_times):\n",
    "\n",
    "    ##generate distributions and counts\n",
    "    if analysis.fit_type=='gaussian':\n",
    "        mean = analysis.distribution_parameter_model.predict(retrodict_times_float[idx].reshape(-1,1))\n",
    "        std = tmp_df[tmp_df['year']==t.year]['log_compute'].std() #just get empirical std\n",
    "    if analysis.count_fit_type=='linear':\n",
    "        count = analysis.counts_model.predict(retrodict_times_float[idx].reshape(-1,1)).item()\n",
    "        count = int(count)\n",
    "\n",
    "    ##generate pred log compute data\n",
    "    log_compute_data = norm.rvs(loc=mean,scale=std,size=count)\n",
    "\n",
    "    ##get obs log compute data\n",
    "    obs_log_compute_data = tmp_df[tmp_df['year']==t.year]['log_compute']\n",
    "\n",
    " \n",
    "    ##do threshold counts\n",
    "    for thr in thresholds:\n",
    "        #pred\n",
    "        thr_count_pr = log_compute_data[log_compute_data>=thr].size\n",
    "        predicted_past_counts.at[t.year,thr] = thr_count_pr\n",
    "\n",
    "        #obs\n",
    "        thr_count_ob = obs_log_compute_data[obs_log_compute_data>=thr].size\n",
    "        observed_past_counts.at[t.year,thr] = thr_count_ob\n",
    "\n",
    "             \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
