{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pwlf #for colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and preprocess df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "modules = [\n",
    "    ('numpy', 'np'),\n",
    "    ('scipy.stats', 'stats'),\n",
    "    ('scipy.optimize', 'optimize'), \n",
    "    ('matplotlib.pyplot', 'plt'),\n",
    "    ('pandas', 'pd'),\n",
    "    ('seaborn', 'sns'),\n",
    "    ('itertools', 'itertools'),\n",
    "    ('copy', 'copy'),\n",
    "    ('re', 're'),\n",
    "    ('pdb', 'pdb'),\n",
    "    ('logging', 'logging')\n",
    "]\n",
    "\n",
    "for module, alias in modules:\n",
    "    start = time.time()\n",
    "    exec(f\"import {module} as {alias}\")\n",
    "    end = time.time()\n",
    "    print(f\"{module}: {end - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats, optimize\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd #taking long to load here\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import copy,re, pdb, logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger=logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"https://epochai.org/data/epochdb/notable_systems.csv\")\n",
    "url = 'https://drive.google.com/file/d/1RLLKPU3bEYK65wlQlU0p20u9M8cHkLMl/view?usp=sharing'\n",
    "url = 'https://drive.google.com/uc?id=' + url.split('/')[-2]\n",
    "\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "df = df[~df[\"Notability criteria\"].isna()]\n",
    "\n",
    "df[\"compute\"] = df[\"Training compute (FLOP)\"]\n",
    "df[\"date\"] = df[\"Publication date\"]\n",
    "df[\"model\"] = df[\"System\"]\n",
    "df[\"poss1e23\"] = df[\"Possibly over 1e23 FLOP\"]\n",
    "df[\"poss1e25\"] = df[\"Estimated over 1e25 FLOP\"]\n",
    "df[\"cost\"] = df[\"Training compute cost (2023 USD)\"]\n",
    "df[\"cost\"] = df[\"cost\"].str.replace(\",\", \"\").str.replace(\"$\", \"\").astype(float)\n",
    "\n",
    "df = df[[\"model\", \"compute\", \"date\", \"cost\", \"poss1e23\", \"poss1e25\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = ['AlphaGo Zero','AlphaZero'] #outliers\n",
    "df = df[~df[\"model\"].isin(to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_append = [\n",
    "  [\"Claude 3.5 Sonnet\", 4.3e25, \"2024-06-21\", np.nan, np.nan, np.nan],\n",
    "  [\"GPT-4o Mini\", 1.2e25, \"2024-07-18\", np.nan, np.nan, np.nan],\n",
    "]\n",
    "\n",
    "for row in to_append:\n",
    "  if row[0] not in df[\"model\"].values:\n",
    "    df.loc[len(df)] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_add_compute = {\n",
    "    \"Claude 3 Opus\": 2.5e25,\n",
    "    \"Claude 3 Sonnet\": 1.1e25,\n",
    "    \"GPT-4o\": 2.9e25,\n",
    "    \"Gemini 1.0 Pro\": 2.8e24,\n",
    "    \"Gemini 1.5 Pro\": 1.9e25,\n",
    "    \"Reka Core\": 8.4e24,\n",
    "    \"GPT-4 Turbo\": 2.1e25,  # rough guess\n",
    "    \"GPT-4V\": 2.1e25,  # rough guess\n",
    "    \"Claude 2.1\": df[df[\"model\"]==\"Claude 2\"][\"compute\"].values,  # rough guess\n",
    "}\n",
    "\n",
    "logger.info('Can add more recent models here')\n",
    "\n",
    "\n",
    "for k, v in to_add_compute.items():\n",
    "  if df.loc[df[\"model\"] == k, \"compute\"].isna().values:\n",
    "    df.loc[df[\"model\"] == k, \"compute\"] = v\n",
    "  else:\n",
    "    print(f\"{k} already has a compute value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the ones we've set\n",
    "df.loc[~df[\"compute\"].isna(), \"poss1e23\"] = np.nan\n",
    "df.loc[~df[\"compute\"].isna(), \"poss1e25\"] = np.nan\n",
    "\n",
    "# Set some temporary placeholder values\n",
    "# TODO: revisit\n",
    "# df.loc[(df[\"poss1e25\"] == \"checked\"), \"compute\"] = 1.01e25  # placeholder\n",
    "# df.loc[((df[\"poss1e23\"] ==\"checked\") & (df[\"poss1e25\"] != \"checked\")), \"compute\"] = 1.01e23  # placeholder\n",
    "\n",
    "# We want to handle these leading models manually via the above compute estimates.\n",
    "assert df[(df[\"poss1e25\"] == \"checked\") & (df[\"compute\"].isna())].size == 0\n",
    "\n",
    "# We sample 1e23-1e25 models with unknown compute from the existing empirical distribution.\n",
    "# TODO: revisit\n",
    "poss1e23 = ((df[\"poss1e23\"] == \"checked\") & (df[\"poss1e25\"] != \"checked\"))\n",
    "df.loc[poss1e23, \"compute\"] = df[(df[\"compute\"] >= 1e23) & (df[\"compute\"] < 1e25)][\"compute\"].sample(poss1e23.sum(), random_state=0).values\n",
    "\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df[\"log_compute\"] = np.log10(df[\"compute\"])\n",
    "\n",
    "df[\"date_float\"] = df[\"date\"].dt.year + df[\"date\"].dt.month/12\n",
    "\n",
    "df['year'] = df['date'].dt.year\n",
    "\n",
    "df = df.sort_values(\"date\")\n",
    "df.dropna(subset=\"compute\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate basic scatterplot\n",
    "if 1:\n",
    "    fig = sns.scatterplot(data=df[df['date']>'2010-01-01'], x='date',y='compute')\n",
    "    fig.set(yscale='log')\n",
    "    plt.grid(alpha=0.5)\n",
    "\n",
    "    # Add line of best fit for historical data\n",
    "    historical_data = df[df['date']>'2010-01-01']\n",
    "    x = historical_data['date'].astype(np.int64) // 10**9  # Convert to unix timestamp\n",
    "    y = historical_data['compute']\n",
    "    z = np.polyfit(x, np.log(y), 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(historical_data['date'], np.exp(p(x)), 'b--', alpha=0.8)\n",
    "\n",
    "    future_dates = pd.date_range(start='2025-01-01', end='2029-12-31', periods=200)\n",
    "    base = 1e25  # Starting point based on 2024 level\n",
    "    noise = np.random.normal(0, 10, len(future_dates))\n",
    "    years_from_2025 = (future_dates.year - 2025)\n",
    "\n",
    "    growth_rate = 3.0  # Exponential growth rate\n",
    "    future_compute = base * np.exp(growth_rate * years_from_2025) * (1 + noise)\n",
    "    plt.scatter(future_dates, future_compute, alpha=0.3, color='red', label='Projected - business as usual')\n",
    "\n",
    "    growth_rate = 0.4\n",
    "    future_compute = base * np.exp(growth_rate * years_from_2025) * (1 + noise)\n",
    "    plt.scatter(future_dates, future_compute, alpha=0.3, color='green', label='Projected - inference scaling')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlim([pd.Timestamp('2020-01-01'),pd.Timestamp('2030-01-01')])\n",
    "\n",
    "    for exp in range(25,31):\n",
    "        plt.axhline(y=10**exp,color='gray',linestyle='--',alpha=0.6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLOP_dollar=2e25/100e6 #FLOP per dollar conversion ~2023 (GPT-4 was ~2e25 FLOP for estimated $1e8)\n",
    "\n",
    "\n",
    "fig = sns.scatterplot(data=df[df['date']>'2010-01-01'], x='date',y=(1/FLOP_dollar)*df['compute'])\n",
    "fig.set(yscale='log')\n",
    "plt.grid(alpha=0.5)\n",
    "plt.axhline(y=1e14,label='World GDP',color='red',linestyle='--',alpha=0.8)\n",
    "plt.axhline(y=30e12,label='US GDP',color='orange',linestyle='--',alpha=0.8)\n",
    "plt.axhline(y=40e9,label='Meta R&D budget 2023',color='green',linestyle='--',alpha=0.8)\n",
    "plt.axhline(y=100e6,label='GPT-4 training cost (est)',color='purple',linestyle='--',alpha=0.8)\n",
    "\n",
    "# Add future projections\n",
    "future_dates = pd.date_range(start='2024-01-01', end='2029-12-31', periods=500)\n",
    "base = (1/FLOP_dollar) * 2e25  # Starting point based on 2024 level\n",
    "noise = np.random.normal(0, 10, len(future_dates))\n",
    "years_from_2024 = (future_dates.year - 2024)\n",
    "\n",
    "growth_rate = 3.0  # Exponential growth rate\n",
    "future_costs = base * np.exp(growth_rate * years_from_2024) * (1 + noise)\n",
    "plt.scatter(future_dates, future_costs, alpha=0.3, color='red', label='Projected - business as usual')\n",
    "\n",
    "#growth_rate = 0.4\n",
    "#future_costs = base * np.exp(growth_rate * years_from_2024) * (1 + noise)\n",
    "#plt.scatter(future_dates, future_costs, alpha=0.3, color='green', label='Projected - inference scaling')\n",
    "\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.xlim([pd.Timestamp('2020-01-01'),pd.Timestamp('2030-01-01')])\n",
    "plt.ylabel(\"Training compute cost ($)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "year_filter=[2020,2021,2022,2023]\n",
    "group_param=5\n",
    "table=pd.DataFrame(index=[f'Group {i}' for i in range(group_param)],columns=year_filter)\n",
    "\n",
    "\n",
    "for year in df['date'].dt.year.unique():\n",
    "    if year not in year_filter: continue\n",
    "    year_data = df[df['date'].dt.year == year]\n",
    "    print(f\"\\nYear: {year}\")\n",
    "    sorted_year_data=year_data.sort_values(by='compute',ascending=False)['compute']\n",
    "    grouped_data=pd.qcut(sorted_year_data,q=group_param,labels=False)\n",
    "    for group in range(group_param):\n",
    "        group_data = sorted_year_data[grouped_data == group]\n",
    "        group_share = group_data.sum() / year_data['compute'].sum() * 100\n",
    "        table.loc[f'Group {group}',year] = group_share\n",
    "        print(f\"Group {group}: {group_share:.1f}% of total compute\")\n",
    "\n",
    "\n",
    "\n",
    "# Plot pie chart of latest year's data\n",
    "latest_year = max(year_filter)\n",
    "latest_data = table[latest_year]\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.pie(latest_data, labels=[f'Group {i}' for i in range(group_param)], autopct='%1.1f%%')\n",
    "plt.title(f'Share of Total Compute by Group ({latest_year})')\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "FLOP_dollar_2024 = 2e25/100e6\n",
    "dollar_FLOP_2024 = 1/FLOP_dollar_2024\n",
    "year_grouped_df=df.groupby(df['date'][df['date']>'2010-01-01'].dt.year)\n",
    "aggregate_compute=year_grouped_df['compute'].sum()\n",
    "aggregate_compute_cost=aggregate_compute*dollar_FLOP_2024\n",
    "log_aggregate_compute=np.log10(aggregate_compute)\n",
    "log_aggregate_compute_cost=np.log10(aggregate_compute_cost)\n",
    "#plot\n",
    "# Plot historical data\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(year_grouped_df.groups.keys(), log_aggregate_compute_cost, label='Historical data')\n",
    "\n",
    "# Fit exponential for extrapolation\n",
    "# Linear regression\n",
    "x = np.array(list(year_grouped_df.groups.keys())).reshape(-1, 1)\n",
    "y = log_aggregate_compute_cost.values\n",
    "reg = LinearRegression().fit(x, y)\n",
    "\n",
    "# Generate future years for extrapolation\n",
    "future_years = np.arange(max(x), 2030).reshape(-1, 1)\n",
    "\n",
    "# Get predictions\n",
    "future_predictions = reg.predict(future_years)\n",
    "\n",
    "\n",
    "# Plot extrapolation\n",
    "plt.plot(future_years, future_predictions, '--', label='Extrapolation')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Log10(Total Compute)')\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_compute_2028 = 1e30\n",
    "cost_2024 = total_compute_2028 * dollar_FLOP_2024\n",
    "print(f\"With 2024 FLOP/dollar costs, the cost of {total_compute_2028} FLOP is approx {cost_2024/1e12:,.2f} trillion USD\")\n",
    "\n",
    "#case 1 - ~ 9 models with 1e29, 100 models with 1e27 \n",
    "#case 1 - ~ 9 models with \n",
    "\n",
    "#case 2 - ~10000 models with 1e26, 0 models above that\n",
    "\n",
    "#case 3 - 1 model 1e29, 10 models 1e28, 100 models 1e27, 1000 models 1e26 etc. \n",
    "\n",
    "years_to_iter=[2020,2021,2022,2023]\n",
    "fig,axs=plt.subplots(nrows=2,ncols=2,figsize=(8,6)); axs_ravel=axs.ravel()\n",
    "kde_fig,kde_axs=plt.subplots(nrows=2,ncols=2,figsize=(8,6)); kde_axs_ravel=kde_axs.ravel()\n",
    "\n",
    "def percentage_formatter(x,pos):\n",
    "        return f'{x:.6f}%'\n",
    "\n",
    "\n",
    "\n",
    "for idx,year in enumerate(years_to_iter):\n",
    "        ax,kde_ax=axs_ravel[idx], kde_axs_ravel[idx]\n",
    "        total_compute=aggregate_compute[aggregate_compute.index==year].values\n",
    "        cost_2023=total_compute*dollar_FLOP_2024\n",
    "        datapoints_year=df[df['date'].dt.year==year]['compute']\n",
    "        mean_log_compute=np.log10(datapoints_year).mean()\n",
    "\n",
    "        #prep data\n",
    "        sorted_computes=np.sort(datapoints_year)\n",
    "        norm_factor=total_compute[0]\n",
    "        norm_sorted_computes=sorted_computes/norm_factor\n",
    "        cumsum=np.cumsum(sorted_computes)\n",
    "        norm_cumsum=cumsum/norm_factor\n",
    "\n",
    "\n",
    "\n",
    "        #T-m plot\n",
    "        ax.plot(norm_sorted_computes,norm_cumsum)\n",
    "        ax.scatter(norm_sorted_computes, norm_cumsum, alpha=0.5, color='blue', s=30,marker='x')\n",
    "\n",
    "        ax.grid(True,alpha=0.3)\n",
    "        ax.set_xscale('log'); ax.set_yscale('log')\n",
    "        #ax.set_xlim([1e18,1e27])\n",
    "        ax.set_xlabel('individual model size'); ax.set_ylabel('Total training compute')\n",
    "        ax.set_title(f'Year: {year}')\n",
    "        ax.text(0.05, 0.95, f'Total compute: {total_compute[0]:.2e} FLOP', \n",
    "                transform=ax.transAxes, verticalalignment='top')\n",
    "        ax.axhline(y=norm_cumsum[-1],color='r',linestyle='--')\n",
    "        ax.axvline(x=1,color='g',linestyle='--',alpha=0.5)\n",
    "        ax.text(1,ax.get_ylim()[0],f'{norm_factor:.2e}',\n",
    "                rotation=90,fontsize=8,verticalalignment='top')\n",
    "        ax.yaxis.set_major_formatter(percentage_formatter)\n",
    "\n",
    "        #KDE plot \n",
    "        kde=stats.gaussian_kde(np.log10(norm_sorted_computes))\n",
    "        x_range=np.logspace(np.log10(norm_sorted_computes).min(),np.log10(1))\n",
    "        kde_ax.plot(x_range,kde(np.log10(x_range)))\n",
    "        kde_ax.set_xscale('log')\n",
    "        kde_ax.set_title(f'Year: {year}')\n",
    "        kde_ax.grid(alpha=0.5)\n",
    "\n",
    "        kde_ax.axvline(x=1,color='g',linestyle='--',alpha=0.5)\n",
    "        kde_ax.text(1,ax.get_ylim()[0],f'{norm_factor:.2e}',\n",
    "                rotation=90,fontsize=8,verticalalignment='top')\n",
    "        if idx>=2: kde_ax.set_xlabel('Model compute (normalised by total)')\n",
    "\n",
    "fig.tight_layout(pad=2.0)\n",
    "kde_fig.tight_layout(pad=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T=245\n",
    "N=10\n",
    "a,b=23,26\n",
    "\n",
    "# Generate all possible integer combinations between log_a and log_b\n",
    "possible_values = np.arange(a, b+1).astype(float)\n",
    "all_combinations = list(itertools.combinations_with_replacement(possible_values, N))\n",
    "\n",
    "# Filter combinations that sum to target\n",
    "valid_combinations = []\n",
    "for combo in all_combinations:\n",
    "    if np.sum(combo)==T:\n",
    "        valid_combinations.append(combo)\n",
    "\n",
    "valid_distributions = np.array(valid_combinations)\n",
    "\n",
    "print(valid_distributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils functions\n",
    "\n",
    " \n",
    "def exponential_fit(x,a,b):\n",
    "    return a*np.exp(b*x)\n",
    "\n",
    "def x_transform_for_exp_fit(ref_x,x,inverse=False):\n",
    "    \n",
    "    '''\n",
    "    Transform timestamps to ~ interval [0,50] for stable exp fit.\n",
    "    Can also do inverse\n",
    "    '''\n",
    "\n",
    "    norm_const = ref_x.min()\n",
    "\n",
    "    if not inverse:\n",
    "        transformed_x = (x-norm_const)/1e7 #normalising x values\n",
    "    else:\n",
    "        transformed_x = 1e7*x + norm_const \n",
    "\n",
    "    return transformed_x\n",
    "\n",
    "\n",
    "def sample_from_gmm(n_samples,params):\n",
    "    \n",
    "    mus,vars,ws = params\n",
    "    mu_l,mu_h=mus\n",
    "    var_l,var_h=vars\n",
    "    w_l,w_h=ws\n",
    "    std_l,std_h=np.sqrt(var_l),np.sqrt(var_h)\n",
    "\n",
    "    components = np.random.choice([0,1], size=n_samples,p=[w_l.item(),w_h.item()])\n",
    "\n",
    "    samples = np.where(\n",
    "        components == 0,\n",
    "        np.random.normal(loc=mu_l,scale=std_l,size=(1,n_samples)),\n",
    "        np.random.normal(loc=mu_h,scale=std_h,size=(1,n_samples))\n",
    "    )\n",
    "\n",
    "    return samples\n",
    "\n",
    "def gmm_density_plot(params,x_min,x_max):\n",
    "    \n",
    "        mus,vars,ws = params\n",
    "        mu_l,mu_h=mus\n",
    "        var_l,var_h=vars[0],vars[1]\n",
    "        w_l,w_h=ws[0],ws[1]\n",
    "        std_l,std_h=np.sqrt(var_l),np.sqrt(var_h)\n",
    "\n",
    "        x = np.linspace(x_min, x_max, 1000).reshape(-1,1)\n",
    "        pdf = np.zeros_like(x)\n",
    "\n",
    "        for mu, var, w in zip([mu_l,mu_h],[var_l,var_h],[w_l,w_h]):\n",
    "            pdf += w*stats.norm.pdf(x, mu, np.sqrt(var))\n",
    "\n",
    "        return x,pdf\n",
    "\n",
    "x,fitted_pdf = gmm_density_plot([np.array([1.5,2.5]),np.array([0.5,0.5]),np.array([0.5,0.5])],0,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture; \n",
    "from scipy.stats import norm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.optimize import curve_fit\n",
    "import pwlf, random\n",
    "\n",
    "random_seed=42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "\n",
    "class DataAnalysis():\n",
    "\n",
    "    def __init__(self,df,window_freq='year'):\n",
    "\n",
    "        self.df = df\n",
    "        self.df['date'] = pd.to_datetime(self.df['date'])\n",
    "        self.working_df = None\n",
    "\n",
    "        self.start_time = '2017-01-01'\n",
    "        self.stop_time = '2024-01-01'\n",
    "        self.predict_start_time = '2024-01-01'\n",
    "        self.predict_stop_time = '2030-01-01'\n",
    "        self.window_freq = window_freq\n",
    "        self.window_size = 'year'\n",
    "\n",
    "        if self.window_freq=='quarter':\n",
    "            times = pd.date_range(self.start_time,self.stop_time,freq='QS')\n",
    "            predict_times = pd.date_range(self.predict_start_time,self.predict_stop_time,freq='QS') \n",
    "        elif self.window_freq=='biannual':\n",
    "            times = pd.date_range(start=self.start_time,end=self.stop_time,freq='6MS')[1:-1] #indexing filters out startyear-01-01, endyear-01-01\n",
    "            predict_times = pd.date_range(self.predict_start_time,self.predict_stop_time,freq='6M')[1:-1] \n",
    "        elif self.window_freq=='year':\n",
    "            times = pd.date_range(start=self.start_time,end=self.stop_time,freq='AS-JUL')\n",
    "            predict_times = pd.date_range(self.predict_start_time,self.predict_stop_time,freq='AS-JUL')\n",
    "        else:\n",
    "            raise ValueError('')\n",
    "        \n",
    "        if self.window_size=='year':\n",
    "            times_lb = times - pd.DateOffset(months=6)\n",
    "            times_ub = times + pd.DateOffset(months=6)\n",
    "\n",
    "        #can use these to quickly filter df and get \n",
    "        self.fit_times = times\n",
    "\n",
    "        self.predict_times = predict_times\n",
    "\n",
    "    def truncate_df_dates(self,start='2017-01-01',end='2024-01-01'):\n",
    "\n",
    "        self.working_df = self.df[(self.df['date']>'2017-01-01') & (self.df['date']<'2024-01-01')]\n",
    "\n",
    "    def window_filter(self,window_time):\n",
    "\n",
    "        '''\n",
    "            Take window time and return filter for dates d such that d in [window_time-0.5*window_size,window_time+0.5*window_size]\n",
    "            e.g: Given time t, find all df entries within [t-6mo,t+6mo] for year window time\n",
    "        '''\n",
    "\n",
    "        if self.window_size=='year':\n",
    "            filtering_condition = (self.working_df['date'] >= (window_time-pd.DateOffset(months=6))) & (self.working_df['date'] < (window_time+pd.DateOffset(months=6)))\n",
    "\n",
    "        return filtering_condition \n",
    "\n",
    "    def fit_distributions(self,fit_type,plot=False):\n",
    "        '''\n",
    "\n",
    "        Help: Fit distributions based on statistics from \n",
    "        \n",
    "        NOTE: May want to look at doing a fit to the rolling windows\n",
    "        \n",
    "        '''\n",
    "\n",
    "        FIT_TYPES = ['gaussian','gaussian mixture']\n",
    "        if fit_type not in FIT_TYPES:\n",
    "            raise ValueError(f'Invalid fit_type. Types: {FIT_TYPES}') \n",
    "        self.fit_type=fit_type\n",
    "\n",
    "        params = {t:None for t in self.fit_times}\n",
    "\n",
    "\n",
    "        if fit_type=='gaussian':\n",
    "            \n",
    "            for t in self.fit_times:\n",
    "                date_filt_condition = self.window_filter(window_time=t)\n",
    "                date_filt_df = self.working_df[date_filt_condition]\n",
    "                log_compute_data = date_filt_df['log_compute']\n",
    "\n",
    "                mean = log_compute_data.mean()\n",
    "                std = log_compute_data.mean()\n",
    "                params[t] = {'mean':mean,'std':std}\n",
    "\n",
    "                if plot: \n",
    "                    fig,ax=plt.subplots()\n",
    "                    plus_minus = \"\\u00B1\"\n",
    "                    sns.kdeplot(log_compute_data,label=f'timestamp: {t.date()} {plus_minus} 6mo ',linewidth=2,ax=ax)\n",
    "  \n",
    "                    mean = log_compute_data.mean()\n",
    "                    std = np.sqrt(log_compute_data.var()) #simple for now\n",
    "                    x=np.linspace(10,30,1000)\n",
    "                    ax.plot(x,norm.pdf(x,loc=mean,scale=std))\n",
    "                    ax.grid(); ax.legend(loc='upper left')\n",
    "\n",
    "        \n",
    "        if fit_type == 'gaussian mixture':\n",
    "\n",
    "            for t in self.fit_times:\n",
    "                date_filt_condition = self.window_filter(window_time=t)\n",
    "                date_filt_df = self.working_df[date_filt_condition]\n",
    "                log_compute_data = date_filt_df['log_compute']\n",
    "\n",
    "                gmm = GaussianMixture(n_components=2,\n",
    "                                    random_state=random_seed,\n",
    "                                    max_iter=100,\n",
    "                                    n_init=10)\n",
    "                gmm.fit(log_compute_data.to_numpy().reshape(-1,1))\n",
    "                means,covariances,weights = gmm.means_,gmm.covariances_,gmm.weights_\n",
    "                params[t] = {'means':means,\n",
    "                            'covars':covariances,\n",
    "                            'weights':weights}\n",
    "\n",
    "        \n",
    "        self.fitted_params = params\n",
    "\n",
    "        return params\n",
    "    \n",
    "    def extrapolate_distributions(self):\n",
    "\n",
    "        '''\n",
    "\n",
    "        Help: Extrapolate distribution parameters to self.predict_times \n",
    "\n",
    "\n",
    "        NOTE: gmm extrapolation of covars and weights is very hacky right now\n",
    "        covars: sample from uniform distribution [0.5,1.4]\n",
    "        weights: sample from uniform distribution [0.15,0.35]\n",
    "        '''\n",
    "        \n",
    "        if self.fit_type=='gaussian':\n",
    "            \n",
    "            #linear extrap means\n",
    "            fit_dates = [t for t in self.fitted_params.keys()]\n",
    "            fit_dates_float = np.array([t.timestamp() for t in fit_dates])\n",
    "            means = np.array([self.fitted_params[t]['mean'] for t in self.fitted_params.keys()])\n",
    "            predicted_dates_float = np.array([t.timestamp() for t in self.predict_times])\n",
    "\n",
    "\n",
    "            model=LinearRegression()\n",
    "            model.fit(fit_dates_float.reshape(-1,1),means)\n",
    "            predicted_means = model.predict(predicted_dates_float.reshape(-1,1))\n",
    "            retr_means = model.predict(fit_dates_float.reshape(-1,1))\n",
    "\n",
    "            #sample std\n",
    "            std_bounds = (1.1,1.6)\n",
    "            predicted_stds = np.random.uniform(low=std_bounds[0],high=std_bounds[1],size=(predicted_means.shape))\n",
    "            retr_stds = np.empty_like(retr_means); retr_stds.fill(np.nan) #not retrodicting params for now\n",
    "\n",
    "            predicted_params = {t:(mu,std) for t,mu,std in list(zip(self.predict_times,predicted_means,predicted_stds))}\n",
    "            retr_params = {t:(mu,std) for t,mu,std in list(zip(self.fit_times\n",
    "                                                               ,retr_means,retr_stds))}\n",
    "\n",
    "            self.distribution_parameter_model = model #not good atm code atm \n",
    "\n",
    "\n",
    "        elif self.fit_type=='gaussian mixture':\n",
    "\n",
    "            lower_dist_mean_truncate = True\n",
    "            lower_var_bounds = (0.5,1.4) #heuristically set (0.5,1.4)\n",
    "            upper_var_bounds = (0.5,1.4) #exluding ~3.5 var for upper dist in 2023. Heuristic set (0.5,1.2)\n",
    "            lower_weights_bound = (0.15,0.40) #heuristically set (0.15,0.35)\n",
    "        \n",
    "\n",
    "            retr_var_bounds = ()\n",
    "            retr_var_bounds = ()\n",
    "            retr_lower_weights_bound = ()\n",
    "\n",
    "            ##get data to fit\n",
    "            lower_idx = [self.fitted_params[t]['means'].argmin() for t in self.fitted_params.keys()]\n",
    "            higher_idx = [self.fitted_params[t]['means'].argmax() for t in self.fitted_params.keys()]\n",
    "\n",
    "            lower_dist_means = np.array([self.fitted_params[t]['means'][idx] for t,idx in list(zip(self.fitted_params.keys(),lower_idx))])\n",
    "            higher_dist_means = np.array([self.fitted_params[t]['means'][idx] for t,idx in list(zip(self.fitted_params.keys(),higher_idx))])\n",
    "\n",
    "            lower_dist_covars = np.array([self.fitted_params[t]['covars'][idx] for t,idx in list(zip(self.fitted_params.keys(),lower_idx))])\n",
    "            higher_dist_covars = np.array([self.fitted_params[t]['covars'][idx] for t,idx in list(zip(self.fitted_params.keys(),higher_idx))])\n",
    "            \n",
    "            lower_dist_weights = np.array([self.fitted_params[t]['weights'][idx] for t,idx in list(zip(self.fitted_params.keys(),lower_idx))])\n",
    "            higher_dist_weights = np.array([self.fitted_params[t]['weights'][idx] for t,idx in list(zip(self.fitted_params.keys(),higher_idx))])\n",
    "\n",
    "            fit_times_float = np.array([t.timestamp() for t in self.fit_times])\n",
    "            predict_times_float = np.array([t.timestamp() for t in self.predict_times])\n",
    "\n",
    "\n",
    "            ##predict/retrodict means\n",
    "            if not lower_dist_mean_truncate:\n",
    "                lower_dist_mean_model = pwlf.PiecewiseLinFit(fit_times_float,lower_dist_means)\n",
    "            else: \n",
    "                lower_dist_mean_model = pwlf.PiecewiseLinFit(fit_times_float[:-1],lower_dist_means[:-1])\n",
    "\n",
    "            lower_dist_mean_model.fit(n_segments=2)\n",
    "            pred_lower_means = lower_dist_mean_model.predict(predict_times_float)\n",
    "            retr_lower_means = lower_dist_mean_model.predict(fit_times_float)\n",
    "\n",
    "            higher_dist_mean_model = pwlf.PiecewiseLinFit(fit_times_float,higher_dist_means)\n",
    "            higher_dist_mean_model.fit(n_segments=2)\n",
    "            pred_higher_means = higher_dist_mean_model.predict(predict_times_float)\n",
    "            retr_higher_means = higher_dist_mean_model.predict(fit_times_float)\n",
    "\n",
    "            ##extrapolate vars\n",
    "            pred_lower_vars = np.random.uniform(low=lower_var_bounds[0],high=lower_var_bounds[1],size=(pred_lower_means.shape))\n",
    "            pred_higher_vars = np.random.uniform(low=upper_var_bounds[0],high=upper_var_bounds[1],size=(pred_higher_means.shape))\n",
    "            retr_lower_vars = np.empty_like(retr_lower_means); retr_lower_vars.fill(np.nan) #emtpy as we're just taking fitted vars \n",
    "            retr_higher_vars = np.empty_like(retr_higher_means); retr_higher_vars.fill(np.nan) #empty as we're just taking fitted vars\n",
    "\n",
    "            ##extraplate weights \n",
    "            pred_lower_weights = np.random.uniform(low=lower_weights_bound[0],high=lower_weights_bound[1],size=(pred_lower_means.shape))\n",
    "            retr_lower_weights = np.empty_like(retr_lower_means); retr_lower_weights.fill(np.nan) #empty as we're just taking fitted vars\n",
    "\n",
    "            ##set predicted params state var\n",
    "            predicted_params = {t:((mu1,mu2),(var1,var2),(w1,w2)) for\n",
    "                                     t,mu1,mu2,var1,var2,w1,w2 in\n",
    "                                     list(zip(self.predict_times,\n",
    "                                              pred_lower_means,pred_higher_means,\n",
    "                                              pred_lower_vars,pred_higher_vars,\n",
    "                                              pred_lower_weights,1-pred_lower_weights))\n",
    "                                    }\n",
    "            \n",
    "            retr_params = {t:((mu1,mu2),(var1,var2),(w1,w2)) for\n",
    "                                        t,mu1,mu2,var1,var2,w1,w2 in\n",
    "                                        list(zip(self.fit_times,\n",
    "                                                retr_lower_means,retr_higher_means,\n",
    "                                                retr_lower_vars,retr_higher_vars,\n",
    "                                                retr_lower_weights,1-retr_lower_weights))\n",
    "                                        }\n",
    "            \n",
    "\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        self.predicted_params = predicted_params\n",
    "        self.retrodicted_params = retr_params\n",
    "\n",
    "        return predicted_params\n",
    "    \n",
    "    def model_counts(self,counts_fit_type):\n",
    "\n",
    "\n",
    "        ## COULD add a kinked exponential\n",
    "        COUNT_FIT_TYPES=['linear','exponential','kinked linear']\n",
    "\n",
    "        if counts_fit_type not in COUNT_FIT_TYPES: raise ValueError(f'Expected fit in {COUNT_FIT_TYPES}')\n",
    "    \n",
    "        \n",
    "        #time_data is bad var name but leftover from old code\n",
    "        counts_data = {}\n",
    "\n",
    "        for t in self.fit_times:\n",
    "\n",
    "            if (t-pd.DateOffset(months=6)) < pd.Timestamp(self.start_time) or (t+pd.DateOffset(months=6)) >= pd.Timestamp(self.stop_time): \n",
    "                print(f'Skip {t} for model counts - cannot form a window of size \"{self.window_size}\" around this time')\n",
    "                continue\n",
    "            else:\n",
    "                date_filt_condition = self.window_filter(window_time=t)\n",
    "                date_tmp_df = self.working_df[date_filt_condition] #filtered df\n",
    "                counts_data[t]=date_tmp_df.shape[0]\n",
    "\n",
    "        #perform fitting\n",
    "        fit_counts = np.array([v for v in counts_data.values()])\n",
    "        fit_times_float_c = np.array([t.timestamp() for t in counts_data.keys()]) #fit to times that have counts value. _c appended as this is JUST for fitting counts model\n",
    "        fit_times_float = np.array([t.timestamp() for t in self.fit_times])\n",
    "        predict_times_float = np.array([t.timestamp() for t in self.predict_times])\n",
    "\n",
    "        #import ipdb; ipdb.set_trace()\n",
    "\n",
    "        if counts_fit_type=='linear':\n",
    "            model = LinearRegression()\n",
    "            model.fit(fit_times_float_c.reshape(-1,1),fit_counts)\n",
    "            predicted_counts = model.predict(predict_times_float.reshape(-1,1))\n",
    "            retr_counts = model.predict(fit_times_float.reshape(-1,1)).astype('int')\n",
    "\n",
    "        elif counts_fit_type=='exponential':\n",
    "            transformed_fit_x = x_transform_for_exp_fit(ref_x=fit_times_float,x=fit_times_float_c)\n",
    "            popt,pcov = curve_fit(exponential_fit,transformed_fit_x,fit_counts)    \n",
    "            a,b = popt; model = popt\n",
    "            transformed_pred_x = x_transform_for_exp_fit(ref_x=fit_times_float,x=predict_times_float)\n",
    "            predicted_counts = exponential_fit(transformed_pred_x,a=a,b=b)\n",
    "            retr_counts = exponential_fit(transformed_fit_x,a=a,b=b)\n",
    "\n",
    "        elif counts_fit_type=='kinked linear':\n",
    "            model = pwlf.PiecewiseLinFit(fit_times_float,fit_counts)\n",
    "            breakpoints = model.fit(2)\n",
    "            predicted_counts = model.predict(predict_times_float)\n",
    "            retr_counts = model.predict(fit_times_float)\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        \n",
    "        #set state vars\n",
    "        self.fit_counts = fit_counts\n",
    "        self.predicted_counts = predicted_counts.astype('int')\n",
    "        self.count_fit_type = counts_fit_type\n",
    "        self.retr_counts = retr_counts; assert len(retr_counts) == len(self.fit_times_float), print(retr_counts,self.fit_times) #sanity check\n",
    "        self.counts_model = model\n",
    "\n",
    "        return predicted_counts\n",
    "\n",
    "    def generate_simulated_data(self):\n",
    "        \"\"\"\n",
    "        Generated simulated log_compute samples based on choice of compute distribution and count model\n",
    "        \"\"\"\n",
    "\n",
    "        self.simulated_data={}\n",
    "\n",
    "        for pred_t,params,counts in list(zip(self.predict_times,self.predicted_params.values(),self.predicted_counts)):\n",
    "                if self.fit_type == 'gaussian':\n",
    "                    mu,sigma = params\n",
    "                    log_compute_samples = norm.rvs(loc=mu,scale=sigma,size=counts)\n",
    "                    self.simulated_data[pred_t] = log_compute_samples\n",
    "    \n",
    "                if self.fit_type== 'gaussian mixture':\n",
    "                    mus,vars,ws = params\n",
    "                    mu_l,mu_h = mus\n",
    "                    var_l,var_h = vars\n",
    "                    w_l,w_h = ws \n",
    "                    std_l,std_h = np.sqrt(var_l),np.sqrt(var_h)\n",
    "    \n",
    "                    log_compute_samples = sample_from_gmm(n_samples=counts,params=params)\n",
    "                    self.simulated_data[pred_t] = log_compute_samples\n",
    "\n",
    "    def count_threshold_models(self):\n",
    "\n",
    "        thresholds = np.arange(23,30+1)\n",
    "        threshold_counts_df = pd.DataFrame(columns=thresholds,index=self.predict_times)\n",
    "\n",
    "        for t in self.predict_times:\n",
    "            log_compute_samples = self.simulated_data[t]\n",
    "            for thr in thresholds:\n",
    "                n_exceed = log_compute_samples[log_compute_samples>=thr].size\n",
    "                threshold_counts_df.at[t,thr] = n_exceed\n",
    "\n",
    "\n",
    "        self.threshold_counts = threshold_counts_df\n",
    "        return threshold_counts_df\n",
    "    \n",
    "    def count_frontier_models(self,frontier_thresholds=[0.5,1,1.5],past_counts=True):\n",
    "        ''' \n",
    "        Do counting for frontier connected threshold.        \n",
    "\n",
    "        Careful with interpretation of this given window size = year, but window freq != year sometimes\n",
    "        '''\n",
    "    \n",
    "        frontier_counts_df = pd.DataFrame(columns=frontier_thresholds,index=self.predict_times)\n",
    "\n",
    "        running_max=0\n",
    "        for t in self.predict_times:\n",
    "            for thr in frontier_thresholds:\n",
    "                #we want number of samples within thr of max\n",
    "                log_compute_samples = self.simulated_data[t]\n",
    "                max_val = log_compute_samples.max()\n",
    "                if max_val>running_max: running_max=max_val\n",
    "                n_exceed = log_compute_samples[(running_max-log_compute_samples)<=thr].size\n",
    "                frontier_counts_df.at[t,thr] = n_exceed\n",
    "\n",
    "        self.frontier_counts_df = frontier_counts_df\n",
    "\n",
    "        #do frontier connected counts for past counts \n",
    "        if past_counts:\n",
    "            frontier_counts_df_past = pd.DataFrame(columns=frontier_thresholds,index=self.fit_times)\n",
    "\n",
    "            running_max=0\n",
    "            for t in self.fit_times:\n",
    "                for thr in frontier_thresholds:\n",
    "                    #we want number of samples within thr of max\n",
    "                    log_compute_data = self.working_df[self.window_filter(t)]['log_compute']\n",
    "                    max_val = log_compute_data.max()\n",
    "                    if max_val>running_max: running_max=max_val\n",
    "                    n_exceed = log_compute_data[(running_max-log_compute_data)<=thr].size\n",
    "                    frontier_counts_df_past.at[t,thr] = n_exceed\n",
    "            \n",
    "            self.past_frontier_counts_df = frontier_counts_df_past\n",
    "            \n",
    "\n",
    "        return frontier_counts_df,frontier_counts_df_past\n",
    "                  \n",
    "    def verify_with_retrodiction(self):\n",
    "\n",
    "        '''\n",
    "        Params:\n",
    "\n",
    "        Return:\n",
    "\n",
    "        NOTE:\n",
    "            - A lot of this isn't 'true' retrodiction - we're using some fitted parameters rather than retrodicted parameters\n",
    "        '''\n",
    "\n",
    "        retrodict_times = self.fit_times #retrodict for fitted time stamps\n",
    "        retrodict_times_float = np.array([t.timestamp() for t in retrodict_times])\n",
    "        retr_counts = self.retr_counts\n",
    "\n",
    "        assert len(retr_counts) == len(retrodict_times), print(retr_counts,retrodict_times)\n",
    "\n",
    "\n",
    "        thresholds = [23,24]\n",
    "        predicted_past_counts = pd.DataFrame(index=retrodict_times,columns=thresholds)\n",
    "        observed_past_counts = pd.DataFrame(index=retrodict_times,columns=thresholds)\n",
    "        percent_error_df = pd.DataFrame(np.nan,index=retrodict_times,columns=thresholds)\n",
    "\n",
    "\n",
    "\n",
    "        #pretty inefficient way to do it right now\n",
    "        for idx,t in enumerate(retrodict_times):\n",
    "\n",
    "            ##generate distributions and counts\n",
    "            count = int(retr_counts[idx])\n",
    "\n",
    "            if self.fit_type=='gaussian':\n",
    "                mean = self.retrodicted_params[t][0]\n",
    "                std = self.retrodicted_params[t][1]\n",
    "\n",
    "                if np.isnan(std): std = self.working_df[self.window_filter(t)]['log_compute'].std() #USE EMPIRICAL STDS\n",
    "\n",
    "                pred_log_compute_data = norm.rvs(loc=mean,scale=std,size=count)\n",
    "\n",
    "            elif self.fit_type=='gaussian mixture':\n",
    "            \n",
    "                #unpack retrodicted params\n",
    "                mus,vars,ws = self.retrodicted_params[t]\n",
    "                mu_l,mu_h = mus\n",
    "                var_l,var_h = vars\n",
    "                w_l,w_h = ws\n",
    " \n",
    "                if np.isnan(var_l): var_l = self.fitted_params[t]['covars'][0] #use fitted vars\n",
    "                if np.isnan(var_h): var_h = self.fitted_params[t]['covars'][1] #use fitted vars\n",
    "                if np.isnan(w_l): w_l = self.fitted_params[t]['weights'][0] #use fitted gmm weights\n",
    "                if np.isnan(w_h): w_h = 1-w_l #use fitted gmm weights\n",
    "\n",
    "                w_l,w_h = np.array([w_l]), np.array([w_h]) #for type compatibility with other parts of workflow\n",
    "\n",
    "                params_retr = ((mu_l,mu_h),(var_l,var_h),(w_l,w_h))\n",
    "                pred_log_compute_data = sample_from_gmm(n_samples=count,params=params_retr)\n",
    "\n",
    "            ##get obs log compute data\n",
    "            obs_log_compute_data = self.working_df[self.window_filter(t)]['log_compute']\n",
    "\n",
    "            ##do threshold counts\n",
    "            for thr in thresholds:\n",
    "                #pred\n",
    "                thr_count_pr = pred_log_compute_data[pred_log_compute_data>=thr].size\n",
    "                predicted_past_counts.at[t,thr] = thr_count_pr\n",
    "\n",
    "                #obs\n",
    "                thr_count_ob = obs_log_compute_data[obs_log_compute_data>=thr].size\n",
    "                observed_past_counts.at[t,thr] = thr_count_ob\n",
    "\n",
    "        diff = predicted_past_counts - observed_past_counts\n",
    "        obs_df_safe = observed_past_counts.replace(0,np.nan) #for safe division\n",
    "        percent_error_df = (np.abs(diff)/obs_df_safe)*100\n",
    "             \n",
    "             \n",
    "        self.predicted_past_counts = predicted_past_counts\n",
    "        self.observed_past_counts = observed_past_counts\n",
    "\n",
    "        return predicted_past_counts,observed_past_counts,diff,percent_error_df\n",
    "\n",
    "    def verify_with_training_compute(self):\n",
    "         \n",
    "        '''\n",
    "        Help: Compare predictions with extrapolations of training compute \n",
    "        NOTE: Not implemented for DataAnalysis class yet\n",
    "\n",
    "        '''\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##study one config\n",
    "random_seed=42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "tmp_df = df[(df['date']>'2017-01-01') & (df['date']<'2024-01-01')]\n",
    "tmp_df['date'] = pd.to_datetime(tmp_df['date'])\n",
    "\n",
    "window_freq = 'year' \n",
    "dist_fit='gaussian'\n",
    "count_fit='linear'\n",
    "\n",
    "analysis = DataAnalysis(df=tmp_df,window_freq=window_freq)\n",
    "analysis.truncate_df_dates()\n",
    "params = analysis.fit_distributions(fit_type=dist_fit,plot=False)\n",
    "predicted_params = analysis.extrapolate_distributions()\n",
    "predicted_counts = analysis.model_counts(counts_fit_type=count_fit)\n",
    "simulated_data = analysis.generate_simulated_data()\n",
    "threshold_counts = analysis.count_threshold_models()\n",
    "frontier_connected_counts,past_frontier_counts = analysis.count_frontier_models(frontier_thresholds=[0.5,1,1.5])\n",
    "\n",
    "#diff = pred - obs\n",
    "pred_past_counts,obs_past_counts,diff,percent_error_df = analysis.verify_with_retrodiction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all model predictions\n",
    "\n",
    "import itertools, warnings, tabulate\n",
    "\n",
    "random_seed=42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "distribution_fits = ['gaussian']\n",
    "model_count_fits = ['linear','exponential','kinked linear']\n",
    "window_freq = 'year'\n",
    "\n",
    "index = [f\"{dist_fit}-{count_fit}\" for (dist_fit,count_fit) in list(itertools.product(distribution_fits,model_count_fits))]\n",
    "all_retrodictions_df = pd.DataFrame(index=index,columns=analysis.fit_times.year)\n",
    "all_predictions_df = pd.DataFrame(index=index,columns=analysis.predict_times.year)\n",
    "\n",
    "for (dist_fit,count_fit) in list(itertools.product(distribution_fits,model_count_fits)):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        analysis = DataAnalysis(df=tmp_df,window_freq=window_freq)\n",
    "        analysis.truncate_df_dates()\n",
    "        _ = analysis.fit_distributions(fit_type=dist_fit,plot=False)\n",
    "        _ = analysis.extrapolate_distributions() \n",
    "        _ = analysis.model_counts(counts_fit_type=count_fit)\n",
    "        _ = analysis.generate_simulated_data()\n",
    "        threshold_counts = analysis.count_threshold_models()\n",
    "        pred_past_counts,obs_past_counts,diff,percent_error_df = analysis.verify_with_retrodiction()\n",
    "\n",
    "        diff_23,diff_24 = diff.loc[:,23],diff.loc[:,24]\n",
    "        diffs = list(zip(diff_23,diff_24))\n",
    "        all_retrodictions_df.loc[f'{dist_fit}-{count_fit}'] = diffs\n",
    "\n",
    "        pred_25,pred_26 = threshold_counts.loc[:,25],threshold_counts.loc[:,26]\n",
    "        all_predictions_df.loc[f'{dist_fit}-{count_fit}'] = list(zip(pred_25,pred_26))\n",
    "\n",
    "print(tabulate.tabulate(all_retrodictions_df,headers='keys',tablefmt='pretty'))\n",
    "print('''\n",
    "    Interpreting the table:\n",
    "    - Rows - distribution fit, count fit combination\n",
    "    - Columns - Retrodicted counts \n",
    "    - Values - (predicted - observed) counts for 1e23 and 1e24 thresholds\n",
    "    - E.g: The gaussian mixture model with linear model count fit underpredicted by 3 for 1e23 threshold\n",
    "        and underpredicted by 2 for 1e24 threshold for 2021\n",
    "\n",
    "''')\n",
    "\n",
    "print(tabulate.tabulate(all_predictions_df,headers='keys',tablefmt='pretty'))\n",
    "print('''\n",
    "    Interpreting the table:\n",
    "    - Rows - distribution fit, count fit combination\n",
    "    - Columns - Predicted counts\n",
    "    - Values - Predicted counts for models exceeding 1e25 and 1e26 thresholds\n",
    "    - E.g: The gaussian mixture model with linear model count fit predicts 16 models exceeding 1e25 FLOP and \n",
    "      2 models exceeding 1e26 FLOP in 2027 \n",
    "      \n",
    "''')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore gmms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed=42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "tmp_df = df[(df['date']>'2017-01-01') & (df['date']<'2024-01-01')]\n",
    "tmp_df['date'] = pd.to_datetime(tmp_df['date'])\n",
    "\n",
    "window_freq = 'year' \n",
    "dist_fit='gaussian mixture'\n",
    "count_fit='linear' \n",
    "\n",
    "#execute workflow \n",
    "analysis = DataAnalysis(df=tmp_df,window_freq=window_freq)\n",
    "analysis.truncate_df_dates()\n",
    "params = analysis.fit_distributions(fit_type=dist_fit,plot=False)\n",
    "predicted_params = analysis.extrapolate_distributions()\n",
    "predicted_counts = analysis.model_counts(counts_fit_type=count_fit)\n",
    "threshold_counts = analysis.count_threshold_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_gmm_params = analysis.fitted_params\n",
    "predicted_gmm_params = analysis.predicted_params\n",
    "\n",
    "#plot KDEs and fitted distributions\n",
    "ncols,nrows = (2,4)\n",
    "fig,axs = plt.subplots(ncols=ncols,nrows=nrows,figsize=(8,10))\n",
    "axs = axs.ravel()\n",
    "\n",
    "for idx,(time,params) in enumerate(fitted_gmm_params.items()):\n",
    "    ax = axs[idx]\n",
    "    means,vars,weights = params['means'],params['covars'],params['weights']\n",
    "    means = (means[0].item(),means[1].item())\n",
    "    \n",
    "    ax.set_title(f'{time.year}')\n",
    "    x,fitted_pdf = gmm_density_plot(params=(means,vars,weights),x_min=10,x_max=30)\n",
    "    log_compute_data = analysis.working_df[analysis.window_filter(time)]['log_compute']\n",
    "\n",
    "    ax.plot(x.squeeze(),fitted_pdf.squeeze(),linewidth=2,label=f'fitted gmm, n={len(log_compute_data)}')\n",
    "    sns.kdeplot(log_compute_data,ax=ax,linewidth=2,alpha=0.7,label='empirical kde')\n",
    "\n",
    "for ax in axs: ax.grid(); ax.legend()\n",
    "fig.tight_layout()\n",
    "\n",
    "ncols,nrows = (4,2)\n",
    "fig,axs = plt.subplots(ncols=ncols,nrows=nrows,figsize=(8,10))\n",
    "axs = axs.ravel()\n",
    "\n",
    "for idx,(time,params) in enumerate(fitted_gmm_params.items()):\n",
    "    ax = axs[idx]\n",
    "    means,vars,weights = params['means'],params['covars'],params['weights']\n",
    "    means = (means[0].item(),means[1].item())\n",
    "    ax.set_title(f'{time.year}')\n",
    "\n",
    "    lower_idx,upper_idx = means.index(min(means)),means.index(max(means))\n",
    "\n",
    "    x = np.linspace(10,30,1000)\n",
    "    pdf_lower = weights[lower_idx]*stats.norm.pdf(x,means[lower_idx],np.sqrt(vars[lower_idx]))\n",
    "    pdf_upper = weights[upper_idx]*stats.norm.pdf(x,means[upper_idx],np.sqrt(vars[upper_idx]))\n",
    "    ax.plot(x,pdf_lower.squeeze(),label='lower dist',c='blue')\n",
    "    ax.plot(x,pdf_upper.squeeze(),label='upper dist',c='red')\n",
    "\n",
    "    ax.grid();ax.legend();ax.set_ylim(0,0.5)\n",
    "\n",
    "fig.set_size_inches(12,6)\n",
    "fig.tight_layout()\n",
    "\n",
    "\n",
    "#visualise fitted and extrapolated params\n",
    "fig,(ax1,ax2,ax3) = plt.subplots(ncols=3)\n",
    "fit_times = list(fitted_gmm_params.keys())\n",
    "predict_times = list(predicted_gmm_params.keys())\n",
    "low_color = 'b'; high_color = 'r'\n",
    "fit_marker = 'o'; predict_marker = 'x'\n",
    "\n",
    "if 1: #plot fitted params (o's)\n",
    "    low_idx = [fitted_gmm_params[t]['means'].argmin() for t in fit_times] #get the idx for the lower dist (by mean)\n",
    "    high_idx = [fitted_gmm_params[t]['means'].argmax() for t in fit_times] #get the idx for the upper dist (by mean)\n",
    "\n",
    "    lower_dist_means = [fitted_gmm_params[t]['means'][idx] for t,idx in list(zip(fit_times,low_idx))]\n",
    "    upper_dist_means = [fitted_gmm_params[t]['means'][idx] for t,idx in  list(zip(fit_times,high_idx))]\n",
    "\n",
    "    lower_dist_vars = [fitted_gmm_params[t]['covars'][idx] for t,idx in list(zip(fit_times,low_idx))]\n",
    "    upper_dist_vars = [fitted_gmm_params[t]['covars'][idx] for t,idx in  list(zip(fit_times,high_idx))]\n",
    "\n",
    "    lower_dist_weight = [fitted_gmm_params[t]['weights'][idx] for t,idx in list(zip(fit_times,low_idx))]\n",
    "    upper_dist_weight = [fitted_gmm_params[t]['weights'][idx] for t,idx in list(zip(fit_times,high_idx))]\n",
    "\n",
    "    ax1.scatter(fit_times,lower_dist_means,c=low_color,marker=fit_marker,label='lower dist')\n",
    "    ax1.scatter(fit_times,upper_dist_means,c=high_color,marker=fit_marker,label='upper dist')\n",
    "\n",
    "    ax2.scatter(fit_times,lower_dist_vars,c=low_color,marker=fit_marker,label='fitted lower dist')\n",
    "    ax2.scatter(fit_times,upper_dist_vars,c=high_color,marker=fit_marker,label='fitted upper dist')\n",
    "\n",
    "    ax3.scatter(fit_times,lower_dist_weight,c=low_color,marker=fit_marker,label='lower dist')\n",
    "    ax3.scatter(fit_times,upper_dist_weight,c=high_color,marker=fit_marker,label='upper dist')\n",
    "\n",
    "\n",
    "if 1: #plot predicted params (x's)\n",
    "    low_dist_means = [predicted_gmm_params[t][0][0] for t in predict_times]\n",
    "    high_dist_means = [predicted_gmm_params[t][0][1] for t in predict_times]\n",
    "\n",
    "    low_dist_vars = [predicted_gmm_params[t][1][0] for t in predict_times]\n",
    "    high_dist_vars = [predicted_gmm_params[t][1][1] for t in predict_times]\n",
    "\n",
    "    low_dist_weight = [predicted_gmm_params[t][2][0] for t in predict_times]\n",
    "    high_dist_weight = [predicted_gmm_params[t][2][1] for t in predict_times]\n",
    "\n",
    "    ax1.scatter(predict_times,low_dist_means,c=low_color,marker=predict_marker,label='lower dist')\n",
    "    ax1.scatter(predict_times,high_dist_means,c=high_color,marker=predict_marker,label='upper dist')\n",
    "\n",
    "    ax2.scatter(predict_times,low_dist_vars,c=low_color,marker=predict_marker,label='predicted lower dist')\n",
    "    ax2.scatter(predict_times,high_dist_vars,c=high_color,marker=predict_marker,label='predicted upper dist')\n",
    "\n",
    "    ax3.scatter(predict_times,low_dist_weight,c=low_color,marker=predict_marker,label='lower dist')\n",
    "    ax3.scatter(predict_times,high_dist_weight,c=high_color,marker=predict_marker,label='upper dist')\n",
    "    \n",
    "\n",
    "\n",
    "for ax in (ax1,ax2,ax3): \n",
    "    #ax.set_xticks(np.concatenate([[t.year for t in fit_times],[t.year for t in predict_times]]))\n",
    "    ax.grid()\n",
    "\n",
    "    if ax==ax2:\n",
    "        pass\n",
    "        ax2.legend(loc=(0.5,0.6))\n",
    "\n",
    "\n",
    "\n",
    "fig.set_size_inches(12,4)\n",
    "fig.suptitle('Fitted and Predicted GMM Parameters')\n",
    "fig.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Explore the 2023 fit\n",
    "## 2023 captures a nice skew\n",
    "## also gmm interpretation is nice - two distributions evolving together \n",
    "\n",
    "##note - I've just manually found a better covar fit than automatic method I've been using\n",
    "import copy\n",
    "\n",
    "gmm_params_2023 = copy.deepcopy(fitted_gmm_params[pd.Timestamp('2023-07-01')])\n",
    "print(gmm_params_2023)\n",
    "means,covars,weights = gmm_params_2023['means'],gmm_params_2023['covars'],gmm_params_2023['weights']\n",
    "covars[0],covars[1] = 0.5,3 #1.7,3\n",
    "weights[0]=0.8 #0.8\n",
    "weights[1] = 1-weights[0]\n",
    "\n",
    "x,fitted_pdf = gmm_density_plot(params=(means,covars,weights),x_min=10,x_max=30)\n",
    "log_compute_data = analysis.working_df[analysis.window_filter(pd.Timestamp('2023-07-01'))]['log_compute']\n",
    "fig,ax=plt.subplots()\n",
    "ax.plot(x.squeeze(),fitted_pdf.squeeze(),linewidth=2,label='fitted gmm',c='tab:blue')\n",
    "#sns.kdeplot(log_compute_data,ax=ax,linewidth=2,alpha=0.7,label='empirical kde',c='tab:red')\n",
    "ax.legend(); ax.grid();ax.set_ylim([0,0.4])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Handfitting the gmm, we see:\n",
    "- we can get a better fit with hand than our automatic process did\n",
    "  - So imperfect fit might not be a result of inflexibility of distribution, but suboptimal fitting process instead.\n",
    "  - Intuitively, what does our fit mean?\n",
    "    - We've got two model families, call them lower and upper for now.\n",
    "    - In 2023, the upper distribution was dominant, with a weight of 80%. That is, we should sample 80% of the time from the upper distribution.\n",
    "    - In 2023, the upper distribution was significantly more concentrated, with a variance of ~55% of the lower mean. So the lower mean was more spread over models.\n",
    "    - This could fit the idea of a recent class of models that are highly 'compute specialised' -> all being trained in the ~1e21 -> 1e25 FLOP range (?)\n",
    "  - Fitted parameter trends\n",
    "    - Around 2021 we see the weights of the upper distribution pick up, a change in the trend from pre-2020 where the weights were similar, and favouring the lower distribution\n",
    "    - Again - around 2021, the upper distribution begins to clearly dominate.\n",
    "    - An increase in the lower dist variance contributes to non-bumpy skew in 2023 distribution. This means we don't see the 'two clear modes', just one instead.\n",
    "      - So *whilst* pre-2023 distributions show a clear double-bumb, if we want to achieve a single-bump + minor skew, we set a low variance for the lower-weighted distribution. \n",
    "    - 2023 surprises me for the variances and weights\n",
    "      - variances - both *jump* relative to pre-2023.\n",
    "      - Weights - again, jumps a noticeable amount.\n",
    "    - AGAIN - let's note, these may not be the best fits. E.g: see our better fit for the 2023 distribution.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- What are the implications of this on future distributions?\n",
    "  - Both classes of models will see mean compute rise\n",
    "  - I can see \"upper class\" spread out a bit, as foundation model compute range becomes more broad\n",
    "  - I can see upper class take increasing weight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sci_comp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
