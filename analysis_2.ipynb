{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and preprocess df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats, optimize\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import copy,re, pdb\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"https://epochai.org/data/epochdb/notable_systems.csv\")\n",
    "url = 'https://drive.google.com/file/d/1RLLKPU3bEYK65wlQlU0p20u9M8cHkLMl/view?usp=sharing'\n",
    "url = 'https://drive.google.com/uc?id=' + url.split('/')[-2]\n",
    "\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "df = df[~df[\"Notability criteria\"].isna()]\n",
    "\n",
    "df[\"compute\"] = df[\"Training compute (FLOP)\"]\n",
    "df[\"date\"] = df[\"Publication date\"]\n",
    "df[\"model\"] = df[\"System\"]\n",
    "df[\"poss1e23\"] = df[\"Possibly over 1e23 FLOP\"]\n",
    "df[\"poss1e25\"] = df[\"Estimated over 1e25 FLOP\"]\n",
    "df[\"cost\"] = df[\"Training compute cost (2023 USD)\"]\n",
    "df[\"cost\"] = df[\"cost\"].str.replace(\",\", \"\").str.replace(\"$\", \"\").astype(float)\n",
    "\n",
    "df = df[[\"model\", \"compute\", \"date\", \"cost\", \"poss1e23\", \"poss1e25\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = ['AlphaGo Zero','AlphaZero']\n",
    "df = df[~df[\"model\"].isin(to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_append = [\n",
    "  [\"Claude 3.5 Sonnet\", 4.3e25, \"2024-06-21\", np.nan, np.nan, np.nan],\n",
    "  [\"GPT-4o Mini\", 1.2e25, \"2024-07-18\", np.nan, np.nan, np.nan],\n",
    "]\n",
    "\n",
    "for row in to_append:\n",
    "  if row[0] not in df[\"model\"].values:\n",
    "    df.loc[len(df)] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_add_compute = {\n",
    "    \"Claude 3 Opus\": 2.5e25,\n",
    "    \"Claude 3 Sonnet\": 1.1e25,\n",
    "    \"GPT-4o\": 2.9e25,\n",
    "    \"Gemini 1.0 Pro\": 2.8e24,\n",
    "    \"Gemini 1.5 Pro\": 1.9e25,\n",
    "    \"Reka Core\": 8.4e24,\n",
    "    \"GPT-4 Turbo\": 2.1e25,  # rough guess\n",
    "    \"GPT-4V\": 2.1e25,  # rough guess\n",
    "    \"Claude 2.1\": df[df[\"model\"]==\"Claude 2\"][\"compute\"].values,  # rough guess\n",
    "}\n",
    "\n",
    "for k, v in to_add_compute.items():\n",
    "  if df.loc[df[\"model\"] == k, \"compute\"].isna().values:\n",
    "    df.loc[df[\"model\"] == k, \"compute\"] = v\n",
    "  else:\n",
    "    print(f\"{k} already has a compute value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the ones we've set\n",
    "df.loc[~df[\"compute\"].isna(), \"poss1e23\"] = np.nan\n",
    "df.loc[~df[\"compute\"].isna(), \"poss1e25\"] = np.nan\n",
    "\n",
    "# Set some temporary placeholder values\n",
    "# TODO: revisit\n",
    "# df.loc[(df[\"poss1e25\"] == \"checked\"), \"compute\"] = 1.01e25  # placeholder\n",
    "# df.loc[((df[\"poss1e23\"] ==\"checked\") & (df[\"poss1e25\"] != \"checked\")), \"compute\"] = 1.01e23  # placeholder\n",
    "\n",
    "# We want to handle these leading models manually via the above compute estimates.\n",
    "assert df[(df[\"poss1e25\"] == \"checked\") & (df[\"compute\"].isna())].size == 0\n",
    "\n",
    "# We sample 1e23-1e25 models with unknown compute from the existing empirical distribution.\n",
    "# TODO: revisit\n",
    "poss1e23 = ((df[\"poss1e23\"] == \"checked\") & (df[\"poss1e25\"] != \"checked\"))\n",
    "df.loc[poss1e23, \"compute\"] = df[(df[\"compute\"] >= 1e23) & (df[\"compute\"] < 1e25)][\"compute\"].sample(poss1e23.sum(), random_state=0).values\n",
    "\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df[\"log_compute\"] = np.log10(df[\"compute\"])\n",
    "\n",
    "df[\"date_float\"] = df[\"date\"].dt.year + df[\"date\"].dt.month/12\n",
    "\n",
    "df['year'] = df['date'].dt.year\n",
    "\n",
    "df = df.sort_values(\"date\")\n",
    "df.dropna(subset=\"compute\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.scatterplot(data=df[df['date']>'2010-01-01'], x='date',y='compute')\n",
    "fig.set(yscale='log')\n",
    "plt.grid(alpha=0.5)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## utils\n",
    "def exponential_fit(x,a,b):\n",
    "    return a*np.exp(b*x)\n",
    "\n",
    "def x_transform_for_exp_fit(ref_x,x,inverse=False):\n",
    "    \n",
    "    '''\n",
    "    Transform timestamps to ~ interval [0,50] for stable exp fit.\n",
    "    Can also do inverse\n",
    "    '''\n",
    "\n",
    "    norm_const = ref_x.min()\n",
    "\n",
    "    if not inverse:\n",
    "        transformed_x = (x-norm_const)/1e7 #normalising x values\n",
    "    else:\n",
    "        transformed_x = 1e7*x + norm_const \n",
    "\n",
    "    return transformed_x\n",
    "\n",
    "\n",
    "def sample_from_gmm(n_samples,params):\n",
    "    \n",
    "    mus,vars,ws = params\n",
    "    mu_l,mu_h=mus\n",
    "    var_l,var_h=vars\n",
    "    w_l,w_h=ws\n",
    "    std_l,std_h=np.sqrt(var_l),np.sqrt(var_h)\n",
    "\n",
    "    components = np.random.choice([0,1], size=n_samples,p=[w_l.item(),w_h.item()])\n",
    "\n",
    "    samples = np.where(\n",
    "        components == 0,\n",
    "        np.random.normal(loc=mu_l,scale=std_l,size=(1,n_samples)),\n",
    "        np.random.normal(loc=mu_h,scale=std_h,size=(1,n_samples))\n",
    "    )\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed=42\n",
    "\n",
    "from sklearn.mixture import GaussianMixture; random_seed=42\n",
    "from scipy.stats import norm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.optimize import curve_fit\n",
    "import pwlf\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class AnalysisConfig:\n",
    "    fit_start_date: str = '2017-01-01'\n",
    "    fit_stop_date: str = '2024-01-01'\n",
    "    predict_start_date: str = '2024-01-01'\n",
    "    predict_stop_date: str = '2030-01-01'\n",
    "\n",
    "class DataAnalysis():\n",
    "\n",
    "    def __init__(self,df,window_freq='year'):\n",
    "\n",
    "        self.df = df\n",
    "        self.df['date'] = pd.to_datetime(self.df['date'])\n",
    "        self.working_df = None\n",
    "\n",
    "        self.start_time = '2017-01-01'\n",
    "        self.stop_time = '2024-01-01'\n",
    "        self.predict_start_time = '2024-01-01'\n",
    "        self.predict_stop_time = '2030-01-01'\n",
    "        self.window_freq = window_freq\n",
    "        self.window_size = 'year'\n",
    "\n",
    "        if self.window_freq=='quarter':\n",
    "            times = pd.date_range(self.start_time,self.stop_time,freq='QS')\n",
    "            predict_times = pd.date_range(self.predict_start_time,self.predict_stop_time,freq='QS') \n",
    "        elif self.window_freq=='biannual':\n",
    "            times = pd.date_range(start=self.start_time,end=self.stop_time,freq='6MS')[1:-1] #indexing filters out startyear-01-01, endyear-01-01\n",
    "            predict_times = pd.date_range(self.predict_start_time,self.predict_stop_time,freq='6M')[1:-1] \n",
    "        elif self.window_freq=='year':\n",
    "            times = pd.date_range(start=self.start_time,end=self.stop_time,freq='AS-JUL')\n",
    "            predict_times = pd.date_range(self.predict_start_time,self.predict_stop_time,freq='AS-JUL')\n",
    "        else:\n",
    "            raise ValueError('')\n",
    "        \n",
    "        if self.window_size=='year':\n",
    "            times_lb = times - pd.DateOffset(months=6)\n",
    "            times_ub = times + pd.DateOffset(months=6)\n",
    "\n",
    "        #can use these to quickly filter df and get \n",
    "        self.window_times = times\n",
    "        self.window_times_lb = times_lb\n",
    "        self.window_times_ub = times_ub\n",
    "\n",
    "        self.predict_times = predict_times\n",
    "\n",
    "    def time_truncate_df(self,start='2017-01-01',end='2024-01-01'):\n",
    "\n",
    "        self.working_df = self.df[(self.df['date']>'2017-01-01') & (self.df['date']<'2024-01-01')]\n",
    "\n",
    "    def window_filter(self,window_time):\n",
    "\n",
    "        '''\n",
    "            Filter df based on window time and window size\n",
    "            NOTE: We're hard coding in year long dataframe window size now\n",
    "\n",
    "        \n",
    "        '''\n",
    "\n",
    "        filtering_condition = (self.working_df['date'] >= (window_time-pd.DateOffset(months=6))) & (self.working_df['date'] < (window_time+pd.DateOffset(months=6)))\n",
    "\n",
    "        return filtering_condition \n",
    "\n",
    "    def fit_distributions(self,fit_type,plot=False):\n",
    "        '''\n",
    "        May want to look at doing a fit to the rolling windows\n",
    "        \n",
    "        '''\n",
    "\n",
    "        FIT_TYPES = ['gaussian','gaussian mixture']\n",
    "        if fit_type not in FIT_TYPES:\n",
    "            raise ValueError(f'Invalid fit_type. Types: {FIT_TYPES}') \n",
    "        self.fit_type=fit_type\n",
    "\n",
    "        params = {t:None for t in self.window_times}\n",
    "\n",
    "\n",
    "        if fit_type=='gaussian':\n",
    "            \n",
    "            for t,t_lb,t_ub in list(zip(self.window_times,self.window_times_lb,self.window_times_ub)):\n",
    "                date_filt_condition = (self.working_df['date']>=t_lb) & (self.working_df['date'] < t_ub)\n",
    "                date_filt_df = self.working_df[date_filt_condition]\n",
    "                log_compute_data = date_filt_df['log_compute']\n",
    "\n",
    "                mean = log_compute_data.mean()\n",
    "                std = log_compute_data.mean()\n",
    "                params[t] = {'mean':mean,'std':std}\n",
    "\n",
    "                if plot: \n",
    "                    fig,ax=plt.subplots()\n",
    "                    plus_minus = \"\\u00B1\"\n",
    "                    sns.kdeplot(log_compute_data,label=f'timestamp: {t.date()} {plus_minus} 6mo ',linewidth=2,ax=ax)\n",
    "  \n",
    "                    mean = log_compute_data.mean()\n",
    "                    std = np.sqrt(log_compute_data.var()) #simple for now\n",
    "                    x=np.linspace(10,30,1000)\n",
    "                    ax.plot(x,norm.pdf(x,loc=mean,scale=std))\n",
    "                    ax.grid(); ax.legend(loc='upper left')\n",
    "\n",
    "        \n",
    "        if fit_type == 'gaussian mixture':\n",
    "\n",
    "            for t in self.window_times:\n",
    "                date_filt_condition = self.window_filter(window_time=t)\n",
    "                date_filt_df = self.working_df[date_filt_condition]\n",
    "                log_compute_data = date_filt_df['log_compute']\n",
    "\n",
    "                gmm = GaussianMixture(n_components=2,random_state=2)\n",
    "                gmm.fit(log_compute_data.to_numpy().reshape(-1,1))\n",
    "                means,covariances,weights = gmm.means_,gmm.covariances_,gmm.weights_\n",
    "                params[t] = {'means':means,\n",
    "                            'covars':covariances,\n",
    "                            'weights':weights}\n",
    "\n",
    "        \n",
    "        self.fitted_params = params\n",
    "\n",
    "        return params\n",
    "    \n",
    "    def extrapolate_distributions(self):\n",
    "\n",
    "        '''\n",
    "        NOTE: Gaussian mixture covar and weight is very hacky right now\n",
    "        '''\n",
    "        \n",
    "        if self.fit_type=='gaussian':\n",
    "            \n",
    "            #linear extrap means\n",
    "            fit_dates = [t for t in self.fitted_params.keys()]\n",
    "            fit_dates_float = np.array([t.timestamp() for t in fit_dates])\n",
    "            means = np.array([self.fitted_params[t]['mean'] for t in self.fitted_params.keys()])\n",
    "            predicted_dates_float = np.array([t.timestamp() for t in self.predict_times])\n",
    "\n",
    "\n",
    "            model=LinearRegression()\n",
    "            model.fit(fit_dates_float.reshape(-1,1),means)\n",
    "            predicted_means = model.predict(predicted_dates_float.reshape(-1,1))\n",
    "            retr_means = model.predict(fit_dates_float.reshape(-1,1))\n",
    "\n",
    "            #sample std\n",
    "            std_bounds = (1.1,1.6)\n",
    "            predicted_stds = np.random.uniform(low=std_bounds[0],high=std_bounds[1],size=(predicted_means.shape))\n",
    "            retr_stds = np.empty_like(retr_means); retr_stds.fill(np.nan) #not retrodicting params for now\n",
    "\n",
    "            predicted_params = {t:(mu,std) for t,mu,std in list(zip(self.predict_times,predicted_means,predicted_stds))}\n",
    "            retr_params = {t:(mu,std) for t,mu,std in list(zip(self.window_times,retr_means,retr_stds))}\n",
    "\n",
    "            self.distribution_parameter_model = model #not good atm code atm \n",
    "\n",
    "\n",
    "        elif self.fit_type=='gaussian mixture':\n",
    "\n",
    "            ##hacky parameterisation\n",
    "            lower_var_bounds = (0.5,1.4) #heuristic set (0.5,1.4)\n",
    "            upper_var_bounds = (0.5,1.4) #exluding ~3.5 var for upper dist in 2023. Heuristic set (0.5,1.2)\n",
    "            lower_weights_bound = (0.15,0.35) #heuristically set (0.15,0.35)\n",
    "\n",
    "            ##get data to fit\n",
    "            lower_idx = [self.fitted_params[t]['means'].argmin() for t in self.fitted_params.keys()]\n",
    "            higher_idx = [self.fitted_params[t]['means'].argmax() for t in self.fitted_params.keys()]\n",
    "\n",
    "            lower_dist_means = np.array([self.fitted_params[t]['means'][idx] for t,idx in list(zip(self.fitted_params.keys(),lower_idx))])\n",
    "            higher_dist_means = np.array([self.fitted_params[t]['means'][idx] for t,idx in list(zip(self.fitted_params.keys(),higher_idx))])\n",
    "\n",
    "            lower_dist_covars = np.array([self.fitted_params[t]['covars'][idx] for t,idx in list(zip(self.fitted_params.keys(),lower_idx))])\n",
    "            higher_dist_covars = np.array([self.fitted_params[t]['covars'][idx] for t,idx in list(zip(self.fitted_params.keys(),higher_idx))])\n",
    "            \n",
    "            lower_dist_weights = np.array([self.fitted_params[t]['weights'][idx] for t,idx in list(zip(self.fitted_params.keys(),lower_idx))])\n",
    "            higher_dist_weights = np.array([self.fitted_params[t]['weights'][idx] for t,idx in list(zip(self.fitted_params.keys(),higher_idx))])\n",
    "\n",
    "\n",
    "            fit_times_float = np.array([t.timestamp() for t in self.window_times])\n",
    "            predict_times_float = np.array([t.timestamp() for t in self.predict_times])\n",
    "\n",
    "\n",
    "\n",
    "            ##predict/retrodict means\n",
    "            lower_dist_mean_model = pwlf.PiecewiseLinFit(fit_times_float,lower_dist_means)\n",
    "            lower_dist_mean_model.fit(n_segments=2)\n",
    "            pred_lower_means = lower_dist_mean_model.predict(predict_times_float)\n",
    "            retr_lower_means = lower_dist_mean_model.predict(fit_times_float)\n",
    "\n",
    "            higher_dist_mean_model = pwlf.PiecewiseLinFit(fit_times_float,higher_dist_means)\n",
    "            higher_dist_mean_model.fit(n_segments=2)\n",
    "            pred_higher_means = higher_dist_mean_model.predict(predict_times_float)\n",
    "            retr_higher_means = higher_dist_mean_model.predict(fit_times_float)\n",
    "\n",
    "            ##extrapolate vars\n",
    "            pred_lower_vars = np.random.uniform(low=lower_var_bounds[0],high=lower_var_bounds[1],size=(pred_lower_means.shape))\n",
    "            pred_higher_vars = np.random.uniform(low=upper_var_bounds[0],high=upper_var_bounds[1],size=(pred_higher_means.shape))\n",
    "            retr_lower_vars = np.empty_like(retr_lower_means); retr_lower_vars.fill(np.nan)\n",
    "            retr_higher_vars = np.empty_like(retr_higher_means); retr_higher_vars.fill(np.nan)\n",
    "\n",
    "            ##extraplate weights \n",
    "            pred_lower_weights = np.random.uniform(low=lower_weights_bound[0],high=lower_weights_bound[1],size=(pred_lower_means.shape))\n",
    "            retr_lower_weights = np.empty_like(retr_lower_means); retr_lower_weights.fill(np.nan)\n",
    "\n",
    "            ##set predicted params state var\n",
    "            predicted_params = {t:((mu1,mu2),(var1,var2),(w1,w2)) for\n",
    "                                     t,mu1,mu2,var1,var2,w1,w2 in\n",
    "                                     list(zip(self.predict_times,\n",
    "                                              pred_lower_means,pred_higher_means,\n",
    "                                              pred_lower_vars,pred_higher_vars,\n",
    "                                              pred_lower_weights,1-pred_lower_weights))\n",
    "                                    }\n",
    "            \n",
    "            retr_params = {t:((mu1,mu2),(var1,var2),(w1,w2)) for\n",
    "                                        t,mu1,mu2,var1,var2,w1,w2 in\n",
    "                                        list(zip(self.window_times,\n",
    "                                                retr_lower_means,retr_higher_means,\n",
    "                                                retr_lower_vars,retr_higher_vars,\n",
    "                                                retr_lower_weights,1-retr_lower_weights))\n",
    "                                        }\n",
    "            \n",
    "\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        self.predicted_params = predicted_params\n",
    "        self.retrodicted_params = retr_params\n",
    "\n",
    "        return predicted_params\n",
    "    \n",
    "    def model_counts(self,counts_fit_type):\n",
    "\n",
    "\n",
    "        ## COULD add a kinked exponential\n",
    "        COUNT_FIT_TYPES=['linear','exponential','kinked linear']\n",
    "\n",
    "        if counts_fit_type not in COUNT_FIT_TYPES: raise ValueError(f'Expected fit in {COUNT_FIT_TYPES}')\n",
    "    \n",
    "        \n",
    "        #time_data is bad var name but leftover from old code\n",
    "        time_data = {t:{'size':None,\n",
    "                   } \n",
    "                   for t in self.window_times}\n",
    "        \n",
    "        for t,t_lb,t_ub in list(zip(self.window_times,self.window_times_lb,self.window_times_ub)):\n",
    "\n",
    "            if t_lb < pd.Timestamp(self.start_time) or t_ub > pd.Timestamp(self.stop_time): \n",
    "                print(f'Skipping {t} - window not in range')\n",
    "                time_data[t]['size']=None\n",
    "\n",
    "                continue\n",
    "            else:\n",
    "                date_filt_condition = (self.working_df['date']>=t_lb) & (self.working_df['date'] < t_ub)\n",
    "                date_tmp_df = self.working_df[date_filt_condition] #filtered df\n",
    "                time_data[t]['size']=date_tmp_df.shape[0]\n",
    "\n",
    "        #perform fitting\n",
    "    \n",
    "        fit_counts = np.array([t['size'] for t in time_data.values()])\n",
    "        fit_times_float = np.array([t.timestamp() for t in self.window_times])\n",
    "        predict_times_float = np.array([t.timestamp() for t in self.predict_times])\n",
    " \n",
    "        if counts_fit_type=='linear':\n",
    "            model = LinearRegression()\n",
    "            model.fit(fit_times_float.reshape(-1,1),fit_counts)\n",
    "            predicted_counts = model.predict(predict_times_float.reshape(-1,1))\n",
    "            retr_counts = model.predict(fit_times_float.reshape(-1,1)).astype('int')\n",
    "\n",
    "        elif counts_fit_type=='exponential':\n",
    "            transformed_fit_x = x_transform_for_exp_fit(ref_x=fit_times_float,x=fit_times_float)\n",
    "            popt,pcov = curve_fit(exponential_fit,transformed_fit_x,fit_counts)    \n",
    "            a,b = popt; model = popt\n",
    "            transformed_pred_x = x_transform_for_exp_fit(ref_x=fit_times_float,x=predict_times_float)\n",
    "            predicted_counts = exponential_fit(transformed_pred_x,a=a,b=b)\n",
    "            retr_counts = exponential_fit(transformed_fit_x,a=a,b=b)\n",
    "\n",
    "        elif counts_fit_type=='kinked linear':\n",
    "            model = pwlf.PiecewiseLinFit(fit_times_float,fit_counts)\n",
    "            breakpoints = model.fit(2)\n",
    "            predicted_counts = model.predict(predict_times_float)\n",
    "            retr_counts = model.predict(fit_times_float)\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        \n",
    "        #set state vars\n",
    "        self.fit_counts = fit_counts\n",
    "        self.predicted_counts = predicted_counts.astype('int')\n",
    "        self.count_fit_type = counts_fit_type\n",
    "        self.retr_counts = retr_counts\n",
    "        self.counts_model = model\n",
    "\n",
    "        return predicted_counts\n",
    "    \n",
    "    def count_threshold_models(self):\n",
    "\n",
    "        thresholds = np.arange(23,30+1)\n",
    "        threshold_counts_df = pd.DataFrame(columns=thresholds,index=self.predict_times)\n",
    "\n",
    "        #not doing rollouts yet\n",
    "        for pred_t,params,counts in list(zip(self.predict_times,self.predicted_params.values(),self.predicted_counts)):\n",
    "            \n",
    "            if self.fit_type == 'gaussian':\n",
    "                mu,sigma = params\n",
    "                log_compute_samples = norm.rvs(loc=mu,scale=sigma,size=counts)\n",
    "                for thr in thresholds:\n",
    "                    n_exceed = log_compute_samples[log_compute_samples>=thr].size\n",
    "                    threshold_counts_df.at[pred_t,thr] = n_exceed\n",
    "\n",
    "            if self.fit_type== 'gaussian mixture':\n",
    "                mus,vars,ws = params\n",
    "                mu_l,mu_h = mus\n",
    "                var_l,var_h = vars\n",
    "                w_l,w_h = ws \n",
    "                std_l,std_h = np.sqrt(var_l),np.sqrt(var_h)\n",
    "\n",
    "                log_compute_samples = sample_from_gmm(n_samples=counts,params=params)\n",
    "                for thr in thresholds:\n",
    "                    n_exceed = log_compute_samples[log_compute_samples>=thr].size\n",
    "                    threshold_counts_df.at[pred_t,thr] = n_exceed\n",
    "        \n",
    "\n",
    "        self.threshold_counts = threshold_counts_df\n",
    "        return threshold_counts_df\n",
    "    \n",
    "    def verify_with_retrodiction(self):\n",
    "\n",
    "        '''\n",
    "        Params:\n",
    "            n_years_retr: Retrodict n years back\n",
    "\n",
    "        Return:\n",
    "\n",
    "        Notes:\n",
    "            - Don't think this is adapted for rolling windows yet (?)\n",
    "            - A lot of this isn't 'true' retrodiction - we use some fitted/observed values\n",
    "        '''\n",
    "\n",
    "        retrodict_times = self.window_times #retrodict for fitted time stamps\n",
    "        retrodict_times_float = np.array([t.timestamp() for t in retrodict_times])\n",
    "        retr_counts = self.retr_counts\n",
    "\n",
    "\n",
    "        thresholds = [23,24]\n",
    "        predicted_past_counts = pd.DataFrame(index=retrodict_times,columns=thresholds)\n",
    "        observed_past_counts = pd.DataFrame(index=retrodict_times,columns=thresholds)\n",
    "        percent_error_df = pd.DataFrame(np.nan,index=retrodict_times,columns=thresholds)\n",
    "\n",
    "\n",
    "\n",
    "        #pretty inefficient way to do it right now\n",
    "        for idx,t in enumerate(retrodict_times):\n",
    "\n",
    "            ##generate distributions and counts\n",
    "            count = int(retr_counts[idx])\n",
    "\n",
    "            if self.fit_type=='gaussian':\n",
    "                mean = self.retrodicted_params[t][0]\n",
    "                std = self.retrodicted_params[t][1]\n",
    "\n",
    "                if np.isnan(std): std = self.working_df[self.window_filter(t)]['log_compute'].std() #USE EMPIRICAL STDS\n",
    "\n",
    "                pred_log_compute_data = norm.rvs(loc=mean,scale=std,size=count)\n",
    "\n",
    "            elif self.fit_type=='gaussian mixture':\n",
    "            \n",
    "                #unpack retrodicted params\n",
    "                mus,vars,ws = self.retrodicted_params[t]\n",
    "                mu_l,mu_h = mus\n",
    "                var_l,var_h = vars\n",
    "                w_l,w_h = ws\n",
    " \n",
    "                if np.isnan(var_l): var_l = self.fitted_params[t]['covars'][0] #use fitted vars\n",
    "                if np.isnan(var_h): var_h = self.fitted_params[t]['covars'][1] #use fitted vars\n",
    "                if np.isnan(w_l): w_l = self.fitted_params[t]['weights'][0] #use fitted gmm weights\n",
    "                if np.isnan(w_h): w_h = 1-w_l #use fitted gmm weights\n",
    "\n",
    "                w_l,w_h = np.array([w_l]), np.array([w_h]) #for type compatibility with other parts of workflow\n",
    "\n",
    "                params_retr = ((mu_l,mu_h),(var_l,var_h),(w_l,w_h))\n",
    "                pred_log_compute_data = sample_from_gmm(n_samples=count,params=params_retr)\n",
    "\n",
    "            ##get obs log compute data\n",
    "            obs_log_compute_data = self.working_df[self.window_filter(t)]['log_compute']\n",
    "\n",
    "            ##do threshold counts\n",
    "            for thr in thresholds:\n",
    "                #pred\n",
    "                thr_count_pr = pred_log_compute_data[pred_log_compute_data>=thr].size\n",
    "                predicted_past_counts.at[t,thr] = thr_count_pr\n",
    "\n",
    "                #obs\n",
    "                thr_count_ob = obs_log_compute_data[obs_log_compute_data>=thr].size\n",
    "                observed_past_counts.at[t,thr] = thr_count_ob\n",
    "\n",
    "        abs_diff = np.abs(observed_past_counts-predicted_past_counts)\n",
    "        obs_df_safe = observed_past_counts.replace(0,np.nan) #for safe division\n",
    "        percent_error_df = (abs_diff/obs_df_safe)*100\n",
    "             \n",
    "             \n",
    "        self.predicted_past_counts = predicted_past_counts\n",
    "        self.observed_past_counts = observed_past_counts\n",
    "\n",
    "        return predicted_past_counts,observed_past_counts,percent_error_df\n",
    "\n",
    "    def verify_with_training_compute(self):\n",
    "         \n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = df[(df['date']>'2017-01-01') & (df['date']<'2024-01-01')]\n",
    "tmp_df['date'] = pd.to_datetime(tmp_df['date'])\n",
    "\n",
    "window_freq = 'year' \n",
    "dist_fit='gaussian mixture'\n",
    "count_fit='linear'\n",
    "\n",
    "analysis = DataAnalysis(df=tmp_df,window_freq=window_freq)\n",
    "analysis.time_truncate_df()\n",
    "params = analysis.fit_distributions(fit_type=dist_fit,plot=False)\n",
    "predicted_params = analysis.extrapolate_distributions()\n",
    "predicted_counts = analysis.model_counts(counts_fit_type=count_fit)\n",
    "threshold_counts = analysis.count_threshold_models()\n",
    "pred_past_counts,obs_past_counts,percent_error_df = analysis.verify_with_retrodiction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools, warnings\n",
    "\n",
    "distribution_fits = ['gaussian','gaussian mixture']\n",
    "model_count_fits = ['linear','exponential','kinked linear']\n",
    "window_freq = 'year'\n",
    "dist_fit = 'gaussian mixture'\n",
    "count_fit = 'linear'\n",
    "\n",
    "\n",
    "\n",
    "for (dist_fit,count_fit) in list(itertools.product(distribution_fits,model_count_fits)):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        analysis = DataAnalysis(df=tmp_df,window_freq=window_freq)\n",
    "        analysis.time_truncate_df()\n",
    "        _ = analysis.fit_distributions(fit_type=dist_fit,plot=False)\n",
    "        _ = analysis.extrapolate_distributions() \n",
    "        _ = analysis.model_counts(counts_fit_type=count_fit)\n",
    "        threshold_counts = analysis.count_threshold_models()\n",
    "        pred_past_counts,obs_past_counts,percent_error_df = analysis.verify_with_retrodiction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore gmms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = df[(df['date']>'2017-01-01') & (df['date']<'2024-01-01')]\n",
    "tmp_df['date'] = pd.to_datetime(tmp_df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture; random_seed=42\n",
    "\n",
    "years = tmp_df['year'].unique()\n",
    "\n",
    "years_choice = [2018,2019,2020,2021,2022,2023]\n",
    "nrows,ncols = (3,2)\n",
    "\n",
    "INDIVIDUAL_PLOTS = True\n",
    "GMM_FIT = True\n",
    "GMM_params = {year:{} for year in years_choice}\n",
    "\n",
    "if INDIVIDUAL_PLOTS:\n",
    "    n_plots = len(years_choice)\n",
    "    fig,axs = plt.subplots(ncols=ncols,nrows=nrows,figsize=(8,6))\n",
    "    axs = axs.ravel()\n",
    "    for ax in axs: ax.grid();ax.legend()\n",
    "    for idx,year in enumerate(years_choice):\n",
    "        ax = axs[idx]\n",
    "        log_compute_data = tmp_df[tmp_df['year']==year]['log_compute']\n",
    "        sns.kdeplot(log_compute_data,ax=ax,alpha=0.5,label=f'{year} KDE')\n",
    "\n",
    "        if GMM_FIT:\n",
    "            gmm = GaussianMixture(n_components=2,random_state=random_seed)\n",
    "            gmm.fit(log_compute_data.to_numpy().reshape(-1,1))\n",
    "            means,covariances,weights = gmm.means_,gmm.covariances_,gmm.weights_\n",
    "            x = np.linspace(14,28,1000)\n",
    "            ax.plot(x,np.exp(gmm.score_samples(x.reshape(-1,1))),label=f'gmm fit, n = {len(log_compute_data)}') #gmm calculates log probs\n",
    "            GMM_params[year]['means'] = means\n",
    "            GMM_params[year]['covariances'] = covariances\n",
    "            GMM_params[year]['weights'] = weights\n",
    "\n",
    "        ax.set_ylim([0,0.4])\n",
    "        ax.legend()\n",
    "\n",
    "else: \n",
    "    fig,ax=plt.subplots()\n",
    "    for idx,year in enumerate(years):\n",
    "        if year not in years_choice:continue\n",
    "        sns.kdeplot(tmp_df[tmp_df['year']==year]['log_compute'],ax=ax,alpha=0.5,label=year)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.set_size_inches(w=8,h=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##visualise fitted params\n",
    "\n",
    "fig,(ax1,ax2,ax3) = plt.subplots(ncols=3)\n",
    "\n",
    "low_idx = [GMM_params[year]['means'].argmin() for year in years_choice] #get the idx for the lower dist (by mean)\n",
    "high_idx = [GMM_params[year]['means'].argmax() for year in years_choice] #get the idx for the upper dist (by mean)\n",
    "\n",
    "lower_dist_means = [GMM_params[year]['means'][idx] for year,idx in list(zip(years_choice,low_idx))]\n",
    "upper_dist_means = [GMM_params[year]['means'][idx] for year,idx in  list(zip(years_choice,high_idx))]\n",
    "\n",
    "lower_dist_vars = [GMM_params[year]['covariances'][idx] for year,idx in list(zip(years_choice,low_idx))]\n",
    "upper_dist_vars = [GMM_params[year]['covariances'][idx] for year,idx in  list(zip(years_choice,high_idx))]\n",
    "\n",
    "lower_dist_weight = [GMM_params[year]['weights'][idx] for year,idx in list(zip(years_choice,low_idx))]\n",
    "upper_dist_weight = [GMM_params[year]['weights'][idx] for year,idx in list(zip(years_choice,high_idx))]\n",
    "\n",
    "plt.rcParams['scatter.marker'] = 'x'\n",
    "\n",
    "if 1:\n",
    "    ax1.set_title('means')\n",
    "    ax1.scatter(years_choice,lower_dist_means,c='b',label='lower dist')\n",
    "    ax1.scatter(years_choice,upper_dist_means,c='r',label='upper dist')\n",
    "    ax1.grid(); ax1.legend()\n",
    "\n",
    "    ax2.set_title('vars')\n",
    "    ax2.scatter(years_choice,lower_dist_vars,c='b',label='low dist')\n",
    "    ax2.scatter(years_choice,upper_dist_vars,c='r',label='upper dist')\n",
    "    ax2.grid(); ax2.legend()\n",
    "    ax2.set_ylim([0.5,1.5])\n",
    "\n",
    "    ax3.set_title('weights')\n",
    "    ax3.scatter(years_choice,lower_dist_weight,c='b',label='low dist')\n",
    "    #ax3.scatter(years_choice,upper_dist_weight,c='r',label='upper dist')\n",
    "    ax3.grid(); ax3.legend()\n",
    "    ax3.set_ylim([0,1.0])\n",
    "    #lower std bound: [0.5,1.4]\n",
    "    #upper std bound: [0.5,1.2]\n",
    "\n",
    "    fig.set_size_inches(w=10,h=6)\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extrapolate gmms and visualise\n",
    "\n",
    "np.random.seed()\n",
    "\n",
    "import pwlf\n",
    "\n",
    "PLOT_EXTRAPOLATED_PARAMS = True\n",
    "\n",
    "fit_years = years_choice\n",
    "predict_years = np.arange(2024,2029+1) #extrapolate\n",
    "\n",
    "lower_var_bounds = (0.5,1.4) #heuristic set (0.5,1.4)\n",
    "upper_var_bounds = (0.5,1.4) #exluding ~3.5 var for upper dist in 2023. Heuristic set (0.5,1.2)\n",
    "lower_weights_bound = (0.15,0.35) # vibes based\n",
    "lower_weights_bound_2 = (0.5,0.5) #heuristic set (0.25,0.15)\n",
    "\n",
    "#lower dist mean model \n",
    "fit_data =  np.array(lower_dist_means)\n",
    "lower_dist_mean_model = pwlf.PiecewiseLinFit(fit_years,fit_data)\n",
    "lower_dist_mean_model.fit(n_segments=2)\n",
    "pred_lower_means = lower_dist_mean_model.predict(predict_years)\n",
    "pred_lower_vars = np.random.uniform(low=lower_var_bounds[0],high=lower_var_bounds[1],size=len(predict_years))\n",
    "pred_lower_weights = np.random.uniform(low=lower_weights_bound[0],high=lower_weights_bound[1],size=len(predict_years))\n",
    "pred_lower_weights_2 = np.linspace(lower_weights_bound_2[0],lower_weights_bound_2[1],num=len(predict_years))\n",
    "\n",
    "\n",
    "#upper dist mean model\n",
    "fit_data = np.array(upper_dist_means)\n",
    "upper_dist_mean_model = pwlf.PiecewiseLinFit(fit_years,fit_data)\n",
    "upper_dist_mean_model.fit(n_segments=2)\n",
    "pred_upper_means = upper_dist_mean_model.predict(predict_years)\n",
    "pred_upper_vars = np.random.uniform(low=upper_var_bounds[0],high=upper_var_bounds[1],size=len(predict_years))\n",
    "pred_upper_weights = 1 - pred_lower_weights\n",
    "pred_upper_weights_2 = 1 - pred_lower_weights_2\n",
    "\n",
    "\n",
    "\n",
    "if PLOT_EXTRAPOLATED_PARAMS:\n",
    "    extrap_marker='o'\n",
    "    fit_marker='x'\n",
    "    low_color='b' \n",
    "    upper_color='r'\n",
    "\n",
    "    fig,axs = plt.subplots(nrows=1,ncols=3)\n",
    "    for ax in axs: \n",
    "        ax.grid()\n",
    "        ax.set_xticklabels(np.concatenate([fit_years,predict_years]),rotation=45)\n",
    "        ax.set_xticks(np.concatenate([fit_years,predict_years]))\n",
    "    ax1,ax2,ax3=axs\n",
    "    \n",
    "\n",
    "    #means\n",
    "\n",
    "    # Plotting the extrapolated parameters\n",
    "    ax1.scatter(fit_years, lower_dist_means, marker=fit_marker, c=low_color)\n",
    "    ax1.scatter(predict_years, pred_lower_means, marker=extrap_marker, c=low_color)\n",
    "\n",
    "    ax1.scatter(fit_years, upper_dist_means, marker=fit_marker, c=upper_color)\n",
    "    ax1.scatter(predict_years, pred_upper_means, marker=extrap_marker, c=upper_color)\n",
    "\n",
    "    # Standard deviations\n",
    "    ax2.scatter(fit_years, np.sqrt(lower_dist_vars), marker=fit_marker, c=low_color)\n",
    "    ax2.scatter(predict_years, np.sqrt(pred_lower_vars), marker=extrap_marker, c=low_color)\n",
    "\n",
    "    ax2.scatter(fit_years, np.sqrt(upper_dist_vars), marker=fit_marker, c=upper_color)\n",
    "    ax2.scatter(predict_years, np.sqrt(pred_upper_vars), marker=extrap_marker, c=upper_color)\n",
    "\n",
    "    # Weights\n",
    "    ax3.scatter(fit_years, lower_dist_weight, marker=fit_marker, c=low_color)\n",
    "    ax3.scatter(predict_years, pred_lower_weights, marker=extrap_marker, c=low_color)\n",
    "\n",
    "    ax3.scatter(fit_years, upper_dist_weight, marker=fit_marker, c=upper_color)\n",
    "    ax3.scatter(predict_years, pred_upper_weights, marker=extrap_marker, c=upper_color)\n",
    "\n",
    "    #std \n",
    "    fig.tight_layout()\n",
    "    fig.set_size_inches(w=10,h=4)\n",
    "\n",
    "\n",
    "x=np.linspace(14,35,1000)\n",
    "\n",
    "nrows,ncols=(3,2)\n",
    "fig,axs=plt.subplots(nrows=nrows,ncols=ncols)\n",
    "axs = axs.ravel()\n",
    "\n",
    "for idx,(year,mu_l,mu_u,var_l,var_u,w_l,w_u) in enumerate(list(zip(\n",
    "    predict_years,\n",
    "    pred_lower_means,pred_upper_means,\n",
    "    pred_lower_vars,pred_upper_vars,\n",
    "    pred_lower_weights_2,pred_upper_weights_2,\n",
    "))):\n",
    "\n",
    "    ax=axs[idx]\n",
    "    std_l,std_u = np.sqrt(var_l),np.sqrt(var_u)\n",
    "    pdf = w_l*norm.pdf(x,loc=mu_l,scale=std_l) + w_u*norm.pdf(x,loc=mu_u,scale=std_u)\n",
    "    ax.plot(x,pdf,label=f'{year}')\n",
    "    ax.grid();ax.legend()\n",
    "        \n",
    "\n",
    "fig.tight_layout()\n",
    "fig.set_size_inches(w=6,h=8)\n",
    "\n",
    "#lower dist std devs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sci_comp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
