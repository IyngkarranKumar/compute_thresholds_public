{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "all_systems_csv_path = \"/Users/iyngkarrankumar/Documents/Misc/Tracking models/data/all_systems.csv\"\n",
    "notable_models_csv_path = \"data/notable_ai_models.csv\"\n",
    "large_scale_models_csv_path = \"data/large_scale_ai_models.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def half_year_bin(date):\n",
    "    #CHATGPT generated\n",
    "    if date.month <= 6:\n",
    "        return f'{date.year}-H1'\n",
    "    else: \n",
    "        return f'{date.year}-H2'\n",
    "\n",
    "def year_bin(date):\n",
    "    return date.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring large-scale model distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "all_systems_csv_path = \"/Users/iyngkarrankumar/Documents/Misc/Tracking models/data/all_systems.csv\"\n",
    "notable_models_csv_path = \"data/notable_ai_models.csv\"\n",
    "large_scale_models_csv_path = \"data/large_scale_ai_models.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LARGE_SCALE_DATA = pd.read_csv(large_scale_models_csv_path)\n",
    "\n",
    "LARGE_SCALE_DATA['Publication date'] = pd.to_datetime(LARGE_SCALE_DATA['Publication date'])\n",
    "start_year = 2017\n",
    "LARGE_SCALE_DATA = LARGE_SCALE_DATA[LARGE_SCALE_DATA['Publication date'] > f'{start_year}-01-01']\n",
    "\n",
    "#binning\n",
    "LARGE_SCALE_DATA['Publication bin'] = LARGE_SCALE_DATA['Publication date'].apply(year_bin)\n",
    "\n",
    "COMPUTE_DATA = LARGE_SCALE_DATA.dropna(subset=['Training compute (FLOP)'])\n",
    "COMPUTE_DATA['log Training compute'] = np.log10(COMPUTE_DATA['Training compute (FLOP)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOUBLE_2024 = True\n",
    "\n",
    "years = list(reversed(COMPUTE_DATA['Publication bin'].unique()))\n",
    "years.insert(1,2018)\n",
    "years.insert(2,2019)\n",
    "\n",
    "fig,axs = plt.subplots(nrows=len(years),ncols=1,figsize=(7,11),sharex=True)\n",
    "bin_range = (22,27)\n",
    "bins=np.arange(bin_range[0],bin_range[-1],1)\n",
    "\n",
    "HISTOGRAM_DATA_DF = pd.DataFrame(columns=years)\n",
    "for bin in bins[:-1]:\n",
    "    HISTOGRAM_DATA_DF.loc[f'{bin}-{bin+1}'] = [np.nan]*len(years)\n",
    "\n",
    "for idx,year in enumerate(years):\n",
    "    ax = axs[idx]\n",
    "    filtered_df = COMPUTE_DATA[COMPUTE_DATA['Publication bin']==year] #year df\n",
    "    \n",
    "    if year==2024 and DOUBLE_2024: filtered_df = pd.concat([filtered_df,filtered_df],ignore_index=True)\n",
    "\n",
    "    #histogram data\n",
    "    bin_count = np.histogram(filtered_df['log Training compute'],bins=bins,range=bin_range)[0]\n",
    "    for idx,bin in enumerate(bins[:-1]): #-1 index to sort out indexing problems\n",
    "        HISTOGRAM_DATA_DF.loc[f'{bin}-{bin+1}',year] = int(bin_count[idx])\n",
    "\n",
    "    #plot\n",
    "    filtered_df['log Training compute'].plot(kind='hist',bins=bins,range=bin_range,edgecolor='black',ax=ax)\n",
    "    ax.set_xlabel('');ax.set_ylabel('')\n",
    "    ax.set_xlim(bin_range)\n",
    "    ax.tick_params(axis='y',labelsize=12)\n",
    "    #if idx==0:     ax.yaxis.set_major_locator(mpl.ticker.MaxNLocator(integer=True))\n",
    "\n",
    "    ax.set_title(f'Year {year}, n={len(filtered_df)}',fontsize=15)\n",
    "    ax.grid(alpha=0.5)\n",
    "    ax.set_ylim([0,40])\n",
    "\n",
    "\n",
    "\n",
    "fig.text(0.5, -0.04, 'Log Compute ($10^X$)', ha='center', fontsize=15)\n",
    "fig.text(-0.04, 0.5, 'Frequency', va='center', rotation='vertical', fontsize=15)\n",
    "plt.xticks(bins,fontsize=15)\n",
    "plt.subplots_adjust(hspace=10)\n",
    "plt.tight_layout(rect=[0.04, 0.04, 1, 1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting gamma distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIT_NORMED = True #this gives way better fits than fitting the unnormalised data\n",
    "PLOT_PROBS = False\n",
    "MANUAL_FIT = True\n",
    "MANUAL_PARAMS = { \n",
    "    'params_2022':(0.5,0,0.15),\n",
    "    'params_2023':(0.35,0,0.20),\n",
    "    'params_2024':(0.25,0,0.35),\n",
    "}\n",
    "#k=1 yields exponential distribution\n",
    "stt=0.2\n",
    "\n",
    "''' Good set of manual params:\n",
    "MANUAL_PARAMS = {\n",
    "    'params_2022':(0.5,0,0.15),\n",
    "    'params_2023':(0.35,0,0.20),\n",
    "    'params_2024':(0.25,0,0.35),\n",
    "}\n",
    "\n",
    "others:\n",
    "{ \n",
    "    'params_2022':(1,0,0.025),\n",
    "    'params_2023':(1,0,0.030),\n",
    "    'params_2024':(1,0,0.025),\n",
    "}\n",
    "'''\n",
    "\n",
    "years_to_fit = [2022,2023,2024] #just from eyeballing plots for now\n",
    "NORMED_FIT_DATA = [] #to store normed fit data for later plotting\n",
    "NORM_FACTORS = []\n",
    "ALPHA = [] #to store shape parameter\n",
    "THETA = [] #to store scale parameter\n",
    "const = 1e-4\n",
    "\n",
    "'''\n",
    "manual fitting for 2022:\n",
    "    alpha = 0.5, theta = 0.15\n",
    "'''\n",
    "\n",
    "\n",
    "for year in years_to_fit:\n",
    "\n",
    "    #adding values\n",
    "    fit_data = HISTOGRAM_DATA_DF[year]\n",
    "    fit_data = fit_data.drop('22-23')\n",
    "    new_data = pd.Series([0,0,0],index=['26-27','27-28','28-29'])\n",
    "    fit_data = pd.concat([fit_data,new_data])\n",
    "\n",
    "    #data structure processing\n",
    "    fit_data_arr = np.array(fit_data) + const\n",
    "    norm_factor = fit_data_arr.sum(); NORM_FACTORS.append(norm_factor)\n",
    "    normed_fit_data = (fit_data_arr/norm_factor)+const #const to prevent fit errors\n",
    "    NORMED_FIT_DATA.append(normed_fit_data)\n",
    "\n",
    "    if FIT_NORMED: #seems to produce a way better fit\n",
    "        shape,_,scale = stats.gamma.fit(normed_fit_data,floc=0) \n",
    "    else: \n",
    "        shape,_,scale = stats.gamma.fit(fit_data_arr,floc=0) \n",
    "\n",
    "    ALPHA.append(shape)\n",
    "    THETA.append(scale)\n",
    "\n",
    "x_labels = fit_data.index\n",
    "\n",
    "\n",
    "if MANUAL_FIT:\n",
    "    params_2022 = MANUAL_PARAMS['params_2022']\n",
    "    params_2023 = MANUAL_PARAMS['params_2023']\n",
    "    params_2024 = MANUAL_PARAMS['params_2024']\n",
    "else: \n",
    "    params_2022 = [ALPHA[0],0,THETA[0]]\n",
    "    params_2023 = [ALPHA[1],0,THETA[1]]\n",
    "    params_2024 = [ALPHA[2],0,THETA[2]]\n",
    "PARAMS = [params_2022,params_2023,params_2024]\n",
    "\n",
    "\n",
    "fig,axs=plt.subplots(ncols=3,figsize=(12,6),sharey=True)\n",
    "for idx,ax in enumerate(axs):\n",
    "    #ax.hist(normed_fit_data)\n",
    "\n",
    "    x=np.linspace(stt,stt+1,len(fit_data)) #don't start at 0 to avoid infs\n",
    "    ax.set_xticks(x)\n",
    "\n",
    "\n",
    "\n",
    "    if PLOT_PROBS:\n",
    "        ax.bar(x,NORMED_FIT_DATA[idx],width=0.1,alpha=0.5,color='g',label='Data') #plot data\n",
    "\n",
    "\n",
    "        fitted_pdf = stats.gamma.pdf(x,*PARAMS[idx])\n",
    "\n",
    "        ax.plot(x,fitted_pdf,'r-',lw=2,label='Fitted gamma distribution')\n",
    "\n",
    "        ax.set_xticklabels(x_labels,rotation=45)\n",
    "        if idx==1: ax.legend()\n",
    "        if idx==0: ax.set_ylabel('Probability')\n",
    "\n",
    "    else: \n",
    "        ax.bar(x,NORM_FACTORS[idx]*NORMED_FIT_DATA[idx],width=0.1,alpha=0.5,color='g',label='Data') #plot data\n",
    "\n",
    "        fitted_pdf = stats.gamma.pdf(x,*PARAMS[idx])\n",
    "        ax.plot(x,NORM_FACTORS[idx]*fitted_pdf,'r-',lw=2,label='Fitted gamma distribution')\n",
    "\n",
    "        ax.set_xticklabels(x_labels,rotation=45)\n",
    "        if idx==1: ax.legend()\n",
    "        if idx==0: ax.set_ylabel('Frequency')\n",
    "\n",
    "    ax.set_title(f'Year: {years_to_fit[idx]}')\n",
    "\n",
    "fig.suptitle('Gamma distribution frequency predictions')\n",
    "\n",
    "\n",
    "\n",
    "ALPHA_FIT = [p[0] for p in PARAMS]\n",
    "THETA_FIT = [p[-1] for p in PARAMS]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "MODEL_COUNTS_FIT = 'linear'\n",
    "\n",
    "x_ = (np.arange(0,len(years_to_fit))).reshape(-1,1)\n",
    "extrap_start_year = 2025\n",
    "extrap_end_year = 2029\n",
    "future_years = np.arange(extrap_start_year,extrap_end_year+1)\n",
    "MAPPED_future_years = (future_years - 2022).reshape(-1,1) #we set year 2022 as x_ = 0\n",
    "\n",
    "\n",
    "#model counts\n",
    "#COULD fit data from 2020 - 2024 for model counts\n",
    "model_counts = [int(elem) for elem in NORM_FACTORS]\n",
    "if MODEL_COUNTS_FIT=='exponential':\n",
    "        def exponential_model(x,a,b,c):\n",
    "            return a*np.exp(b*x) + c\n",
    "        p0 = (1,0,1)\n",
    "        popt,pcov = curve_fit(exponential_model,x_.flatten(),np.array(model_counts))\n",
    "        predicted_counts = exponential_model(MAPPED_future_years,*popt)\n",
    "elif MODEL_COUNTS_FIT=='linear':\n",
    "    lin_counts_model = LinearRegression()\n",
    "    lin_counts_model.fit(x_,model_counts)\n",
    "    predicted_counts = (lin_counts_model.predict(MAPPED_future_years)).astype('int')\n",
    "else: raise Exception(f'Unknown fit entered: {MODEL_COUNTS_FIT}')\n",
    "\n",
    "##Parameter extrapolation\n",
    "#we're fixing alpha and going to assume linear regression for beta\n",
    "\n",
    "def exp_func(x,a,b):\n",
    "    return a*np.exp(-b*x)\n",
    "popt,_ = curve_fit(exp_func,x_.flatten(),ALPHA_FIT)\n",
    "PRED_ALPHA = exp_func(MAPPED_future_years.flatten(),*popt)\n",
    "\n",
    "\n",
    "theta_linear_model = LinearRegression()\n",
    "theta_linear_model.fit(x_,THETA_FIT)\n",
    "PRED_THETA = theta_linear_model.predict(MAPPED_future_years)\n",
    "\n",
    "\n",
    "if 1:\n",
    "    #getting predictions for future years\n",
    "    PREDICTED_HISTOGRAM_DATA_DF = pd.DataFrame(index=x_labels,columns=future_years)\n",
    "    #PREDICTED_HISTOGRAM_DATA_DF.set_index(pd.Index(x_labels),inplace=True)\n",
    "\n",
    "    for idx,year in enumerate(future_years):\n",
    "        pdf_predictions = stats.gamma.pdf(x,np.mean(ALPHA_FIT),loc=0,scale=PRED_THETA[idx]) #\n",
    "        frequency_predictions = predicted_counts[idx]*pdf_predictions\n",
    "        PREDICTED_HISTOGRAM_DATA_DF[year] = frequency_predictions.astype('int')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting truncated exponentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncated_exponential_log_likelihood(data,lambda_,t):\n",
    "    log_pdf = np.log(lambda_) - lambda_*data\n",
    "    cdf_value = 1-np.exp(-lambda_* t)\n",
    "    log_likelihood = np.sum(log_pdf) - len(data)*np.log(cdf_value)\n",
    "\n",
    "    return -log_likelihood #we seek to minimise negative LL\n",
    "\n",
    "def fit_truncated_exponential(data,threshold,lambda_0=1):\n",
    "    #lambda 0 is initial guess\n",
    "    return None\n",
    "    \n",
    "\n",
    "def test_fit():\n",
    "    #generate some sample data and check if the optimisation finds the right parameters\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years_to_fit = [2022,2023,2024]\n",
    "frontier_bins = [24,25,25] #number is bin lower bound. So no probability mass assigned to bin > (item+1)\n",
    "\n",
    "start_year = 2025\n",
    "end_year = 2029\n",
    "future_years = np.arange(start_year,end_year+1,1)\n",
    "future_frontier_bins = [25,26,26,27,27] #just fixing for now\n",
    "assert len(future_years)==len(future_frontier_bins)\n",
    "\n",
    "NORMED_FIT_DATA = []\n",
    "NORM_FACTORS = []\n",
    "LAMBDA = [] #exponential distribution rate parameter\n",
    "const = 1e-4\n",
    "\n",
    "for year in years_to_fit: \n",
    "    #adding values\n",
    "    fit_data = HISTOGRAM_DATA_DF[year]\n",
    "    fit_data = fit_data.drop('22-23')\n",
    "    new_data = pd.Series([0,0,0],index=['26-27','27-28','28-29'])\n",
    "    fit_data = pd.concat([fit_data,new_data])\n",
    "\n",
    "    #data structure processing\n",
    "    fit_data_arr = np.array(fit_data) + const\n",
    "    norm_factor = fit_data_arr.sum(); NORM_FACTORS.append(norm_factor)\n",
    "    normed_fit_data = (fit_data_arr/norm_factor)+const #const to prevent fit errors\n",
    "    NORMED_FIT_DATA.append(normed_fit_data)\n",
    "\n",
    "    #fit truncated exponential\n",
    "    lambda_ =  fit_truncated_exponential(normed_fit_data)\n",
    "    LAMBDA.append(lambda_)\n",
    "\n",
    "#PLOTTING\n",
    "x_labels = fit_data.index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('FTM')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f33f59546c507a35a4881afce9503208f6c8f0e8d914c07bf4768a8e3992010"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
