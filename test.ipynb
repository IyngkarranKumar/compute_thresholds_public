{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import scipy.optimize as optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy example for exponential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLL_from_data(lambda_,data):\n",
    "    log_pdf = np.log(lambda_) - lambda_*data\n",
    "    log_likelihood = np.sum(log_pdf)\n",
    "    return -log_likelihood\n",
    "\n",
    "\n",
    "def NLL_from_pdf(pdf):\n",
    "    log_pdf = np.log(pdf)\n",
    "    log_likelihood = np.sum(log_pdf)\n",
    "    return -log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "lambda_ = 3.0\n",
    "size=100\n",
    "data = stats.expon.rvs(scale=1/lambda_,size=size) #draw 'size' datapoints from expon dist with given parameter\n",
    "pdf = stats.expon.pdf(data,scale=1/lambda_) #calculate probs of the datapoints\n",
    "\n",
    "print(NLL_from_data(lambda_=lambda_,data=data),NLL_from_pdf(pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#data gives the arguments to be fixed\n",
    "#minimise call expects that the first argument is one to be optimised!\n",
    "p0=[1.0]\n",
    "bounds = [(1e-6,None)]\n",
    "options = {'maxiter':1000}\n",
    "est_1 = (optimize.minimize(NLL_from_data,x0=p0,args=(data,),method='L-BFGS-B',bounds=bounds,options=options)).x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truncated exponential fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#taken from 'normed fit data' in other file\n",
    "DATA = [np.array([8.94813851e-01, 1.05365097e-01, 0, 0,\n",
    "        0, 0]),\n",
    " np.array([7.14377891e-01, 2.14385034e-01, 7.15299320e-02, 0,\n",
    "        0, 0]),\n",
    " np.array([6.55267360e-01, 2.06996136e-01, 1.38031332e-01, 0,\n",
    "        0, 0])]\n",
    "\n",
    "FLOP_bins = ['23-24', '24-25', '25-26', '26-27', '27-28', '28-29']\n",
    "mapped_FLOP_bins = np.arange(1,len(FLOP_bins)+1) #this x range is quite arbitrary\n",
    "assert len(FLOP_bins)==len(mapped_FLOP_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discret_exp_dist(lambda_,start=0,end=1,bin_width=0.1):\n",
    "    n_bins = int(((end-start)/bin_width)) #number of values in pmf\n",
    "    res=n_bins*100 #100 pdf values for each bin\n",
    "    dx = (end-start)/res\n",
    "    data = np.linspace(start,end,res)\n",
    "    pdf = stats.expon.pdf(data,scale=1/lambda_)\n",
    "    pmf = np.zeros(n_bins)\n",
    "\n",
    "    for i in range(n_bins):\n",
    "        stt_idx = int((i*res)/n_bins)\n",
    "        end_idx = int((((i+1)/n_bins)*res))\n",
    "        prob_mass = np.sum(pdf[stt_idx:end_idx]*dx) #integrate\n",
    "        pmf[i]=prob_mass\n",
    "\n",
    "    return pmf\n",
    "\n",
    "\n",
    "def truncated_exp_dist_pdf(lambda_,t,start=0,stop=1,size=100):\n",
    "    '''\n",
    "    Generate a truncated exponential distribution (continous)\n",
    "    '''\n",
    "    data = np.linspace(start=start,stop=stop,num=size)\n",
    "    threshold_idx = int((t/(stop-start))*size)\n",
    "    data_t = data[:threshold_idx]\n",
    "\n",
    "    expon_pdf_t = stats.expon.pdf(x=data_t,scale=1/lambda_) #pdf from start -> threshold\n",
    "    norm_factor = stats.expon.cdf(t,scale=1/lambda_) - stats.expon.cdf(start,scale=1/lambda_)\n",
    "    truncated_pdf = expon_pdf_t/norm_factor\n",
    "    \n",
    "    rem_pdf = np.zeros(len(data)-len(data_t))\n",
    "\n",
    "    full_pdf = np.concatenate([truncated_pdf,rem_pdf])\n",
    "\n",
    "    return data,full_pdf\n",
    "\n",
    "def truncated_exp_dist_pmf(pdf,data,t,bin_width=0.1):\n",
    "\n",
    "    first_zero_idx = np.where(pdf==0)[0][0]\n",
    "    start,stop=data[0],data[-1]\n",
    "\n",
    "    #ensure that bins do not straddle non-zero part of pmf and zero part\n",
    "    div = (t-start)/bin_width\n",
    "    assert div%1==0 \n",
    "\n",
    "    bin_lbs = np.arange(start,stop,bin_width)\n",
    "    bin_ubs = bin_lbs+bin_width\n",
    "    bin_mps = (bin_lbs+bin_ubs)/2\n",
    "\n",
    "    n_bins = int((stop-start)/bin_width)\n",
    "    res = len(pdf) #number of pdf values\n",
    "    dx = (stop-start)/res\n",
    "    pmf = np.zeros(n_bins)\n",
    "\n",
    "    for i in range(n_bins):\n",
    "        stt_idx = int((i*res)/n_bins)\n",
    "        end_idx = int((((i+1)/n_bins)*res))\n",
    "        prob_mass = np.sum(pdf[stt_idx:end_idx]*dx)\n",
    "        pmf[i]=prob_mass\n",
    "    \n",
    "    return bin_mps,pmf\n",
    "\n",
    "def compute_predicted_pmf(lambda_,bin_centers,threshold_bin):\n",
    "\n",
    "    '''\n",
    "    Determines a truncated exponential pmf given a scale parameter and bins to integrate over.\n",
    "    Arguments:\n",
    "        lambda_: scale parameter\n",
    "        bin_centers: used to determine how to integrate pdf\n",
    "        threshold_bin: used to determine where truncated exp pdf should go to 0\n",
    "\n",
    "    \n",
    "    '''\n",
    "\n",
    "    bin_width = (np.diff(bin_centers))[0] #should be const.\n",
    "\n",
    "    #determine useful bin properties\n",
    "    bin_lbs = bin_centers-(bin_width/2)\n",
    "    bin_ubs = bin_centers+(bin_width/2)\n",
    "    bin_bounds = np.array(list(zip(bin_lbs,bin_ubs)))\n",
    "\n",
    "    #determine where pdf should go to 0\n",
    "    threshold = threshold_bin-(bin_width/2) #states where pdf should go to 0\n",
    "\n",
    "    #producing pdf over most of input space\n",
    "    start=0\n",
    "    stop=2\n",
    "    lambda_=lambda_\n",
    "    t=threshold\n",
    "    size=1000; assert np.log10(size)%1==0 #in this implementation we require size to be power of 10\n",
    "    data,pdf=data,pdf = truncated_exp_dist_pdf(lambda_=lambda_,t=t,start=start,stop=stop,size=size)\n",
    "    dx=np.round(np.mean(np.diff(data)),int(np.log10(len(data)))) #assuming data is of for 10^n\n",
    "    assert(np.round(sum(pdf*dx),1)==1),print(np.sum(pdf*dx)) #check that prob mass sums to 0\n",
    "\n",
    "    pred_pmf = np.zeros(len(bin_centers))\n",
    "    for idx,bin_bound in enumerate(bin_bounds):\n",
    "        lb,ub=bin_bound\n",
    "        idx_a=np.argmin(np.abs(data-lb))\n",
    "        idx_b=np.argmin(np.abs(data-ub))\n",
    "        sliced_pdf = pdf[idx_a:idx_b]\n",
    "        prob_mass = np.sum(pdf[idx_a:idx_b]*dx)\n",
    "        pred_pmf[idx]=prob_mass\n",
    "\n",
    "    return pred_pmf\n",
    "\n",
    "def MSE_LOSS(lambda_,bin_centers,threshold_bin,true_pmf):\n",
    "    pred_pmf = compute_predicted_pmf(lambda_,bin_centers,threshold_bin)\n",
    "    loss=np.mean((pred_pmf-true_pmf)**2)\n",
    "    #print(f\"lambda: {lambda_}, loss: {loss}\")\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSE FIT for optimal lambdas\n",
    "import scipy.optimize as optimize\n",
    "\n",
    "OPT_LAMBDA = []\n",
    "\n",
    "for pmf in DATA:\n",
    "\n",
    "    true_pmf=pmf\n",
    "    bin_centers = np.linspace(0.1,1.1,len(FLOP_bins)) #mapped FLOP_bins\n",
    "    bin_width = np.diff(bin_centers)[0]\n",
    "    threshold_bin = bin_centers[(np.where(true_pmf==0)[0][0])]\n",
    "\n",
    "    initial_lambda = [2.0]\n",
    "    args = (bin_centers,threshold_bin,true_pmf) #args to minimize that are fixed\n",
    "\n",
    "    result = optimize.minimize(MSE_LOSS,initial_lambda,args=args)\n",
    "\n",
    "    opt_lambda = result.x\n",
    "    OPT_LAMBDA.append(opt_lambda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs=plt.subplots(nrows=3,ncols=2,figsize=(8,8))\n",
    "\n",
    "bin_centers = np.linspace(0.1,1.1,len(FLOP_bins)) #mapped FLOP_bins\n",
    "width = np.diff(bin_centers)[0]\n",
    "\n",
    "#plot params\n",
    "alpha=0.6\n",
    "y_lim = [0,1]\n",
    "\n",
    "for row_idx,ax_row in enumerate(axs):\n",
    "    year = 2022+row_idx\n",
    "\n",
    "    true_pmf = DATA[row_idx]\n",
    "    opt_lambda = OPT_LAMBDA[row_idx]\n",
    "    threshold_bin = bin_centers[(np.where(true_pmf==0)[0][0])]\n",
    "    best_fit_pred_pmf = compute_predicted_pmf(opt_lambda,bin_centers,threshold_bin)\n",
    "\n",
    "    for col_idx,ax in enumerate(ax_row):\n",
    "        if col_idx==0: ax.bar(bin_centers,true_pmf,edgecolor='black',width=width,label=f'true pmf {year}',alpha=alpha,color='tab:blue')\n",
    "        if col_idx==1: ax.bar(bin_centers,best_fit_pred_pmf,edgecolor='black',width=width,label=f'pred pmf {year}, $\\lambda={np.round(opt_lambda[0],1)}$',alpha=alpha,color='red')\n",
    "\n",
    "        ax.set_ylim(y_lim)\n",
    "        ax.legend()\n",
    "        ax.grid(alpha=0.6)\n",
    "\n",
    "        if row_idx==2:\n",
    "            ax.set_xticks(bin_centers)\n",
    "            ax.set_xticklabels(FLOP_bins,rotation=45)\n",
    "            ax.set_xlabel('FLOP bins',fontsize=12)\n",
    "        else:\n",
    "            ax.set_xticklabels([])\n",
    "\n",
    "\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT_LAMBDA=False\n",
    "PLOT_PREDICTED_DIST=False\n",
    "\n",
    "\n",
    "FLOP_bins = ['23-24', '24-25', '25-26', '26-27', '27-28', '28-29']\n",
    "start_year = 2025\n",
    "end_year = 2029\n",
    "future_years = np.arange(start_year,end_year+1)\n",
    "years = np.array([2022,2023,2024])\n",
    "\n",
    "mapped_years = years - 2022 #0 indexing\n",
    "mapped_future_years = future_years - 2022\n",
    "OPT_LAMBDA_arr = np.array([elem[0] for elem in OPT_LAMBDA])\n",
    "\n",
    "\n",
    "def exp_func(x,a,b):\n",
    "    return a*np.exp(-b*x)\n",
    "\n",
    "popt,pcov = optimize.curve_fit(exp_func,mapped_years,OPT_LAMBDA_arr,p0=(1,1))\n",
    "PRED_LAMBDAS = exp_func(mapped_future_years,*popt)\n",
    "\n",
    "\n",
    "#opt lambda extrapolation\n",
    "if PLOT_LAMBDA:\n",
    "    fig,ax=plt.subplots()\n",
    "\n",
    "    all_years = np.concatenate([years,future_years])\n",
    "    lambdas = np.concatenate([OPT_LAMBDA_arr,PRED_LAMBDAS])\n",
    "    \n",
    "    ax.plot(all_years,lambdas)\n",
    "\n",
    "    #ax.set_xlim([0,10])\n",
    "    ax.set_ylim([0,11])\n",
    "\n",
    "#threshold bin extrapolation\n",
    "threshold_bins = ['25-26','26-27','26-27']\n",
    "future_threshold_bins = ['26-27','27-28','27-28','28-29','28-29'] #manually extrapolating right now - that's enough. Assuming that frontier training compute growing by an OOM every two years\n",
    "\n",
    "#model number extrapolation\n",
    "future_model_numbers = [75,110,180,280,400] #just vibing it right now - more interested in the distributions\n",
    "\n",
    "\n",
    "fig,axs = plt.subplots(nrows=len(future_years),figsize=(6,10))\n",
    "bin_centers = np.linspace(0.1,1.1,len(FLOP_bins))\n",
    "width = np.diff(bin_centers)[0]\n",
    "\n",
    "for idx,year in enumerate(future_years):\n",
    "    pred_lambda = PRED_LAMBDAS[0]\n",
    "    threshold_bin = future_threshold_bins[idx]\n",
    "    ax = axs[idx]\n",
    "\n",
    "    threshold_bin_idx = np.where(np.array(FLOP_bins)==threshold_bin)[0][0]; print(threshold_bin_idx)\n",
    "    mapped_threshold_bin = bin_centers[threshold_bin_idx]\n",
    "    predicted_pmf = compute_predicted_pmf(pred_lambda,bin_centers,mapped_threshold_bin)\n",
    "\n",
    "    ax.bar(bin_centers,predicted_pmf,width=width,edgecolor='black')\n",
    "    ax.set_title(f'{year} predicted pmf')\n",
    "    ax.set_ylim([0,1])\n",
    "    ax.grid(alpha=0.5)\n",
    "    ax.set_xticks(bin_centers)\n",
    "    ax.set_xticklabels(FLOP_bins)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "PREDICTED_HISTOGRAM_DATA_DF = pd.DataFrame(index=FLOP_bins,columns=future_years)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
