{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "all_systems_csv_path = \"/Users/iyngkarrankumar/Documents/Misc/Tracking models/data/all_systems.csv\"\n",
    "notable_models_csv_path = \"data/notable_ai_models.csv\"\n",
    "large_scale_models_csv_path = \"data/large_scale_ai_models.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def half_year_bin(date):\n",
    "    #CHATGPT generated\n",
    "\n",
    "    if date.month <= 6:\n",
    "        return f'{date.year}-H1'\n",
    "    else: \n",
    "        return f'{date.year}-H2'\n",
    "\n",
    "def year_bin(date):\n",
    "    return date.year\n",
    "\n",
    "\n",
    "def exponential_model(x,a,b):\n",
    "    return a*np.exp(b*(x-2017))\n",
    "\n",
    "def geometric_model(x,a,r):\n",
    "    return a*r**(x-2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nan ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_systems_csv_path = \"/Users/iyngkarrankumar/Documents/Misc/Tracking models/data/all_systems.csv\"\n",
    "notable_models_csv_path = \"data/notable_ai_models.csv\"\n",
    "large_scale_models_csv_path = \"data/large_scale_ai_models.csv\"\n",
    "\n",
    "ALL_SYSTEMS_DATA = pd.read_csv(all_systems_csv_path)\n",
    "NOTABLE_SYSTEMS_DATA = pd.read_csv(notable_models_csv_path)\n",
    "LARGE_SCALE_DATA = pd.read_csv(large_scale_models_csv_path)\n",
    "\n",
    "AS_nan_ratio = (ALL_SYSTEMS_DATA['Training compute (FLOP)'].isna().sum())/len(ALL_SYSTEMS_DATA)\n",
    "NS_nan_ratio = (NOTABLE_SYSTEMS_DATA['Training compute (FLOP)'].isna().sum())/len(NOTABLE_SYSTEMS_DATA)\n",
    "LS_nan_ratio = (LARGE_SCALE_DATA['Training compute (FLOP)'].isna().sum())/len(LARGE_SCALE_DATA)\n",
    "\n",
    "\n",
    "print(AS_nan_ratio,NS_nan_ratio,LS_nan_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## all systems v.s. large scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "\n",
    "all_systems_csv_path = \"/Users/iyngkarrankumar/Documents/Misc/Tracking models/data/all_systems.csv\"\n",
    "notable_models_csv_path = \"data/notable_ai_models.csv\"\n",
    "large_scale_models_csv_path = \"data/large_scale_ai_models.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def half_year_bin(date):\n",
    "    #CHATGPT generated\n",
    "\n",
    "    if date.month <= 6:\n",
    "        return f'{date.year}-H1'\n",
    "    else: \n",
    "        return f'{date.year}-H2'\n",
    "\n",
    "def year_bin(date):\n",
    "    return date.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_SYSTEMS_DATA = pd.read_csv(all_systems_csv_path)\n",
    "NOTABLE_SYSTEMS_DATA = pd.read_csv(notable_models_csv_path)\n",
    "LARGE_SCALE_DATA = pd.read_csv(large_scale_models_csv_path)\n",
    "\n",
    "start_year = 2017\n",
    "\n",
    "DATASHEET_LIST = [ALL_SYSTEMS_DATA,LARGE_SCALE_DATA]\n",
    "\n",
    "for DATASHEET in DATASHEET_LIST:\n",
    "    DATASHEET['Publication date'] = pd.to_datetime(DATASHEET['Publication date'])\n",
    "    DATASHEET = DATASHEET[DATASHEET['Publication date'] > f'{start_year}-01-01'] #data filtering\n",
    "\n",
    "\n",
    "    DATASHEET['Publication bin'] = DATASHEET['Publication date'].apply(year_bin)\n",
    "    DATASHEET.dropna(subset=['Training compute (FLOP)'],inplace=True)\n",
    "    DATASHEET['log Training compute'] = np.log10(DATASHEET['Training compute (FLOP)'])\n",
    "\n",
    "\n",
    "fig,ax= plt.subplots()\n",
    "\n",
    "ax.scatter(ALL_SYSTEMS_DATA['Publication bin'],ALL_SYSTEMS_DATA['log Training compute'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring large-scale model distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "all_systems_csv_path = \"/Users/iyngkarrankumar/Documents/Misc/Tracking models/data/all_systems.csv\"\n",
    "notable_models_csv_path = \"data/notable_ai_models.csv\"\n",
    "large_scale_models_csv_path = \"data/large_scale_ai_models.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LARGE_SCALE_DATA = pd.read_csv(large_scale_models_csv_path)\n",
    "\n",
    "LARGE_SCALE_DATA['Publication date'] = pd.to_datetime(LARGE_SCALE_DATA['Publication date'])\n",
    "start_year = 2017\n",
    "LARGE_SCALE_DATA = LARGE_SCALE_DATA[LARGE_SCALE_DATA['Publication date'] > f'{start_year}-01-01']\n",
    "\n",
    "#binning\n",
    "LARGE_SCALE_DATA['Publication bin'] = LARGE_SCALE_DATA['Publication date'].apply(year_bin)\n",
    "\n",
    "COMPUTE_DATA = LARGE_SCALE_DATA.dropna(subset=['Training compute (FLOP)'])\n",
    "COMPUTE_DATA['log Training compute'] = np.log10(COMPUTE_DATA['Training compute (FLOP)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOUBLE_2024 = True\n",
    "\n",
    "years = list(reversed(COMPUTE_DATA['Publication bin'].unique()))\n",
    "years.insert(1,2018)\n",
    "years.insert(2,2019)\n",
    "\n",
    "fig,axs = plt.subplots(nrows=len(years),ncols=1,figsize=(7,11),sharex=True)\n",
    "bin_range = (22,27)\n",
    "bins=np.arange(bin_range[0],bin_range[-1],1)\n",
    "\n",
    "HISTOGRAM_DATA_DF = pd.DataFrame(columns=years)\n",
    "for bin in bins[:-1]:\n",
    "    HISTOGRAM_DATA_DF.loc[f'{bin}-{bin+1}'] = [np.nan]*len(years)\n",
    "\n",
    "for idx,year in enumerate(years):\n",
    "    ax = axs[idx]\n",
    "    filtered_df = COMPUTE_DATA[COMPUTE_DATA['Publication bin']==year] #year df\n",
    "    \n",
    "    if year==2024 and DOUBLE_2024: filtered_df = pd.concat([filtered_df,filtered_df],ignore_index=True)\n",
    "\n",
    "    #histogram data\n",
    "    bin_count = np.histogram(filtered_df['log Training compute'],bins=bins,range=bin_range)[0]\n",
    "    for idx,bin in enumerate(bins[:-1]): #-1 index to sort out indexing problems\n",
    "        HISTOGRAM_DATA_DF.loc[f'{bin}-{bin+1}',year] = int(bin_count[idx])\n",
    "\n",
    "    #plot\n",
    "    filtered_df['log Training compute'].plot(kind='hist',bins=bins,range=bin_range,edgecolor='black',ax=ax)\n",
    "    ax.set_xlabel('');ax.set_ylabel('')\n",
    "    ax.set_xlim(bin_range)\n",
    "    ax.tick_params(axis='y',labelsize=12)\n",
    "    #if idx==0:     ax.yaxis.set_major_locator(mpl.ticker.MaxNLocator(integer=True))\n",
    "\n",
    "    ax.set_title(f'Year {year}, n={len(filtered_df)}',fontsize=15)\n",
    "    ax.grid(alpha=0.5)\n",
    "    ax.set_ylim([0,40])\n",
    "\n",
    "\n",
    "\n",
    "fig.text(0.5, -0.04, 'Log Compute ($10^X$)', ha='center', fontsize=15)\n",
    "fig.text(-0.04, 0.5, 'Frequency', va='center', rotation='vertical', fontsize=15)\n",
    "plt.xticks(bins,fontsize=15)\n",
    "plt.subplots_adjust(hspace=10)\n",
    "plt.tight_layout(rect=[0.04, 0.04, 1, 1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT_PROBS = False\n",
    "\n",
    "\n",
    "years_to_fit = [2022,2023,2024] #just from eyeballing plots for now\n",
    "NORMED_FIT_DATA = [] #to store normed fit data for later plotting\n",
    "NORM_FACTORS = []\n",
    "ALPHA = [] #to store shape parameter\n",
    "BETA = [] #to store rate parameter\n",
    "const = 1e-4\n",
    "\n",
    "for year in years_to_fit:\n",
    "\n",
    "    #adding values\n",
    "    fit_data = HISTOGRAM_DATA_DF[year]\n",
    "    fit_data = fit_data.drop('22-23')\n",
    "    new_data = pd.Series([0,0,0],index=['26-27','27-28','28-29'])\n",
    "    fit_data = pd.concat([fit_data,new_data])\n",
    "\n",
    "    #data structure processing\n",
    "    fit_data_arr = np.array(fit_data) + const\n",
    "    norm_factor = fit_data_arr.sum(); NORM_FACTORS.append(norm_factor)\n",
    "    normed_fit_data = (fit_data_arr/norm_factor)+const #const to prevent fit errors\n",
    "    NORMED_FIT_DATA.append(normed_fit_data)\n",
    "\n",
    "    shape,_,scale = stats.gamma.fit(fit_data_arr,floc=0)\n",
    "\n",
    "    ALPHA.append(shape)\n",
    "    BETA.append(scale)\n",
    "\n",
    "x_labels = fit_data.index\n",
    "\n",
    "fig,axs=plt.subplots(ncols=3,figsize=(12,6),sharey=True)\n",
    "for idx,ax in enumerate(axs):\n",
    "    #ax.hist(normed_fit_data)\n",
    "\n",
    "    x=np.linspace(0.2,1.2,len(fit_data))\n",
    "    ax.set_xticks(x)\n",
    "\n",
    "    if PLOT_PROBS:\n",
    "        ax.bar(x,NORMED_FIT_DATA[idx],width=0.1,alpha=0.5,color='g',label='Data') #plot data\n",
    "\n",
    "        fitted_pdf = stats.gamma.pdf(x,ALPHA[idx],0,BETA[idx])\n",
    "        ax.plot(x,fitted_pdf,'r-',lw=2,label='Fitted gamma distribution')\n",
    "\n",
    "        ax.set_xticklabels(x_labels,rotation=45)\n",
    "        if idx==1: ax.legend()\n",
    "        if idx==0: ax.set_ylabel('Probability')\n",
    "\n",
    "    else: \n",
    "        ax.bar(x,NORM_FACTORS[idx]*NORMED_FIT_DATA[idx],width=0.1,alpha=0.5,color='g',label='Data') #plot data\n",
    "\n",
    "        fitted_pdf = stats.gamma.pdf(x,ALPHA[idx],0,BETA[idx])\n",
    "        ax.plot(x,NORM_FACTORS[idx]*fitted_pdf,'r-',lw=2,label='Fitted gamma distribution')\n",
    "\n",
    "        ax.set_xticklabels(x_labels,rotation=45)\n",
    "        if idx==1: ax.legend()\n",
    "        if idx==0: ax.set_ylabel('Frequency')\n",
    "\n",
    "    ax.set_title(f'Year: {years_to_fit[idx]}')\n",
    "\n",
    "fig.suptitle('Gamma distribution frequency predictions')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('FTM')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f33f59546c507a35a4881afce9503208f6c8f0e8d914c07bf4768a8e3992010"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
