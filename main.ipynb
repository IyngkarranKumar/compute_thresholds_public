{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression #for linear regression\n",
    "from scipy.optimize import curve_fit #for exponential fit\n",
    "\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def half_year_bin(date):\n",
    "    #CHATGPT generated\n",
    "\n",
    "    if date.month <= 6:\n",
    "        return f'{date.year}-H1'\n",
    "    else: \n",
    "        return f'{date.year}-H2'\n",
    "\n",
    "def year_bin(date):\n",
    "    return date.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = '/Users/iyngkarrankumar/Documents/Misc/Tracking models/data/all_systems.csv'\n",
    "DATA = pd.read_csv(csv_path)\n",
    "start_year = 2017\n",
    "DATA['Publication date'] = pd.to_datetime(DATA['Publication date'])\n",
    "DATA = DATA[DATA['Publication date'] > f'{start_year}-01-01'] #data filtering\n",
    "\n",
    "bin_type = 'year'\n",
    "if bin_type=='year':\n",
    "    DATA['Publication bin'] = DATA['Publication date'].apply(year_bin)\n",
    "elif bin_type=='half year':\n",
    "    DATA['Publication bin'] = DATA['Publication date'].apply(half_year_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extrapolating model number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geometric_model(x,a,r):\n",
    "    return a*r**(x-2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOUBLE_2024 = False\n",
    "\n",
    "if DOUBLE_2024:\n",
    "    years = (sorted(DATA['Publication bin'].unique()))\n",
    "    model_counts = list(DATA['Publication bin'].value_counts().sort_index())\n",
    "    print(years,model_counts)\n",
    "    model_counts[-1] = 2*model_counts[-1]\n",
    "    future_years = np.arange(2025,2030)\n",
    "else:\n",
    "    years = (sorted(DATA['Publication bin'].unique())); years.pop()\n",
    "    model_counts = list(DATA['Publication bin'].value_counts().sort_index()); model_counts.pop() #remove 2024 count\n",
    "    future_years = np.arange(2024,2030)\n",
    "\n",
    "years = np.array(years)\n",
    "model_counts = np.array(model_counts)\n",
    "\n",
    "#Linear extrapolation\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(years.reshape(-1,1),model_counts)\n",
    "linear_pred = linear_model.predict(future_years.reshape(-1,1))\n",
    "\n",
    "#Polynomial extrapolation\n",
    "degree = 2 \n",
    "coefficients = np.polyfit(years, model_counts,degree)\n",
    "polynomial = np.poly1d(coefficients)\n",
    "poly_pred = polynomial(future_years)\n",
    "\n",
    "\n",
    "#geometric series\n",
    "popt_geometric, _ = curve_fit(geometric_model,years,model_counts)\n",
    "geometric_pred = geometric_model(future_years,*popt_geometric)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing data compute distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_DATA = DATA.dropna(subset=['Training compute (FLOP)']) #knocks out ~400 models\n",
    "nan_compute_DATA = DATA[DATA['Training compute (FLOP)'].isna()]\n",
    "\n",
    "compute_DATA['log10 Training compute'] = np.log10(compute_DATA['Training compute (FLOP)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inverse weighting\n",
    "\n",
    "#find counts\n",
    "years = list(reversed(DATA['Publication bin'].unique()))\n",
    "bin_range = (15,27)\n",
    "bins=np.arange(bin_range[0],bin_range[-1],1)\n",
    "bins_nudged = bins+1\n",
    "bin_ranges = list(zip(bins,bins_nudged))\n",
    "\n",
    "YEAR_DATA_STORE = {year:None for year in years}\n",
    "\n",
    "for idx,year in enumerate(years):\n",
    "    DATA_STORE = {\n",
    "        'Bin frequency':None,\n",
    "        'nan number':None,\n",
    "    }\n",
    "    BIN_FREQUENCY = []\n",
    "\n",
    "    compute_year_filtered = compute_DATA[compute_DATA['Publication bin']==year]\n",
    "\n",
    "    for bin_range in bin_ranges:\n",
    "        bin_range_lower_bound_bool = compute_year_filtered['log10 Training compute'] > bin_range[0]\n",
    "        bin_range_upper_bound_bool = compute_year_filtered['log10 Training compute'] < bin_range[-1]\n",
    "        bin_range_filtered_df = compute_year_filtered[bin_range_lower_bound_bool & bin_range_upper_bound_bool]\n",
    "        BIN_FREQUENCY.append(len(bin_range_filtered_df))\n",
    "    DATA_STORE['Bin frequency'] = BIN_FREQUENCY\n",
    "\n",
    "\n",
    "    year_nan_number = (nan_compute_DATA['Publication bin']==year).sum()\n",
    "    DATA_STORE['nan number'] = year_nan_number\n",
    "\n",
    "    YEAR_DATA_STORE[year] = DATA_STORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    DATA_STORE = YEAR_DATA_STORE[year]\n",
    "    compute_bin_counts = np.array(DATA_STORE['Bin frequency'])\n",
    "    new_compute_bin_counts = compute_bin_counts.copy()\n",
    "    nan_number = DATA_STORE['nan number']\n",
    "\n",
    "    #defining probability distribution\n",
    "    norm_const = 1/sum(compute_bin_counts)\n",
    "    prob_dist = norm_const * compute_bin_counts\n",
    "    \n",
    "    samples = np.random.choice(a=len(bin_ranges),size=nan_number,p=prob_dist)\n",
    "\n",
    "    #new counts\n",
    "    for s in samples:\n",
    "        new_compute_bin_counts[s]+=1\n",
    "\n",
    "\n",
    "    DATA_STORE['New counts'] = new_compute_bin_counts\n",
    "    YEAR_DATA_STORE[year]=DATA_STORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### PLOT\n",
    "\n",
    "years = list(reversed(DATA['Publication bin'].unique()))\n",
    "fig,axs = plt.subplots(nrows=len(years), ncols=1,figsize=(8,12),sharex=True)\n",
    "bin_range = (15,28)\n",
    "bins=np.arange(bin_range[0],bin_range[-1],1)\n",
    "\n",
    "for idx,year in enumerate(years):\n",
    "    ax = axs[idx]\n",
    "    filtered_df = compute_DATA[compute_DATA['Publication bin']==year] #year df\n",
    "    filtered_df['log10 Training compute'].plot(kind='hist',bins=bins,range=bin_range,edgecolor='black',ax=ax)\n",
    "    ax.set_xlabel('');ax.set_ylabel('')\n",
    "    ax.set_xlim(bin_range)\n",
    "    ax.tick_params(axis='y',labelsize=12)\n",
    "    if idx==0:     ax.yaxis.set_major_locator(mpl.ticker.MaxNLocator(integer=True))\n",
    "\n",
    "    ax.set_title(f'Year {year}, n={len(filtered_df)}',fontsize=15)\n",
    "    ax.grid(alpha=0.5)\n",
    "\n",
    "\n",
    "\n",
    "fig.text(0.5, -0.04, 'Log Compute ($10^X$)', ha='center', fontsize=15)\n",
    "fig.text(-0.04, 0.5, 'Frequency', va='center', rotation='vertical', fontsize=15)\n",
    "plt.xticks(bins,fontsize=15)\n",
    "plt.subplots_adjust(hspace=10)\n",
    "plt.tight_layout(rect=[0.04, 0.04, 1, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining for prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('FTM')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f33f59546c507a35a4881afce9503208f6c8f0e8d914c07bf4768a8e3992010"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
