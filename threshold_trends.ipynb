{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_lTUR72DwBf"
      },
      "source": [
        "### Setup & process data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qc4FQn3e_H5J"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy import stats, optimize\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "import copy,re\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dc1ROggn_Jdc"
      },
      "outputs": [],
      "source": [
        "# df = pd.read_csv(\"https://epochai.org/data/epochdb/notable_systems.csv\")\n",
        "url = 'https://drive.google.com/file/d/1RLLKPU3bEYK65wlQlU0p20u9M8cHkLMl/view?usp=sharing'\n",
        "url = 'https://drive.google.com/uc?id=' + url.split('/')[-2]\n",
        "\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "df = df[~df[\"Notability criteria\"].isna()]\n",
        "\n",
        "df[\"compute\"] = df[\"Training compute (FLOP)\"]\n",
        "df[\"date\"] = df[\"Publication date\"]\n",
        "df[\"model\"] = df[\"System\"]\n",
        "df[\"poss1e23\"] = df[\"Possibly over 1e23 FLOP\"]\n",
        "df[\"poss1e25\"] = df[\"Estimated over 1e25 FLOP\"]\n",
        "df[\"cost\"] = df[\"Training compute cost (2023 USD)\"]\n",
        "df[\"cost\"] = df[\"cost\"].str.replace(\",\", \"\").str.replace(\"$\", \"\").astype(float)\n",
        "\n",
        "df = df[[\"model\", \"compute\", \"date\", \"cost\", \"poss1e23\", \"poss1e25\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzQLJ6zoAPCJ"
      },
      "outputs": [],
      "source": [
        "to_remove = ['AlphaGo Zero','AlphaZero']\n",
        "df = df[~df[\"model\"].isin(to_remove)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvpkSfsi_Ky7"
      },
      "outputs": [],
      "source": [
        "to_append = [\n",
        "  [\"Claude 3.5 Sonnet\", 4.3e25, \"2024-06-21\", np.nan, np.nan, np.nan],\n",
        "  [\"GPT-4o Mini\", 1.2e25, \"2024-07-18\", np.nan, np.nan, np.nan],\n",
        "]\n",
        "\n",
        "for row in to_append:\n",
        "  if row[0] not in df[\"model\"].values:\n",
        "    df.loc[len(df)] = row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DrJxXbK_P2_",
        "outputId": "2f6b9c12-f5a4-4974-b658-7b93770c9ea8"
      },
      "outputs": [],
      "source": [
        "to_add_compute = {\n",
        "    \"Claude 3 Opus\": 2.5e25,\n",
        "    \"Claude 3 Sonnet\": 1.1e25,\n",
        "    \"GPT-4o\": 2.9e25,\n",
        "    \"Gemini 1.0 Pro\": 2.8e24,\n",
        "    \"Gemini 1.5 Pro\": 1.9e25,\n",
        "    \"Reka Core\": 8.4e24,\n",
        "    \"GPT-4 Turbo\": 2.1e25,  # rough guess\n",
        "    \"GPT-4V\": 2.1e25,  # rough guess\n",
        "    \"Claude 2.1\": df[df[\"model\"]==\"Claude 2\"][\"compute\"].values,  # rough guess\n",
        "}\n",
        "\n",
        "for k, v in to_add_compute.items():\n",
        "  if df.loc[df[\"model\"] == k, \"compute\"].isna().values:\n",
        "    df.loc[df[\"model\"] == k, \"compute\"] = v\n",
        "  else:\n",
        "    print(f\"{k} already has a compute value\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "j5FIbiwO_TSM"
      },
      "outputs": [],
      "source": [
        "# Reset the ones we've set\n",
        "df.loc[~df[\"compute\"].isna(), \"poss1e23\"] = np.nan\n",
        "df.loc[~df[\"compute\"].isna(), \"poss1e25\"] = np.nan\n",
        "\n",
        "# Set some temporary placeholder values\n",
        "# TODO: revisit\n",
        "# df.loc[(df[\"poss1e25\"] == \"checked\"), \"compute\"] = 1.01e25  # placeholder\n",
        "# df.loc[((df[\"poss1e23\"] == \"checked\") & (df[\"poss1e25\"] != \"checked\")), \"compute\"] = 1.01e23  # placeholder\n",
        "\n",
        "# We want to handle these leading models manually via the above compute estimates.\n",
        "assert df[(df[\"poss1e25\"] == \"checked\") & (df[\"compute\"].isna())].size == 0\n",
        "\n",
        "# We sample 1e23-1e25 models with unknown compute from the existing empirical distribution.\n",
        "# TODO: revisit\n",
        "poss1e23 = ((df[\"poss1e23\"] == \"checked\") & (df[\"poss1e25\"] != \"checked\"))\n",
        "df.loc[poss1e23, \"compute\"] = df[(df[\"compute\"] >= 1e23) & (df[\"compute\"] < 1e25)][\"compute\"].sample(poss1e23.sum(), random_state=0).values\n",
        "\n",
        "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "df[\"log_compute\"] = np.log10(df[\"compute\"])\n",
        "\n",
        "df[\"date_float\"] = df[\"date\"].dt.year + df[\"date\"].dt.month/12\n",
        "\n",
        "df['year'] = df['date'].dt.year\n",
        "\n",
        "df = df.sort_values(\"date\")\n",
        "df.dropna(subset=\"compute\", inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcSAbGjr_UX1",
        "outputId": "440a0f41-1b3b-4141-8f59-63d56b197da7"
      },
      "outputs": [],
      "source": [
        "fig = sns.scatterplot(data=df[df['date']>'2010-01-01'], x='date',y='compute')\n",
        "fig.set(yscale='log')\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzezAuEwD5X7"
      },
      "source": [
        "# Method 1: Advancing normal distributions + exp.count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grCg7WlTAjIx"
      },
      "source": [
        "### Model counts extrapolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWd1rfow_oaQ"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import t\n",
        "\n",
        "def exp_fit(x,a,b):\n",
        "  return a*np.exp(b*x)\n",
        "\n",
        "def exp_pred_counts(years,year_counts,future_years,alpha=0.1):\n",
        "  mapped_years = np.arange(0,len(year_counts)).astype(float)\n",
        "  popt, pcov = optimize.curve_fit(exp_fit, mapped_years, year_counts.values.astype(float))\n",
        "  pred_counts = exp_fit(future_years-years[0],*popt).astype(int)\n",
        "\n",
        "\n",
        "\n",
        "  #conf bounds\n",
        "  #assuming log normal uncertainty\n",
        "  fit_pred_counts = exp_fit(mapped_years,*popt) #predicted counts for fitted years\n",
        "  log_pred_counts_fit = np.log(fit_pred_counts)\n",
        "  log_obs_counts = np.log(year_counts.values.astype(float))\n",
        "  residuals = log_pred_counts_fit - log_obs_counts #we're calculating residuals of log counts\n",
        "  SEP = np.sqrt(np.sum(residuals**2)/(len(year_counts-2)))\n",
        "\n",
        "  dof = len(year_counts)-2\n",
        "  crit_t_val = t.ppf(1-alpha/2, dof)\n",
        "  pred_delta = crit_t_val*SEP\n",
        "\n",
        "  years_all = np.concatenate([years,future_years])\n",
        "  preds_all = np.concatenate([fit_pred_counts,pred_counts])\n",
        "  log_pred_UB = np.log(preds_all)+pred_delta\n",
        "  log_pred_LB = np.log(preds_all)-pred_delta\n",
        "  pred_counts_UB = np.exp(log_pred_UB)\n",
        "  pred_counts_LB = np.exp(log_pred_LB)\n",
        "\n",
        "  return years_all,preds_all,pred_counts_UB,pred_counts_LB\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omNWLc0LCnak",
        "outputId": "f5354234-ea88-4ea8-dd71-264d1d878e7a"
      },
      "outputs": [],
      "source": [
        "tmp_df = df[(df['date']>'2017-01-01') & (df['date']<'2024-01-01')]\n",
        "years = np.arange(2017,2023+1)\n",
        "year_counts = tmp_df.groupby(['year']).size()\n",
        "future_years = np.arange(2024,2030+1)\n",
        "all_years, pred_counts, pred_counts_UB,pred_counts_LB = exp_pred_counts(years,year_counts,future_years,alpha=0.1)\n",
        "\n",
        "fig,ax=plt.subplots()\n",
        "ax.plot(all_years,pred_counts,label='Predicted counts')\n",
        "ax.fill_between(all_years,pred_counts_UB,pred_counts_LB,alpha=0.1,label='90% CI')\n",
        "ax.legend()\n",
        "ax.grid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9Nx7nE2kz3ji"
      },
      "outputs": [],
      "source": [
        "## Bayesian approach\n",
        "from scipy import optimize\n",
        "from scipy.stats import multivariate_normal\n",
        "\n",
        "def exp_fit(x,a,b):\n",
        "  return a*np.exp(b*x)\n",
        "\n",
        "\n",
        "tmp_df = df[(df['date']>'2017-01-01') & (df['date']<'2024-01-01')]\n",
        "years = np.arange(2017,2023+1)\n",
        "year_counts = tmp_df.groupby(['year']).size()\n",
        "future_years = np.arange(2024,2030+1)\n",
        "\n",
        "mapped_years = np.arange(0,len(years)).astype(float)\n",
        "popt_counts, cov_counts = optimize.curve_fit(exp_fit, mapped_years, year_counts.values.astype(float))\n",
        "#pred_counts = exp_fit(future_years-years[0],*popt).astype(int)\n",
        "\n",
        "\n",
        "window_size = '30D'\n",
        "dates = pd.to_datetime(tmp_df['date'])\n",
        "rolling = dates.rolling(window=window_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyWN_HFKEC99"
      },
      "source": [
        "### Fit compute distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZghJY_MKEYR-"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import gaussian_kde,norm,linregress\n",
        "from scipy.optimize import minimize\n",
        "from sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o36zOT4pEKEk"
      },
      "outputs": [],
      "source": [
        "## utils\n",
        "\n",
        "#placeholder vars\n",
        "data_to_fit=None\n",
        "mu_0=None\n",
        "\n",
        "#to fit right tail of normal dist to 2024 data\n",
        "def trunc_norm_NLL(sigma):\n",
        "    ll = norm.logpdf(data_to_fit.to_numpy(),mu_0,sigma) - np.log(1-norm.cdf(data_to_fit.to_numpy().min(),mu_0,sigma))\n",
        "    return -np.sum(ll)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "thTDoNvoGm1J",
        "outputId": "4ccc048a-449c-4ea2-f78d-83626d4d7235"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "LinearRegression()"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## mean log compute prediction\n",
        "tmp_df = df[(df['date']>'2017-01-01') & (df['date']<'2024-01-01')]\n",
        "tmp_df = tmp_df[~tmp_df['compute'].isna()]\n",
        "\n",
        "year_grouped_df = tmp_df.groupby(['year'])\n",
        "mean_log_compute = year_grouped_df['log_compute'].mean().reset_index()\n",
        "\n",
        "X = mean_log_compute['year'].values\n",
        "\n",
        "mean_log_compute_model = LinearRegression()\n",
        "mean_log_compute_model.fit(X.reshape(-1,1),mean_log_compute)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "8DA1kuAYE_Dy",
        "outputId": "b0bc0750-8d1d-4cba-a4f3-08679e5cba0e"
      },
      "outputs": [],
      "source": [
        "tmp_df = df[(df['date']>'2017-01-01') & (df['date']<'2024-01-01')]\n",
        "tmp_df = tmp_df[~tmp_df['compute'].isna()]\n",
        "year_grouped_df = tmp_df.groupby(['year'])\n",
        "\n",
        "mean_log_compute = year_grouped_df['log_compute'].mean().reset_index()\n",
        "X = mean_log_compute['year'].values\n",
        "mean_log_compute_model = LinearRegression()\n",
        "mean_log_compute_model.fit(X.reshape(-1,1),mean_log_compute)\n",
        "\n",
        "MEAN_log_compute = year_grouped_df['log_compute'].mean()\n",
        "STD_log_compute = year_grouped_df['log_compute'].std()\n",
        "\n",
        "#predict stats for 2024\n",
        "mean_log_compute_2024 = mean_log_compute_model.predict([[2024]])[0][-1]\n",
        "MEAN_log_compute.loc[2024] = mean_log_compute_2024\n",
        "tmp_df_2024 = df[df['date']>'2024-01-01']\n",
        "RT_log_compute_data_2024 = tmp_df_2024[tmp_df_2024['log_compute']>mean_log_compute_2024]['log_compute'] #right tail of 2024 log compute data\n",
        "init_sigma = np.std(RT_log_compute_data_2024)\n",
        "data_to_fit = RT_log_compute_data_2024\n",
        "mu_0 = mean_log_compute_2024\n",
        "sigma = (minimize(trunc_norm_NLL,[init_sigma])).x\n",
        "STD_log_compute.loc[2024]=sigma[0] #2024 std found assuming that we just have right tail data\n",
        "\n",
        "\n",
        "years = np.arange(2017,2024+1)\n",
        "future_years = np.arange(2025,2029+1)\n",
        "years_concat = np.concatenate([years,future_years])\n",
        "\n",
        "\n",
        "CONST_VAR=True\n",
        "last_n = 4\n",
        "\n",
        "\n",
        "fig,ax=plt.subplots()\n",
        "\n",
        "for idx,year in enumerate(years_concat):\n",
        "\n",
        "  if year in years:\n",
        "    log_compute_data = tmp_df[tmp_df['year']==year]['log_compute']\n",
        "    if idx%2==0:\n",
        "      sns.kdeplot(log_compute_data,label=f'{year}',alpha=0.5,linewidth=2,linestyle='--',color='tab:blue')\n",
        "\n",
        "\n",
        "\n",
        "  if year in future_years:\n",
        "    pred_mean_log_compute = mean_log_compute_model.predict(np.array(year).reshape(-1,1))[0]\n",
        "    if CONST_VAR:\n",
        "      pred_std_log_compute = STD_log_compute[-1*last_n:].mean() #take mean of last n std vals\n",
        "    else: #smth more fancy\n",
        "      pass\n",
        "\n",
        "    x_min,x_max = pred_mean_log_compute[-1]-3*pred_std_log_compute,pred_mean_log_compute[-1]+3*pred_std_log_compute\n",
        "    x =  np.linspace(x_min,x_max,1000)\n",
        "    norm_pdf = norm.pdf(x,pred_mean_log_compute[-1],scale=pred_std_log_compute)\n",
        "\n",
        "    if idx%2==0:\n",
        "      ax.plot(x,norm_pdf,label=f'predicted {year}',linewidth=3,color='tab:red',alpha=0.8)\n",
        "\n",
        "    ax.legend()\n",
        "\n",
        "ax.grid(alpha=0.6)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c78CNKS7MN9Q"
      },
      "source": [
        "### Count models above threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGgvgxZDQbhb"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "SET_G_MEAN = True\n",
        "mean_2023,g_mean = MEAN_log_compute.loc[2023],0.65 #set growth rate of distribution mean in OOMs\n",
        "CONST_STD = False #const std for model distributions\n",
        "\n",
        "\n",
        "tmp_df = df[(df['date']>'2017-01-01') & (df['date']<'2024-01-01')]\n",
        "tmp_year = 2021\n",
        "\n",
        "years = np.arange(2017,2023+1)\n",
        "future_years = np.arange(2024,2028+1)\n",
        "all_years = np.concatenate([years,future_years])\n",
        "\n",
        "if CONST_STD:\n",
        "    idx_tmp_year = np.where(years==tmp_year)[0][0]\n",
        "    dist_std = STD_log_compute.loc[tmp_year:].mean()\n",
        "else:\n",
        "    std_bounds = [1.1,1.6]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "stt_bin,stop_bin,num = 23,30,1000\n",
        "x=np.linspace(start=stt_bin,stop=stop_bin,num=num)\n",
        "bin_edges = np.arange(stt_bin,stop_bin+1)\n",
        "\n",
        "#\n",
        "rollouts = 1000\n",
        "\n",
        "\n",
        "sampled_count_params = multivariate_normal.rvs(mean=popt_counts,cov=cov_counts,size=rollouts) #assume flat prior, normal posterior\n",
        "\n",
        "mapped_future_years = future_years-years[0]\n",
        "count_predictions = np.array([exp_fit(mapped_future_years,a,b) for a,b in sampled_count_params])\n",
        "\n",
        "SAMPLES = []\n",
        "\n",
        "if SET_G_MEAN:\n",
        "  future_means = np.array([mean_2023 + (year-2023)*g_mean for year in future_years]) #grow at .5 OOMs per year\n",
        "else:\n",
        "  future_means = mean_log_compute_model.predict(future_years.reshape(-1,1))[:,1] #direct mean extrap\n",
        "\n",
        "if CONST_STD:\n",
        "  pass\n",
        "else:\n",
        "  rollout_stds = np.random.uniform(*std_bounds,size=(1000,len(future_years))) #pretty hacky way to model stds\n",
        "\n",
        "\n",
        "for rollout_idx in range(rollouts):\n",
        "  rollout_count_predictions = count_predictions[rollout_idx]\n",
        "  if CONST_STD:\n",
        "    future_stds = np.array([dist_std]*len(future_years))\n",
        "  if not CONST_STD:\n",
        "    future_stds = rollout_stds[rollout_idx]\n",
        "\n",
        "  sample = [norm.rvs(loc=mean,scale=std,size=n.astype(int)) for mean,std,n in zip(future_means,future_stds,rollout_count_predictions)]\n",
        "  SAMPLES.append(sample)\n",
        "\n",
        "\n",
        "\n",
        "bin_strs = [f'>{b} lF' for b in bin_edges[:-1]]\n",
        "\n",
        "\n",
        "tmp_df_50 = pd.DataFrame(index=bin_strs,columns=future_years) #tmp df to compute median cumulative number of models exceeding theshold\n",
        "tmp_df_95 = copy.deepcopy(tmp_df_50)\n",
        "tmp_df_5 = copy.deepcopy(tmp_df_50)\n",
        "\n",
        "import sys\n",
        "\n",
        "for year_idx,year in enumerate(future_years):\n",
        "  year_samples = [sample[year_idx] for sample in SAMPLES]\n",
        "\n",
        "  for bin in bin_edges[:-1]:\n",
        "    exceed_thresh = [(sample>bin).sum() for sample in year_samples] #count how many exceed threshold\n",
        "    tmp_df_50.at[f'>{bin} lF',year] = (np.percentile(exceed_thresh,50)).astype(int)\n",
        "    tmp_df_95.at[f'>{bin} lF',year] = (np.percentile(exceed_thresh,95)).astype(int)\n",
        "    tmp_df_5.at[f'>{bin} lF',year] = (np.percentile(exceed_thresh,5)).astype(int)\n",
        "\n",
        "    if bin==27:\n",
        "      #sys.exit()\n",
        "      pass\n",
        "\n",
        "cumulative_df_50 = (tmp_df_50).cumsum(axis=1)\n",
        "cumulative_df_95 = (tmp_df_95).cumsum(axis=1)\n",
        "cumulative_df_5 = (tmp_df_5).cumsum(axis=1)\n",
        "\n",
        "\n",
        "index = cumulative_df_50.index\n",
        "cols = cumulative_df_50.columns\n",
        "\n",
        "tmp_df = copy.deepcopy(cumulative_df_50)\n",
        "\n",
        "for idx in index:\n",
        "  for col in cols:\n",
        "    n_50,n_5,n_95 = cumulative_df_50.at[idx,col], cumulative_df_5.at[idx,col], cumulative_df_95.at[idx,col]\n",
        "    input = f'{n_50}  ({n_5}:{n_95})'\n",
        "    tmp_df.at[idx,col] = input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "collapsed": true,
        "id": "6GHzOJrqRAqD",
        "outputId": "76bed965-979e-4b36-fe74-923bf44eb6a3"
      },
      "outputs": [],
      "source": [
        "#replace with UB/LB cumulative df if need\n",
        "\n",
        "fig,ax=plt.subplots(figsize=(10,6))\n",
        "for label,values in cumulative_df_50.T.items():\n",
        "  bin = int((label.split(' '))[0].split('>')[-1])\n",
        "  label_ = f'> {bin} log FLOP'\n",
        "  ax.plot(values.index.values,values.values,label=label_,marker='x')\n",
        "\n",
        "ax.grid()\n",
        "ax.legend(fontsize=12)\n",
        "ax.set_title('Cumulative model count exceeding thresholds')\n",
        "yticks = np.arange(0,700,100)\n",
        "ax.set_yticks(yticks,labels=yticks,fontsize=12)\n",
        "ax.set_xticks(future_years,labels=future_years,rotation=45,fontsize=12)\n",
        "#ax.set_yscale('log')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "X0EXS0T83aZb",
        "outputId": "91316269-6074-482e-a933-0a5ac69e4ee1"
      },
      "outputs": [],
      "source": [
        "plt.scatter(STD_log_compute.loc[:2024].index,STD_log_compute.loc[:2024].values)\n",
        "plt.xlim([2016,2030])\n",
        "plt.xticks(np.arange(2016,2030),rotation=45)\n",
        "plt.ylim([0.5,2.0])\n",
        "\n",
        "bounds = [1.0,1.6]\n",
        "future_std = np.random.uniform(*bounds,size=(5,1))\n",
        "plt.scatter(np.arange(2025,2030),future_std,color='red')\n",
        "plt.axhline(y=bounds[0],color='r',linestyle='--',alpha=0.6)\n",
        "plt.axhline(y=bounds[-1],color='r',linestyle='--',alpha=0.6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-v5P4vmVIqK"
      },
      "source": [
        "### Backtesting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 843
        },
        "id": "MKI6r9HNVKo4",
        "outputId": "eb38fa61-d2e0-4f06-c67b-3b7e7acbbb48"
      },
      "outputs": [],
      "source": [
        "backtest_years = np.arange(2020,2023+1)\n",
        "tmp_df = df[df['date'].dt.year.isin(backtest_years)]\n",
        "tmp_df = tmp_df[~tmp_df['compute'].isna()]\n",
        "log_compute_data = tmp_df['log_compute']\n",
        "log_compute_data.index = tmp_df['year'].loc[log_compute_data.index]\n",
        "\n",
        "\n",
        "index = cumulative_df_50.index.values[:2] #first two threhsolds in cum df\n",
        "obs_df = pd.DataFrame(index=index,columns=backtest_years)\n",
        "pred_df = pd.DataFrame(index=index,columns=backtest_years)\n",
        "\n",
        "thresholds = [23,24]\n",
        "\n",
        "#populate obs df\n",
        "for thr in thresholds:\n",
        "  threshold_count = (log_compute_data[log_compute_data>thr]).groupby(['year']).size()\n",
        "  cum_threshold_count = threshold_count.cumsum()\n",
        "  obs_df.loc[f'>{thr} lF'] = cum_threshold_count\n",
        "\n",
        "obs_df=obs_df.fillna(0)\n",
        "\n",
        "#populate pred df\n",
        "for year in backtest_years:\n",
        "  mean,sigma = MEAN_log_compute.loc[year],STD_log_compute.loc[year]\n",
        "  model_count = (pred_counts[np.where(all_years==year)[0][0]]).astype(int)\n",
        "  bin_edges = np.arange(15,30)\n",
        "  norm_cdf = norm.cdf(bin_edges,loc=mean,scale=sigma)\n",
        "  bin_pmfs = np.diff(norm_cdf)\n",
        "  bin_counts = (model_count*bin_pmfs).astype(int)\n",
        "\n",
        "\n",
        "  pred_df.loc[f'>{23} lF',year] = np.sum(bin_counts[np.where(bin_edges==23)[0][0]:])\n",
        "  pred_df.loc[f'>{24} lF',year] = np.sum(bin_counts[np.where(bin_edges==24)[0][0]:])\n",
        "\n",
        "\n",
        "fig,ax=plt.subplots()\n",
        "fig2,ax2=plt.subplots()\n",
        "palette=['tab:blue','tab:red']\n",
        "\n",
        "for idx,(k,v) in enumerate(pred_df.transpose().items()):\n",
        "  ax.plot(pred_df.columns.values,v,label=f'{k}(predicted)',linestyle='--',color=palette[idx])\n",
        "\n",
        "  #joint_pred_x=np.concatenate([pred_df.columns.values,cumulative_df.columns.values])\n",
        "  #joint_pred_y=np.concatenate([v.values,cumulative_df.loc[k].values])\n",
        "  #ax2.plot(joint_pred_x,joint_pred_y,linestyle='--',color=palette[idx],label=f'{k}(predicted)')\n",
        "\n",
        "  ax2.plot(pred_df.columns.values,v,label=f'{k}(predicted)',linestyle='--',color=palette[idx])\n",
        "  ax2.plot(cumulative_df_50.columns.values,cumulative_df_50.loc[k].values,linestyle='--',color=palette[idx])\n",
        "\n",
        "for idx,(k,v) in enumerate(obs_df.transpose().items()):\n",
        "  ax.plot(obs_df.columns.values,v,label=f'{k}(observed)',color=palette[idx])\n",
        "  ax2.plot(obs_df.columns.values,v,label=f'{k}(observed)',color=palette[idx])\n",
        "\n",
        "\n",
        "\n",
        "ax.legend()\n",
        "ax.set_ylabel('Cumulative model count')\n",
        "ax.grid()\n",
        "\n",
        "ax2.legend()\n",
        "ax2.set_ylabel('Cumulative model count')\n",
        "ax2.grid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "BYnYwAMNMaub",
        "outputId": "a1312ebd-0cc7-4d97-ebfe-630736b33fd1"
      },
      "outputs": [],
      "source": [
        "### on backtest, current model is underestimating number of models > 1e23 FLOP\n",
        "### exp. counts fit seems fine, so this is in the compute distribution assumptions\n",
        "\n",
        "fig,axs = plt.subplots(nrows=2,ncols=2)\n",
        "axs_ravel = axs.ravel()\n",
        "\n",
        "palette = ['tab:blue','tab:red','tab:green','tab:orange']\n",
        "\n",
        "for idx,year in enumerate(backtest_years):\n",
        "  ax = axs_ravel[idx]\n",
        "\n",
        "  mean,sigma = MEAN_log_compute.loc[year],STD_log_compute.loc[year]\n",
        "  x = np.linspace(mean-3*sigma,mean+3*sigma,num=1000)\n",
        "\n",
        "  sns.kdeplot(tmp_df[tmp_df['year']==year]['log_compute'],label=f'{year}',alpha=0.5,linewidth=2,color=palette[year-backtest_years[0]],ax=ax)\n",
        "  ax.plot(x,norm.pdf(x,mean,sigma),alpha=0.5,linestyle='--',linewidth=2,color=palette[year-backtest_years[0]])\n",
        "\n",
        "  ax.legend()\n",
        "  ax.grid()\n",
        "\n",
        "fig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdagyWR4QOIg"
      },
      "source": [
        "### Verify with investment/training budget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "collapsed": true,
        "id": "c79IqRjUmP2Y",
        "outputId": "e1e66bc2-8cf4-43fa-cc3e-e1a3deb59a89"
      },
      "outputs": [],
      "source": [
        "tmp_df = df[(df['date']>'2017-01-01') & (df['date']<'2024-01-01')]\n",
        "\n",
        "\n",
        "## fill na values with samples from empirical compute distribution of\n",
        "def fill_na_with_sample(group):\n",
        "  non_na_values = group.dropna()\n",
        "  return group.apply(lambda x: np.random.sample(non_na_values) if pd.isna(x) else x)\n",
        "\n",
        "year_grouped_df = tmp_df.groupby('year')\n",
        "tmp_df['compute'] = year_grouped_df['compute'].transform(fill_na_with_sample)\n",
        "tmp_df['log_compute'] = np.log10(tmp_df['compute'])\n",
        "\n",
        "total_log_compute = np.log10(tmp_df.groupby('year')['compute'].sum())\n",
        "\n",
        "log_compute_model = LinearRegression()\n",
        "log_compute_model.fit(total_log_compute.index.values.reshape(-1,1),total_log_compute.values)\n",
        "predicted_total_log_compute = log_compute_model.predict(future_years.reshape(-1,1))\n",
        "\n",
        "\n",
        "#from distributions\n",
        "def get_total_log_compute(percentile=50):\n",
        "  sample_total_compute = np.array([np.array([sum(10**s) for s in tmp_smp]) for tmp_smp in SAMPLES])\n",
        "  percentile = np.percentile(sample_total_compute,q=percentile,axis=0)\n",
        "  return np.log10(percentile)\n",
        "\n",
        "dist_total_log_compute = get_total_log_compute(percentile=50)\n",
        "\n",
        "\n",
        "\n",
        "fig,ax=plt.subplots(figsize=(10,6))\n",
        "ax.plot(total_log_compute.index.values,10**total_log_compute.values,color='tab:blue',marker='.',linewidth=3)\n",
        "ax.plot(future_years,10**predicted_total_log_compute,color='tab:blue',marker='x',linestyle='--',label='Total training compute direct extrapolation',linewidth=3)\n",
        "ax.plot(future_years,(10**dist_total_log_compute)*1.05,color='tab:red',marker='x',linestyle='--',label='Total predicted training compute',linewidth=3)\n",
        "ax.legend(fontsize=12)\n",
        "ax.set_yscale('log')\n",
        "ax.set_ylabel('Total compute',fontsize=12)\n",
        "ax.tick_params(axis='both',labelsize=12)\n",
        "ax.grid()\n",
        "\n",
        "\n",
        "total_compute_ratio = (10**dist_total_log_compute)/(10**predicted_total_log_compute)\n",
        "delta_total_log_compute = (10**dist_total_log_compute) - (10**predicted_total_log_compute)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULMPgr2DQInX"
      },
      "source": [
        "# Hardware capacity check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJgaAkLwQ-LG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCFD8lrEQPTc"
      },
      "outputs": [],
      "source": [
        "# source: https://www.macrotrends.net/stocks/charts/NVDA/nvidia/revenue\n",
        "\n",
        "temp_dict = {\n",
        "    \"2017-01-31\": 2173.0,\n",
        "    \"2017-04-30\": 1937.0,\n",
        "    \"2017-07-31\": 2230.0,\n",
        "    \"2017-10-31\": 2636.0,\n",
        "    \"2018-01-31\": 2911.0,\n",
        "    \"2018-04-30\": 3207.0,\n",
        "    \"2018-07-31\": 3123.0,\n",
        "    \"2018-10-31\": 3181.0,\n",
        "    \"2019-01-31\": 2205.0,\n",
        "    \"2019-04-30\": 2220.0,\n",
        "    \"2019-07-31\": 2579.0,\n",
        "    \"2019-10-31\": 3014.0,\n",
        "    \"2020-01-31\": 3105.0,\n",
        "    \"2020-04-30\": 3080.0,\n",
        "    \"2020-07-31\": 3866.0,\n",
        "    \"2020-10-31\": 4726.0,\n",
        "    \"2021-01-31\": 5003.0,\n",
        "    \"2021-04-30\": 5661.0,\n",
        "    \"2021-07-31\": 6507.0,\n",
        "    \"2021-10-31\": 7103.0,\n",
        "    \"2022-01-31\": 7643.0,\n",
        "    \"2022-04-30\": 8288.0,\n",
        "    \"2022-07-31\": 6704.0,\n",
        "    \"2022-10-31\": 5931.0,\n",
        "    \"2023-01-31\": 6051.0,\n",
        "    \"2023-04-30\": 7192.0,\n",
        "    \"2023-07-31\": 13507.0,\n",
        "    \"2023-10-31\": 18120.0,\n",
        "    \"2024-01-31\": 22103.0,\n",
        "    \"2024-04-30\": 26044.0\n",
        "} ## (1e6)\n",
        "\n",
        "#add predicted from next two quarters: https://finance.yahoo.com/quote/NVDA/analysis/?guccounter=1\n",
        "temp_dict[\"2024-07-31\"]=28520.0\n",
        "temp_dict[\"2024-10-31\"]=31400.0\n",
        "\n",
        "\n",
        "##predicted future annual revenues: https://finance.yahoo.com/quote/NVDA/analysis/?guccounter=1\n",
        "temp_dict_2 = {}\n",
        "temp_dict_2['2025'] = 120750.0\n",
        "temp_dict_2['2026'] = 166250.0\n",
        "\n",
        "quarterly_revenue_data = pd.DataFrame()\n",
        "quarterly_revenue_data['date'] = np.array(list(temp_dict.keys()))\n",
        "quarterly_revenue_data['date'] = pd.to_datetime(quarterly_revenue_data['date'])\n",
        "quarterly_revenue_data['revenue'] = np.array(list(temp_dict.values()))*1e6\n",
        "quarterly_revenue_data['log_revenue'] = np.log10(quarterly_revenue_data['revenue'])\n",
        "\n",
        "\n",
        "quarterly_revenue_data['year'] = quarterly_revenue_data['date'].dt.year\n",
        "year_grouped_df = quarterly_revenue_data.groupby('year')\n",
        "quarterly_revenue_data['revenue fraction'] = quarterly_revenue_data['revenue']/year_grouped_df['revenue'].transform('sum')\n",
        "\n",
        "revenue_fractions = quarterly_revenue_data.groupby('year')['revenue fraction'].apply(list).to_dict()\n",
        "avg_revenue_fractions = np.array([np.array(v) for k,v in revenue_fractions.items()]).mean(axis=0)\n",
        "\n",
        "quarters = ['01-31','04-30','07-31','10-31']\n",
        "\n",
        "new_rows=[]\n",
        "if 1:\n",
        "  for year,v in temp_dict_2.items():\n",
        "    for idx,q in enumerate(quarters):\n",
        "      new_date = pd.to_datetime(f'{int(year)}-'+q)\n",
        "      new_revenue = v*1e6*avg_revenue_fractions[idx]\n",
        "      quarterly_revenue_data['date'].loc[new_date] = v*1e6*avg_revenue_fractions[idx]\n",
        "      new_row = {\n",
        "          'date': pd.to_datetime(new_date),\n",
        "          'revenue': new_revenue,\n",
        "          'log_revenue': np.log10(new_revenue),\n",
        "          'year': year,\n",
        "        }\n",
        "      new_rows.append(new_row)\n",
        "\n",
        "quarterly_revenue_data = pd.concat([quarterly_revenue_data, pd.DataFrame(new_rows)], ignore_index=True)\n",
        "quarterly_revenue_data['date'] = pd.to_datetime(quarterly_revenue_data['date'])\n",
        "\n",
        "\n",
        "### FLOP/$ extrap\n",
        "#hardware (FLOP/$)\n",
        "start_year=2014\n",
        "log_FLOP_per_dollar_2014 = np.log10(2.2e17)\n",
        "OOM_growth_rate = 0.125 #figure 6 here https://epochai.org/blog/trends-in-machine-learning-hardware says 10x every 8 years --> 0.125 OOMs every year\n",
        "years = np.arange(start_year,2030+1)\n",
        "log_FLOP_per_dollar = log_FLOP_per_dollar_2014+OOM_growth_rate*(years-2014)\n",
        "FLOP_per_dollar = 10**log_FLOP_per_dollar\n",
        "temp_dict_3 = {k:v for k,v in list(zip(years,FLOP_per_dollar))}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 549
        },
        "id": "Ny-7MgkAQzA2",
        "outputId": "1b25710e-c920-4de6-d064-1a80af647b70"
      },
      "outputs": [],
      "source": [
        "PLOT_REVENUE_EXTRAP=False\n",
        "\n",
        "## regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "ref_date = pd.to_datetime('2000-01-01')\n",
        "quarterly_revenue_data['date_float'] = (quarterly_revenue_data['date']-ref_date).dt.days\n",
        "\n",
        "log_revenue_model = LinearRegression() #assume growing exponentially\n",
        "X = quarterly_revenue_data['date_float'].values.reshape(-1,1)\n",
        "Y = quarterly_revenue_data['log_revenue'].values\n",
        "log_revenue_model.fit(X,Y)\n",
        "\n",
        "future_dates = pd.to_datetime(pd.date_range(start='2027-01-01',end='2031-01-01',freq='Q'))\n",
        "future_dates_float = (future_dates-ref_date).days\n",
        "future_log_revenue_preds = log_revenue_model.predict(future_dates_float.values.reshape(-1,1))\n",
        "\n",
        "future_data = pd.DataFrame({\n",
        "    'date': future_dates,\n",
        "    'log_revenue': future_log_revenue_preds,\n",
        "    'revenue': 10**future_log_revenue_preds,\n",
        "    'date_float': future_dates_float\n",
        "})\n",
        "\n",
        "quarterly_revenue_data['date'] = pd.to_datetime(quarterly_revenue_data['date'])\n",
        "\n",
        "#plot quarterly revenue\n",
        "if PLOT_REVENUE_EXTRAP:\n",
        "  fig,ax = plt.subplots(figsize=(10,6))\n",
        "  ax.scatter(quarterly_revenue_data['date'],quarterly_revenue_data['revenue'],color='tab:blue')\n",
        "  ax.plot(quarterly_revenue_data['date'],10**(log_revenue_model.predict(X)),alpha=0.8,linestyle='--',color='tab:blue',linewidth=2)\n",
        "  ax.scatter(future_data['date'],future_data['revenue'],color='tab:red',label='pred')\n",
        "  ax.set_xlabel('Date')\n",
        "  ax.tick_params(axis='both',labelsize=12)\n",
        "  ax.set_ylabel('Revenue',fontsize=12)\n",
        "  ax.set_yscale('log')\n",
        "  ax.set_title('NVIDIA Log revenue extrapolation')\n",
        "  ax.grid()\n",
        "\n",
        "\n",
        "#plot cumulative FLOP\n",
        "chip_lifetime = 3\n",
        "t_lag = 1 #number of years from chip release to commericialisation\n",
        "\n",
        "combined_df = pd.concat([quarterly_revenue_data,future_data])\n",
        "combined_df['year'] = combined_df.date.dt.year\n",
        "year_grouped_df = combined_df.groupby('year')\n",
        "annual_revenue = year_grouped_df['revenue'].sum()\n",
        "annual_FLOP_production = pd.DataFrame(index=annual_revenue.index)\n",
        "annual_FLOP_production['FLOP produced'] = annual_revenue.index.map(temp_dict_3)*annual_revenue.values\n",
        "\n",
        "running_FLOP_total = (annual_FLOP_production['FLOP produced'].rolling(window=chip_lifetime,min_periods=chip_lifetime).sum().shift(t_lag)).dropna()\n",
        "m = round((np.log10(running_FLOP_total).max()-np.log10(running_FLOP_total).min())/(running_FLOP_total.index.values.max() - running_FLOP_total.index.values.min()),2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "FRAC=False\n",
        "\n",
        "fig2,(ax2,ax3) = plt.subplots(ncols=2,figsize=(12,6))\n",
        "\n",
        "xs_temp = total_log_compute.index.values\n",
        "xs_temp_2 = np.arange(xs_temp[-1]+1,xs_temp[-1]+1+len(predicted_total_log_compute))\n",
        "xs = np.concatenate([xs_temp,xs_temp_2])\n",
        "ys = 10**(np.concatenate([total_log_compute.values,predicted_total_log_compute]))\n",
        "\n",
        "#plotting direct FLOP extrap\n",
        "\n",
        "ax2.plot(running_FLOP_total.index.values,running_FLOP_total.values,marker='x',label=f'GPU FLOP available')\n",
        "ax2.plot(xs,ys,marker='x',color='tab:red',label='GPU FLOP used for ML training')\n",
        "ax2.grid()\n",
        "ax2.set_ylabel('FLOP',fontsize=14)\n",
        "#ax2.set_title(f'Total FLOP (chip lifetime={chip_lifetime})',fontsize=12)\n",
        "ax2.tick_params(axis='both',labelsize=12)\n",
        "#ticks = np.arange(2019,2030+1)\n",
        "#ax2.set_xticks(ticks,labels=ticks,rotation=45)\n",
        "ax2.set_yscale('log')\n",
        "ax2.set_xlim([2020,2030])\n",
        "ax2.legend(fontsize=12)\n",
        "ax2.set_ylim([1e23,1e32])\n",
        "ax2.set_xlim\n",
        "\n",
        "x_lb= max(xs.min(),running_FLOP_total.index.min())\n",
        "x_ub = min(xs.max(),running_FLOP_total.index.max())\n",
        "total_compute_bounded = (running_FLOP_total.loc[x_lb:x_ub]).values\n",
        "ML_flop_bounded = ys[np.where(xs==x_lb)[0][0]:np.where(xs==x_ub)[0][0]+1]\n",
        "assert(len(total_compute_bounded)==len(ML_flop_bounded))\n",
        "\n",
        "ax3.plot(np.arange(x_lb,x_ub+1),ML_flop_bounded/total_compute_bounded,color='tab:blue',marker='x',linestyle='--')\n",
        "ax3.set_yscale('log')\n",
        "ax3.set_title('Fraction of GPU FLOP spent on ML training')\n",
        "ax3.grid()\n",
        "ax3.tick_params(axis='both',labelsize=12)\n",
        "\n",
        "\n",
        "fig.tight_layout()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Y_lTUR72DwBf",
        "grCg7WlTAjIx"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "sci_comp_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
